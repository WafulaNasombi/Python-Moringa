{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression- used for predictive analysis.\n",
    "- Finding the best fit line based on one independent and dependent variables.- The line where the errors between the actual and predicted values are minimized \n",
    "- This means it shows how dependent variable y changes depending on how independent variable x changes.\n",
    "- Uses the equation y = b0 + b1x  + e or y = mx + c + e\n",
    "  where b1/m = the slope   b0/c = y intercept   y = dependent variable   e = standard error\n",
    "- The variables should have the following assumptions:\n",
    "\n",
    "1.  Linear relationship - between the independent and dependent variables. This means that a change in X leads to a proportional change in Y.\n",
    "* Check: Scatterplot of X and Y to visually inspect linearity.\n",
    "\n",
    "2. Homoscedasticity - no clear pattern in the residuals/errors. If they fan out or gather together as x increases that shows heteroscedasticity.\n",
    "* Check: plot residuals vs fitted values. Look for a random scatter.\n",
    "\n",
    "3. Independence - independence of the observations. This means that the residuals (errors) for one observation should not be correlated with the residuals for another observation.\n",
    "* Check: Durbin-Watson test for detecting autocorrelation.\n",
    "\n",
    "4. Normality of the Residuals- normal distribution of the residuals.\n",
    "* Check: Use a histogram or qq plot of the residuals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the assumptions in order to get the linear regression model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#loading the data\n",
    "x = np.array([1,2,3,4,5,6,7,8,9,10]).reshape(-1,1)\n",
    "y= np.array([2,3,5,7,11,13,17,19,23,29])\n",
    "\n",
    "#create the model and fit the data\n",
    "model = LinearRegression()\n",
    "model.fit(x,y)\n",
    "\n",
    "#predict the y values\n",
    "y_pred = model.predict(x)\n",
    "residuals = y - y_pred\n",
    "\n",
    "#testing the assumptions\n",
    "#1. Linearity\n",
    "# Linearity\n",
    "plt.scatter(x, y, color='blue')\n",
    "plt.plot(x, y_pred, color='red')\n",
    "plt.title('Linearity Check')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "#2. Independence- Durbin Watson test for autocorrelation. THERE SHOULD BE NO AUTOCORRELATION.This means that the residuals should not be dependent on each other in that they should be randomly distributed\n",
    "dw = durbin_watson(residuals)\n",
    "print('Durbin-Watson statistic:', dw) #should be close to 2. If it is between 1.5 and 2.5, then there is no autocorrelation between the residuals\n",
    "\n",
    "#3. Homoscedasticity- NO CLERAR PATTERN IN THE RESIDUALS\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Homoscedasticity Check')\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#4. Normality- The residuals should be normally distributed\n",
    "sm.qqplot(residuals, line='45')\n",
    "plt.title('Normality of Residuals')\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Residuals Histogram')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of linear regression\n",
    "## 1. Simple linear regression \n",
    "                           - positive linear regression - as x increases y increases y = b0 + b1x + e\n",
    "                            - negative linear regression -as x decreases y decreases  y = -b0 + b1x + e\n",
    "\n",
    "    - A). Cost Function/ Objective Function/Loss Function - is used to measure how well the model's predictions match the actual values.\n",
    "    * MSE- Mean Squared Error. Average of Squared Errors.\n",
    "    * RESIDUALS - the distance between the observed data points and the regression line. The more far away the observed points are from the regression line the higher the residual,thus the higher the cost function.\n",
    "    - The goal is to reduce the cost function as much as possible.\n",
    "\n",
    "    -B). Optimization - finding the parameters(slope and y intercept that minimizes the cost function). \n",
    "         - Thus adjust the model parameters to minimize MSE.(Mean Squared Error).\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use sklearn library to create a linear regression model\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1)) # reshape the array to 2D\n",
    "y = np.array([5, 20, 14, 32, 22, 38])\n",
    "\n",
    "# create a linear regression model]\n",
    "model = LinearRegression()\n",
    "\n",
    "#fit the model\n",
    "model.fit(x, y)\n",
    "\n",
    "#make the predictions\n",
    "y_pred = model.predict(x)\n",
    "\n",
    "# print the coefficient and intercept\n",
    "print('Slope:', model.coef_)\n",
    "print('Intercept:', model.intercept_)\n",
    "# the coefficient and intercept are the parameters of the line. Thus can help us to predict the value of y for any given x and \n",
    "#are used to create the line of best fit\n",
    "\n",
    "# print the metrics\n",
    "#mean squared error is the average of the squared differences between the actual and predicted values. It is a measure of how well the model is performing. If the value is 0, then the model is perfect.\n",
    "print('Mean squared error:', mean_squared_error(y,y_pred))\n",
    "\n",
    "#get the r2 score/coefficient of determination - to determine how well the model is fitting the data\n",
    "print('R2 score:', r2_score(y,y_pred))\n",
    " \"\"\" \n",
    "R2 score is the proportion of the variance in the dependent variable that is predictable from the independent variable\n",
    "it is a measure of how well the model is fitting the actual data. The closer the value is to 1, the better the model is fitting the data.\n",
    "The The high value of R-square determines the less difference between the predicted values and actual values and hence represents a good model.\n",
    "\"\"\"\n",
    "# plot the data\n",
    "plt.scatter(x, y, color = 'blue')\n",
    "plt.plot(x, y_pred, color = 'red', label = 'Regression Line')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using splitting the data into training and testing data\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1)) # reshape the array to 2D\n",
    "y = np.array([5, 20, 14, 32, 22, 38])\n",
    "\n",
    "#split the data into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)#test size is 20% of the data. Random state is the seed value/ starting point for the random number generator\n",
    "\n",
    "# create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "#fit the model using the training data\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "#make the predictions using the testing data\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "#print the metrics\n",
    "print('Mean squared error:', mean_squared_error(y_test,y_pred))\n",
    "print('R2 score:', r2_score(y_test,y_pred))\n",
    "\n",
    "# plot the data\n",
    "plt.scatter(x, y, color = 'blue')\n",
    "plt.plot(x, model.predict(x), color = 'red', label = 'Regression Line')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries used to Perform Linear regression\n",
    "1. statsmodels.api - used to get the statistical summary/ analysis of a model.\n",
    "          - returns the p- value, t-statistic, durbin_watson, coefficients.\n",
    "          - When used you need to add the constant eg x = sm.add_constant(x)\n",
    "2. sklearn - provides a simpler method for fitting the model and making predictions.\n",
    "           - provides the coefficient and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing linear regression using  statsmodels.api \n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = np.array([5, 15, 25, 35, 45, 55])\n",
    "y = np.array([5, 20, 14, 32, 22, 38])\n",
    "\n",
    "# add a constant to the x values\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "#split the data into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)#test size is 20% of the data. Random state is the seed value/ starting point for\n",
    "\n",
    "# create a linear regression model\n",
    "model = sm.OLS(y_train, x_train).fit()\n",
    "\n",
    "#make the predictions using the testing data\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "#print the metrics  \n",
    "print('Mean squared error:', mean_squared_error(y_test,y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing linear regression using the sklearn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create the data and reshape the x values to a 2D array\n",
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([5, 20, 14, 32, 22, 38])\n",
    "\n",
    "#split the data into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=0)\n",
    "\n",
    "#create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "#fit the model using the training data\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "#make the predictions using the testing data\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "#print the metrics\n",
    "print('Mean squared error:', mean_squared_error(y_test,y_pred))\n",
    "\n",
    "print('R2 score:', r2_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple Linear Regression - used to establish the relationship between one dependent variable and multiple independent variables.-apart from r2_score/rsquared you can use MAE(Mean Absolute Error (sum of absolute residuals/len of y)) to get if the model is fitting the data. Use MAE when units are important.\n",
    "\n",
    "- The y variable must be continuous/real value whilt ehe x variables can be continuous or categorical. And each feature must model a linear relationship with the y variable.\n",
    "- MLR tries to fit the regression line through a MULTIDIMENSIONAL SPACE of data-points. Thus we are finding the best fit for a plane\n",
    "\n",
    "- The formula is :\n",
    "\n",
    " y = B0 + B1x1 + B2x2+......Bnxn / y = c +  m1x1 + m2x2 + mnxn\n",
    "\n",
    " * Assumptions\n",
    " - Homoscedasticity of the errors/residuals - no clear pattern of the residuals.\n",
    " - Normality of the residuals - residuals should follow a normal distribution \n",
    " - Linearity of the variables- the independent and dependent variable should be linear to each other\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Statsmodels.api as sm\n",
    "- With multiple linear regression use rsquared_adj instead as rsquared tends to leave out some assumptions for linear regression.\n",
    "- Adjusted R-squared adjusts the R-squared value based on the number of predictors in the model, making it more suitable for multiple regression where multiple predictors are involved.\n",
    "- Unlike R-squared, which always increases with the addition of more predictors, adjusted R-squared accounts for the number of predictors and can decrease if the added variables do not improve the model sufficiently. This helps to prevent overfitting.\n",
    "- Use other error metrics as well, like MAE and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a simple linear regression model using a baseline independent variable\n",
    "y = ames_subset['SalePrice']\n",
    "X = ames_subset[['GrLivArea']]\n",
    "#baseline independent variable is the variable that has the most correlation wih the dependent variable\n",
    "#meaning if I were to plot a scatter plot that plow will be the most linear compared to other independent variables.\n",
    "\n",
    "import statsmodels.api as sm \n",
    "\n",
    "#create a model\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "#fit the model to the data\n",
    "results = model.fit()\n",
    "\n",
    "#display the results\n",
    "print(results.summary())#this summary will give you the r2_score, coefficients, intercept\n",
    "\n",
    "#building a multiple linear regression model\n",
    "X = ames_subset[['LotArea', '1stFlrSF', 'GrLivArea']]\n",
    "y = ames_subset['SalePrice']\n",
    "\n",
    "#create a linear regression model and fit it \n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "results = model.fit()\n",
    "\n",
    "\n",
    "\n",
    "#display results \n",
    "print(results.summary())#gives you the intercept based on all the independent variables.\n",
    "\n",
    "#getting MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('mean absolute error:', mean_absolute_error(y_pred,y))\n",
    "\n",
    "\n",
    "or \n",
    " \n",
    "mae = results.resid.abs().sum()/len (y)\n",
    "\n",
    "#to get the rmse - rmse and mae are interpreted the same way \n",
    " mse = results.resid.pow(2).sum()/len(y) or (results/resid **2).mean()\n",
    "\n",
    "rmse = mse ** 0.5 or np.sqrt(mse) or mean_squared_error(y, y_pred, squared=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Partial regression plots for the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import statsmodels.api as sm\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "#use .graphis.plot_partregress_grid for each independent variable to have it's own plot \n",
    "sm.graphics.plot_partregress_grid(results, exog_idx=['LotArea', '1stFlrSF', 'GrLivArea'], fig=fig)\n",
    "#use graphics.plot.regress_exog to plot the residuals/ccpr of independent variable against the dependent variable- gives 4 plots\n",
    "sm.graphics.plot_regress_exog(results,one_independent_variable(baseline_X),fig)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usine scikit-learn for multiple linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#create the model \n",
    "model = LinearRegression()\n",
    "#fit the model \n",
    "model.fit(ames_subset[['LotArea', '1stFlrSF', 'GrLivArea']], ames_subset['SalePrice'])\n",
    "#get the coefficient and the intercept\n",
    "print('Coefficient:',model.coef_)\n",
    "print('Intercept:',model.intercept_)\n",
    "#get the y predicted values\n",
    "y_pred = model.predict(ames_subset[['LotArea', '1stFlrSF', 'GrLivArea']])\n",
    "\n",
    "#model results\n",
    "print('r2_score:',r2_score(y_pred,ames_subset['SalePrice']))\n",
    "\n",
    "or  using the score method\n",
    "print('r2_score:',model.score(ames_subset[['LotArea', '1stFlrSF', 'GrLivArea']], ames_subset['SalePrice']))\n",
    "# the difference between the score and the r2_score is that the score method is used to get the r2_score of the model on the training data while the r2_score is used to get the r2_score of the model on the testing data. \n",
    "#that's why we do not need to pass the y_pred values to the score method because it already has the y_pred values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical variables\n",
    "- Encoding - convert categorical data to numerical representation.  \n",
    "1. Label Encoding - converting each category to have a unique integer.\n",
    "                  - done when there is meaningful order among the categories.\n",
    "                  eg High School, Bachelor, Masters, PHD\n",
    "2. One hot Encoding- there's no meaningful order needed. With categories norminal in nature. USED WITH SMALL NUMBER OF UNIQUE VALUES.\n",
    "                 - Converts each column into a binary column.(eg 0 and 1)\n",
    "                 eg encoding the state Carlifonia, New York, Florida\n",
    "                - Here is where we mark one category as True (1) and the other categories as False(0)\n",
    "\n",
    "            Example: For a feature with categories “Red”, “Green”, and “Blue”:\n",
    "Red: [1, 0, 0]\n",
    "Green: [0, 1, 0]\n",
    "Blue: [0, 0, 1]\n",
    "                \n",
    "3. Target Encoding/Mean encoding - used with high cardinality features. \n",
    "                - Each category is replaced by the mean target value for that category.\n",
    "                Example: For binary target values, a feature with categories “A”, “B”, and “C”:\n",
    "                 Category A: mean(target|A)\n",
    "                 Category B: mean(target|B)\n",
    "                 Category C: mean(target|C)\n",
    "\n",
    "4. Binary Encoding - using binary digits to encode data. Eg 00, 01, 10,11\n",
    "- Specifically we used one-hot encoding to create dummy variables (1s or 0s) representing each category\n",
    "In order to avoid the dummy variable trap we need to drop one of the dummy variable columns. Whichever column is dropped becomes the reference category, where all other category coefficients are compared to this category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using statsmodels.api to perform One Hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "#use sm to create a model using multiple independent variables \n",
    "x = ames_subset[['LotArea', '1stFlrSF', 'GrLivArea','origin']]\n",
    "y = ames_subset['SalePrice']\n",
    "\n",
    "#since origin is categorical in nature we encode it\n",
    "x = pd.get_dummies(x,columns=['origin'], drop_first=True)#drop_first= True is the same as drop = 'first'\n",
    "\n",
    "#create a model\n",
    "model = sm.OLS(y, sm.add_constant(x))\n",
    "\n",
    "#fit the model\n",
    "results = model.fit()\n",
    "\n",
    "#display the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using scikit learn to perform one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn to perform One Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#use the OneHotEncoder to encode the origin column\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)#drop the first column to avoid multicollinearity. Sparse is set to False to return a numpy array\n",
    "#fit the encoder to the origin column\n",
    "encoded = encoder.fit_transform(ames_subset[['origin']])\n",
    "#convert the encoded array to a dataframe\n",
    "encoded = pd.DataFrame(encoded, columns=encoder.get_feature_names(['origin']))\n",
    "\n",
    "#concatenate the encoded dataframe with the original dataframe\n",
    "ames_subset = pd.concat([ames_subset, encoded], axis=1)\n",
    "\n",
    "#drop the original origin column\n",
    "ames_subset.drop(subset=['origin'], axis=1, inplace=True)\n",
    "\n",
    "#use the encoded origin columns to create a model\n",
    "x = ames_subset[['LotArea', '1stFlrSF', 'GrLivArea','origin_2','origin_3']]\n",
    "y = ames_subset['SalePrice']\n",
    "\n",
    "#create a model\n",
    "model = sm.OLS(y, sm.add_constant(x))\n",
    "\n",
    "#fit the model\n",
    "results = model.fit()\n",
    "\n",
    "#display the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Transformations\n",
    "- Scaling - changing the units eg weight from lbs to kgs. Multiplying or dividing by a value.\n",
    "- Shifting - the intercept. Adding or subtracting a value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Transformations- This is useful when features have different scales, especially in models like linear regression, logistic regression, or neural networks.\n",
    "## When you want to transform the data to have a mean of 0 and a standard deviation of 1.\n",
    "## Mostly with continuous numerical data with different scales.\n",
    "\n",
    "- 1. Scaling - changing the units of coefficient to a more readable unit.\n",
    "      *  Multiplying /Dividing the coefficients.(variables)\n",
    "           eg. scaling the weight predictor so that the units are kilograms rather than pounds.Changing the scale.\n",
    "           The initial model is saying:\n",
    "\n",
    "     * From: For each increase of 1 lb in weight, we see an associated decrease of about .006 in MPG\n",
    "\n",
    "     * To The second model saying : For each increase of 1 kg in weight, we see an associated decrease of about .014 in MPG \n",
    "     \n",
    "\n",
    "- 2. Shifting - changing the intercept by adding/subtracting a value from the variable.\n",
    "      *  Scaling impacts the predictor coefficients, whereas shifting impacts the constant coefficient (i.e. the intercept). \n",
    "    eg X_years_ce = X_initial.copy() # X_initial is the dataframe of X. Independent variables.\n",
    "       X_years_ce[\"model year\"] = X_years_ce[\"model year\"] + 1900\n",
    "       X_years_ce\n",
    "    * Common method of Shifting- Zero centering- using 0 as the mean. Taking each value of that column and subtracting the mean of that column \n",
    "\n",
    " - 3. Standardizing: Centering + Scaling: Standardization is a combination of    \n",
    "         zero-centering  the variables and dividing by the standard deviation.\n",
    "      *  After performing this transformation, the result will have mean of 0 and a standard deviation of 1.\n",
    "    * Standardization changes the units of the coefficients so that they are in standard deviations rather than the specific units of each predictor. This allows us to make just that comparison AND HELPS YOU IDENTIFY THE MOST IMPORTANT PARAM AS IT HAS THE HIGHEST COEFFICIENT.\n",
    "\n",
    "   * Examples:\n",
    "\n",
    "     * Linear Transformation:\n",
    "        * Scenario: You have a dataset with features like height, weight, and age. These features have different ranges and units.\n",
    "        * Action: Apply standardization (Z-score normalization) to scale them to a mean of 0 and a standard deviation of 1.\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing - zero centering and Scaling- works best with continuous variables\n",
    "X_standardized = X_initial.copy()\n",
    "\n",
    "for col in X_standardized.columns:\n",
    "    X_standardized[col] = (X_standardized[col] - X_standardized[col].mean()) \\\n",
    "                            / X_standardized[col].std()\n",
    "    \n",
    "\"\"\"\n",
    "The result of standardizing is that the mean of each column is 0 and the standard deviation is 1. This is useful when we have features that are on different scales.\n",
    "Thus each coefficient in the model will be on the same scale and we can compare the coefficients to see which feature has the most impact on the dependent variable.\n",
    "\n",
    "eg if we have a model with the following coefficients:\n",
    "const         23.445918\n",
    "cylinders     -0.258817\n",
    "weight        -5.407040\n",
    "model year     2.770244\n",
    "\n",
    "For every increase in the weight of the car by 1 standard deviation, we see a decrease of about 5.4 MPG\n",
    "For every increase in the model year by 1 standard deviation, we see an increase of about 2.77 MPG\n",
    "For every increase in the number of cylinders by 1 standard deviation, we see a decrease of about 0.26 MPG\n",
    "\n",
    "This means that the weight of the car has the most impact on the dependent variable followed by the model year and then the number of cylinders.\n",
    "TO understand each standard deviation, divide the coefficient by the standard deviation of the feature.This will give you the impact of each feature on the dependent variable in its own units.\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "#scaling\n",
    "X_scaled = X_initial.copy()\n",
    "\n",
    "X_scaled['weight'] = X_scaled['weight'] * 0.45 # TO CHANGE every value of weight in lbs to kg\n",
    "\n",
    "\n",
    "#shifting\n",
    "X_shifted = X_initial.copy()\n",
    "\n",
    "X_shifted['years'] = X_shifted['years'] + 1900 # TO SHIFT the years to start from 1900\n",
    "\n",
    "#min-max scaling- to scale the data between 0 and 1\n",
    "X_min_max = X_initial.copy()\n",
    "\n",
    "for col in X_min_max.columns:\n",
    "    X_min_max[col] = (X_min_max[col] - X_min_max[col].min()) / (X_min_max[col].max() - X_min_max[col].min())\n",
    "\n",
    "#mean normalization- to scale the data between -1 and 1\n",
    "X_mean_norm = X_initial.copy()\n",
    "\n",
    "for col in X_mean_norm.columns:\n",
    "    X_mean_norm[col] = (X_mean_norm[col] - X_mean_norm[col].mean()) / (X_mean_norm[col].max() - X_mean_norm[col].min())\n",
    "\n",
    "\n",
    "#unit vector transformation- to scale the data to have a unit norm. This means that the magnitude of each column will be 1\n",
    "X_unit = X_initial.copy()\n",
    "\n",
    "for col in X_unit.columns:\n",
    "    X_unit[col] = X_unit[col] / np.linalg.norm(X_unit[col])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non - linear Transformations\n",
    "1. Log Transformations\n",
    "2. Interaction terms\n",
    "3. Polynomial regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic Transformations- Used to change normalize data that is highly skewed. \n",
    "## Right skewed. Has outliers. If the model is sensitive to outliers (e.g., linear regression), a log transformation can reduce the impact of extreme values.\n",
    "- The reason to apply this kind of transformation is that you believe that the underlying relationship is not linear.\n",
    "- Then by applying these techniques, you may be able to model a linear relationship between the transformed variables, even though there wasn't a linear relationship between the raw, un-transformed variables.\n",
    " - 1. Logarithmic functions: Logarithmic functions are the inverse of exponential functions. where x = np.arange(0,10,2)\n",
    "        - exponential => e_x =  np.exp(x)\n",
    "        - natural log => ln_e_x =  np.log(e_x)\n",
    "\n",
    "    - So, the conventional way to interpret a log-transformed coefficient is like this:\n",
    "- For each increase of 1% in <feature>, we see an associated change of <coefficient / 100> in <target>. Since logarithmic increases by 1% = np.log(1.01)=0.01 thus coefficient * 0.01 is similar to coefficient/100\n",
    "\n",
    "- do coefficient/100 when interpretation the coefficient.\n",
    "\n",
    "- If you fit a model with a log-transformed target, the algorithm will be minimizing the error in log(y) rather than y units. In other words, minimizing the percentage/proportional/multiplicative error rather than the raw additive error.\n",
    "\n",
    "* Log Transformation:\n",
    "\n",
    "    * Scenario: You are working with income data where most people earn between $20,000 and $50,000, but a few individuals earn over $1 million.\n",
    "    * Action: Apply a log transformation to reduce skewness and bring the income distribution closer to a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logarithmic transformation\n",
    "X_log = X_initial.copy()\n",
    "\n",
    "X_log['weight'] = np.log(X_log['weight'])# to transform the weight column to a logarithmic scale\n",
    "#to understand the coefficient, take coefficient/ 100 to get the percentage change in the dependent variable for a 1% change in the independent variable. Similar to 100% change in the independent variable\n",
    "weight_log_results.params[\"log(weight)\"] / 100  # 100 is the percentage change. This will give you the percentage change in the dependent variable for a 100% change in the independent variable\n",
    "#To describe this in terms of a 50% increase, we would want to multiply the coefficient by this value:\n",
    "weight_log_results.params[\"log(weight)\"] * np.log(1.5) # 1.5 is the percentage increase. This will give you the percentage change in the dependent variable for a 50% increase in the independent variable\n",
    "\n",
    "#log transforming the predictor\n",
    "X_log = np.log(X_raw)# x_weight is the one that is log transformed\n",
    "X_log\n",
    "\n",
    "\n",
    "#building model with log transformed x variable\n",
    "model_log = sm.OLS(y_raw, sm.add_constant(X_log))\n",
    "model_log_results = model_log.fit()\n",
    "model_log_results.summary()\n",
    "\n",
    "#results of above with log transformed x variable\n",
    "\"\"\"\n",
    "log(weight)   -20.4949  - For each increase of 1 in the natural log of the weight, we see an associated decrease of about 20 in the MPG\n",
    "model year      0.7809  - For each increase of 1 year in model year, we see an associated increase of about 0.78 in the MPG\n",
    "\"\"\"\n",
    "#interpretation of the results\n",
    "weight_log_results.params[\"log(weight)\"] / 100  # 100 is the percentage change. This will give you the percentage change in the dependent variable for a 100% change in the independent variable\n",
    "or\n",
    "weight_log_results.params[\"log(weight)\"] * np.log(1.01) # 1.01 is the percentage increase. This will give you the percentage change in the dependent variable for a 100% increase in the independent variable\n",
    "\"\"\"\n",
    "Interpretation: \n",
    "-0.2039306812432118  - For each increase of 1% in weight, we see an associated decrease of about 0.2 in MPG\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#log transformation of the target variable\n",
    "y_log = np.log(y_raw)\n",
    "y_log.name = \"log(mpg)\"\n",
    "y_log\n",
    "\n",
    "#building the model with log transformed y variable\n",
    "model_log = sm.OLS(y_log, sm.add_constant(X_raw))\n",
    "model_log_results = model_log.fit()\n",
    "model_log_results.summary()\n",
    "\n",
    "#results of above with log transformed y variable\n",
    "\"\"\"\n",
    "weight        -0.0003  - For each increase of 1 lb in weight, we see an associated decrease of about 0.0003 in the natural log of the MPG\n",
    "model year     0.0313   - For each increase of 1 year in model year, we see an associated increase of about 0.03 in the natural log of the MPG\n",
    "\"\"\"\n",
    "\n",
    "#interpretation of the results\n",
    "y_log_results.params[\"weight\"]\n",
    "#-0.00030860096419966\n",
    "np.exp(y_log_results.params[\"weight\"]) #increasing weight by 1 causes MPG to become about 99.969% of its original value\n",
    "# 0.99969144664818\n",
    "or \n",
    "y_log_results.params[\"weight\"] * 100 \n",
    "#-0.030860096419966 - For each increase of 1% in weight, we see an associated decrease of about 0.03 in the natural log of the MPG\n",
    "\n",
    "#log transforming both the predictor and the target\n",
    "X_log = np.log(X_raw)\n",
    "y_log = np.log(y_raw)\n",
    "\n",
    "#building the model with log transformed x and y variables\n",
    "model_log = sm.OLS(y_log, sm.add_constant(X_log))\n",
    "model_log_results = model_log.fit()\n",
    "model_log_results.summary()\n",
    "\n",
    "#results of above with log transformed x and y variables\n",
    "\"\"\"\n",
    "log(weight)    -0.9341 - For each increase of 1 in the natural log of weight, there is a decrease of about 0.9 (β) in the natural log of MPG\n",
    "model year      0.0328\n",
    "\"\"\"\n",
    "\n",
    "#interpretation of the results\n",
    "np.exp(np.log(1.01)*log_results.params[\"log(weight)\"])\n",
    "# 0.9907486046766623 - For each increase of 1% in weight, we see an associated decrease of about 0.9 in MPG\n",
    "\n",
    "#We can subtract 1 and multiply by 100 to interpret as a percentage change:\n",
    "(np.exp(np.log(1.01)*log_results.params[\"log(weight)\"]) - 1) * 100 \n",
    "#-0.925139532333768 - For each increase of 1% in weight, we see an associated decrease of about 0.9% in MPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transforming both the predictor and the target\n",
    "X = np.log(ames[['GrLivArea','1stFlrSF','GarageArea']])\n",
    "#define the y_variable as the log of the SalePrice column \n",
    "y= np.log(ames['SalePrice'])\n",
    "\n",
    "#create a model\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "\n",
    "#fit the model\n",
    "results = model.fit()\n",
    "\n",
    "#display the results\n",
    "print(results.summary())\n",
    "\n",
    "#interpretation of the results\n",
    "\"\"\"\n",
    "GrLivArea      0.5806 - For each increase of GrLivArea by 1% we see an associated increase of about 0.58% `in the SalePrice\n",
    "1stFlrSF       0.2688  - For each increase of 1stFlrSF by 1% we see an associated increase of about 0.27% in the SalePrice\n",
    "GarageArea     0.2976  - For each increase of GarageArea by 1% we see an associated increase of about 0.30% in the SalePrice\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions\n",
    "- Interactions in multiple linear regression refer to situations where the effect of one predictor variable on the target variable depends on the value of another predictor variable. In other words, the combined effect of two variables on the outcome is different from the sum of their individual effects.\n",
    " *  Without Interaction: The effect of each predictor on the response variable is assumed to be independent of the other predictors. For example, if you have two predictors, X1 and X2, the model would look like:\n",
    "  - Y=β0+β1×X1+β2×X2+ϵ\n",
    "  - Y=β0​+β1​×X1+β2​×X2+ϵ\n",
    "\n",
    "  * Here, the effect of X1 on Y is β1​, and the effect of X2 on Y is β2​, with no consideration of how X1 and X2 together might influence Y.\n",
    "  * With Interaction: You include a term in the model that represents the interaction between the predictors. For example:\n",
    "  - Y=β0+β1×X1+β2×X2+β3×(X1×X2)+ϵ\n",
    "  - Y=β0​+β1​×X1+β2​×X2+β3​×(X1×X2)+ϵ\n",
    "\n",
    "In this case, β3 is the coefficient for the interaction term X1×X2, meaning the effect of X1 on Y depends on the value of X2, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming you have a DataFrame 'ames' with 'GrLivArea' and 'GarageArea'\n",
    "# Create interaction term manually\n",
    "ames['GrLivArea_GarageArea'] = ames['GrLivArea'] * ames['GarageArea']\n",
    "\n",
    "# Select independent variables including the interaction term\n",
    "X = ames[['GrLivArea', 'GarageArea', 'GrLivArea_GarageArea']]\n",
    "\n",
    "# Add a constant (intercept term)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Define the dependent variable\n",
    "y = ames['SalePrice']\n",
    "\n",
    "# Fit the model\n",
    "interaction_model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(interaction_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures # for creating polynomial features that will be used to create interaction terms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming you have a DataFrame 'ames' with 'GrLivArea' and 'GarageArea'\n",
    "# Select independent variables including the interaction term\n",
    "X = ames[['GrLivArea', 'GarageArea']]\n",
    "y = ames['SalePrice']\n",
    "\n",
    "\n",
    "# Create polynomial features\n",
    "poly_features = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)#degree is the degree of the polynomial. 2 means that we will have a linear and quadratic polynomial. interaction_only=True means that we will only have interaction terms not polynomial terms. include_bias=False means Excludes the constant term (bias)\n",
    "X_poly = poly_features.fit_transform(X) #fit and transform the data to create the polynomial features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)#test size is 20% of the data. Random state is the seed value/ starting point for the random number generator. 80% of the data is used for training\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "model_results = model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_results.predict(X_test)\n",
    "\n",
    "# Print the metrics\n",
    "print('Mean squared error:', mean_squared_error(y_test, y_pred))\n",
    "print('R2 score:', r2_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression :\n",
    "- transforming your features into quadratic (squared) or higher-order polynomial terms so that you can model a non-linear relationship using linear regression.\n",
    "- polynomial to mean a feature raised to a power of 2 or higher. (This is a looser definition than you might have learned in math class.) The power that the feature is raised to is called the degree of the polynomial.\n",
    "  * formula = y = ax^2 + bx + c\n",
    "  * y=β0​+β1​x+β2​x2+β3​x3+…+βn​xn+ϵ\n",
    "\n",
    "     - β0​,β1​,…,βn​ are the coefficients of the polynomial.\n",
    "\n",
    "     * When to use:\n",
    "       1. Non-Linear Relationships: Use polynomial regression when the relationship between the independent variables and the dependent variable is non-linear.\n",
    "       2. Curve Fitting: It’s useful for fitting curves to data when linear regression is insufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming you have a DataFrame 'ames' with 'GrLivArea' and 'GarageArea'\n",
    "X = ames[['GrLivArea', 'GarageArea']]\n",
    "y = ames['SalePrice']\n",
    "\n",
    "# Create polynomial features\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False) # Set degree=2 to include both linear and quadratic terms (e.g., x1x1​, x2x2​, x12x12​, x1x2x1​x2​, x22x22​).\n",
    "X_poly = poly_features.fit_transform(X) # Fit and transform the data to create the polynomial features\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)  # Train the model on polynomial features\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "#root mean squared error\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
