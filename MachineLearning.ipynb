{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methodologies of Data Science Projects**\n",
    "1. CRISP-DM :- Cross industry Standard Process for Data Mining.\n",
    "  1. Business Understanding:-\n",
    "      - Who are the stakeholders in this project? Who will be directly affected by the creation of this project?\n",
    "     - What business problem(s) will this Data Science project solve for the organization?\n",
    "    - What problems are inside the scope of this project?\n",
    "    - What problems are outside the scope of this project?\n",
    "    - What data sources are available to us?\n",
    "    - What is the expected timeline for this project? Are there hard deadlines (e.g. \"must be live before holiday season shopping\") or is this an ongoing project?\n",
    "   - Do stakeholders from different parts of the company or organization all have the exact same understanding about what this project is and isn't? \n",
    "\n",
    " 2. Data Understanding :-\n",
    "   - What data is available to us? Where does it live? Do we have the data, or can we scrape/buy/source the data from somewhere else?\n",
    "   - Who controls the data sources, and what steps are needed to get access to the data?\n",
    "   - What is our target?\n",
    "   - What predictors are available to us?\n",
    "   - What data types are the predictors we'll be working with?\n",
    "   - What is the distribution of our data?\n",
    "   - How many observations does our dataset contain? Do we have a lot of data? Only a little?\n",
    "   - Do we have enough data to build a model? Will we need to use resampling methods?\n",
    "   - How do we know the data is correct? How is the data collected? Is there a chance the data could be wrong? \n",
    "\n",
    " 3. Data Preparation:\n",
    " - Detecting and dealing with missing values\n",
    " - Data type conversions (e.g. numeric data mistakenly encoded as strings)\n",
    " - Checking for and removing multicollinearity (correlated predictors)\n",
    " - Normalizing our numeric data\n",
    " - Converting categorical data to numeric format through one-hot encoding\n",
    "\n",
    " 4. Modeling -:\n",
    "  - Is this a classification task? A regression task? Something else?\n",
    "  - What models will we try?\n",
    "  - How do we deal with overfitting?\n",
    "  - Do we need to use regularization or not?\n",
    "  - What sort of validation strategy will we be using to check that our model works well on unseen data?\n",
    "  - What loss functions will we use?\n",
    "  - What threshold of performance do we consider as successful?\n",
    "\n",
    "5. Evaluation.\n",
    "6. Deployment.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. KDD - Knowledge Discovery in Databases.\n",
    "   1. Selection - Business Understanding in CRISP.\n",
    "   - The output of this stage is the dataset you'll be using for the Data Science project.\n",
    "   2. Preprocessing - The output of this stage is preprocessed data that is more \"clean\" than it was at the start of this stage -- although the dataset is not quite ready for modeling yet.\n",
    "   3. Transformation - The output of this stage is a dataset that is now ready for modeling.\n",
    "   4. Data Mining - refers to using different modeling techniques to try and build a model that solves the problem we're after -- often, this is a classification or regression task. During this stage, you'll also define your parameters for given models, as well as your overall criteria for measuring the performance of a model.\n",
    "   5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. OSEMN \n",
    "   1. Obtain - data\n",
    "   2. Scrub - filter the data\n",
    "   3. Explore - the data/visualizations\n",
    "   4. Model - modeling. Regression/ Classification\n",
    "   5. Interpret results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling Approaches**\n",
    "1. For Inference-: modeling data to draw conclusions.\n",
    "- What is the relationship between X and y?\n",
    "- How does X affect y?\n",
    "2. For predictions -: modeling data to make/draw predictions from it.\n",
    "- it is important for the model to generalize to unseen data. \n",
    "* Model Generalization - training a model based on some data then feeding the model new data to make predictions.\n",
    "- The gap between the training error and prediction error for new data (labeled \"optimism\") is growing as model complexity increases, which means that we are getting worse at generalizing.\n",
    "* Model Validation -: \n",
    "- Model validation is a process of measuring overfitting and indicates the degree of generalizability.\n",
    "\n",
    "- Here is how we perform validation, in its simplest form:\n",
    "\n",
    " 1. Split the data into two parts with a 70/30, 80/20, or a similar split\n",
    " 2. Use the larger part for training so the model learns from it\n",
    " 3. Use the smaller part for testing the model\n",
    "\n",
    " - This is called a train-test split and means that you can compare the model performance on training data vs. testing data using a given metric. The metric can be R-Squared or it can be an error-based metric like RMSE.\n",
    "\n",
    " - Or **CROSS VALIDATION**.splitting the data multiple times and training multiple models, to get more of a distribution of possible metrics rather than relying on metrics from a single train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Fundamentals\n",
    "- involves building models that model the relationship between independent and dependent variables emphasizing on prediction.\n",
    "1. Model Validation-(Assess how well the model will perform to unseen data)\n",
    "- Involve use of validation techniques.eg train_test_split from sklearn.model_selection and cross_validation from sklearn.preprocessing\n",
    "and sklearn.model_selection as well.\n",
    "\n",
    "2. Bias -: in ML this is the amount in which the model's predictions differ from the true value compared to the training data.Mainly caused by resampling/assumptions    in the model that make the target function easier to learn.\n",
    "- Error due to overly simplistic assumptions in the model (underfitting).The gap between the training error and prediction error for new data (labeled \"optimism\") is growing as model complexity increases, which means that we are getting worse at generalizing.\n",
    "\n",
    "* Variance -: variance describes how much a random variable differs from its expected value. Variance is based on a single training set. Variance measures the inconsistency of different predictions using different training sets.\n",
    "- Error due to the model being too sensitive to small fluctuations in the training data (overfitting).\n",
    "* **Bias-variance tradeoff** -:\n",
    "- Involves striking the balance between the bias and variance(balance between underfitting and overfitting)\n",
    "- Involves getting the lowest possible bias while also ensuring the model's performance is generalizable to unseen data to **REDUCE ERRORS**\n",
    "\n",
    "- **Underfitting** -: a model showing small variance and high bias will underfit the target.Making the model too simple.Thus, model will fail to campute the underlying patterns in the data.\n",
    "\n",
    "- eg A linear regression model used to fit non-linear data would have high bias because it assumes a linear relationship between the features and the target variable, which oversimplifies the true relationship.\n",
    "\n",
    "- **Overfitting** -: a model with high variance and little bias will overfit the target.\n",
    "\n",
    "3. Regularization -:\n",
    "- used to help avoid overfitting.(**REDUCE VARIANCE**)\n",
    " Ridge and Lasso regression.(Extensions to linear regression with penalty terms to help prevent overfitting). To reduce model complexity or gather more training data.\n",
    "\n",
    " - To reduce bias: Use a more complex model that can capture more patterns in the data (e.g., adding polynomial features to a linear regression model)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to reduce bias you make the model more complex\n",
    "#using Polynomial Regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Position_Salaries.csv')\n",
    "X = dataset.iloc[:, 1:2].values\n",
    "y = dataset.iloc[:, 2].values\n",
    "\n",
    "# Fitting Linear Regression to the dataset\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "# Fitting Polynomial Regression to the dataset\n",
    "poly_reg = PolynomialFeatures(degree = 4)\n",
    "X_poly = poly_reg.fit_transform(X)\n",
    "lin_reg_2 = LinearRegression()\n",
    "lin_reg_2.fit(X_poly, y)\n",
    "\n",
    "# Visualising the Linear Regression results\n",
    "plt.scatter(X, y, color = 'red')\n",
    "plt.plot(X, lin_reg.predict(X), color = 'blue') \n",
    "plt.title('Truth or Bluff (Linear Regression)') \n",
    "plt.xlabel('Position level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "\n",
    "# Visualising the Polynomial Regression results\n",
    "plt.scatter(X, y, color = 'red')\n",
    "plt.plot(X, lin_reg_2.predict(X_poly), color = 'blue')\n",
    "plt.title('Truth or Bluff (Polynomial Regression)')\n",
    "plt.xlabel('Position level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Cross Validation to get the Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=0)\n",
    "\n",
    "# Define polynomial degrees to test\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Store cross-validation results\n",
    "cv_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    scores = cross_val_score(model, X, y, cv=10, scoring='neg_mean_squared_error')#neg_root_mean_squared_error\n",
    "    cv_scores.append(-np.mean(scores))  # Convert to positive MSE\n",
    "\n",
    "# Plot results\n",
    "plt.plot(degrees, cv_scores, marker='o')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Bias-Variance Tradeoff')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or \n",
    "# Import the relevant function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Get the cross validated scores for our baseline model\n",
    "baseline_cv = cross_val_score(baseline_model, X_train, y_train, scoring=\"neg_root_mean_squared_error\")\n",
    "\n",
    "# Display the average of the cross-validated scores\n",
    "baseline_cv_rmse = -(baseline_cv.mean())\n",
    "baseline_cv_rmse\n",
    "\n",
    "#after performing polynomial regression\n",
    "# Get the cross validated scores for our new model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Instantiate polynomial features transformer\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "# Fit transformer on entire X_train\n",
    "poly.fit(X_train)\n",
    "\n",
    "# Create transformed data matrix by transforming X_train\n",
    "X_train_poly = poly.transform(X_train)\n",
    "\n",
    "# Fit the model on the transformed data\n",
    "# Get the cross validated scores for our transformed features\n",
    "poly_cv = cross_val_score(baseline_model,X_train_poly, y_train, scoring=\"neg_root_mean_squared_error\")\n",
    "\n",
    "# Display the average of the cross-validated scores\n",
    "poly_cv_rmse = -(poly_cv.mean())\n",
    "poly_cv_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using train_test_split to validate the model- get the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use train_test_split to split the data into training and testing data\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=0)\n",
    "\n",
    "#train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "#build the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#predict the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#get the error\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after  performing polynomial regression\n",
    "# Instantiate polynomial features transformer\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "# Fit transformer on entire X_train\n",
    "poly.fit(X_train)\n",
    "\n",
    "# Create transformed data matrix by transforming X_train\n",
    "X_train_poly = poly.transform(X_train)\n",
    "\n",
    "# Fit the model on the transformed data\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "#get the error\n",
    "y_pred = model.predict(X_train_poly)\n",
    "rmse = mean_squared_error(y_train, y_pred, squared=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating bias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#write a function that calculates the bias of a model\n",
    "def bias(y, y_pred):\n",
    "   # Calculate the mean of the predictions\n",
    "    mean_y_pred = np.mean(y_pred)\n",
    "    \n",
    "    # Calculate the bias as the difference between the mean predictions and the mean actual values\n",
    "    bias_value = np.mean(mean_y_pred - y)\n",
    "    \n",
    "    return bias_value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variance\n",
    "def variance(y_pred):\n",
    "    mean_y_pred = np.mean(y_pred)\n",
    "    variance_value = np.mean((y_pred - mean_y_pred) ** 2)\n",
    "    return variance_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge and Lasso**\n",
    "1. **Ridge** - (L2 Regularization): Prevents overfitting by shrinking the coefficients to zero but doesn't set them exactly to zero.\n",
    "-  It’s useful when you believe that all the features have some impact on the target variable, but you want to reduce the magnitude of the coefficients to prevent overfitting.\n",
    "- Bias-Variance Tradeoff: Ridge regression reduces variance at the cost of introducing some bias.It makes the model less complex, thus reducing variance, but it introduces bias by not allowing the coefficients to fit the data as well as they could without.\n",
    "\n",
    "- Use Case: Ridge regression is often **used when you have many features that are somewhat correlated**, and you want to avoid overfitting while still keeping all features in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.**Lasso** (L1 regularization):\n",
    "-  Lasso regression shrinks the coefficients to zero and also set some of them exactly to zero. \n",
    "- **used for feature selection**, as it effectively removes features that are not strongly associated with the target variable.\n",
    "- Bias-Variance Tradeoff: Lasso reduces variance by shrinking the coefficients. However, because Lasso can set some coefficients to zero, it can increase bias by excluding some variables entirely from the model.\n",
    "\n",
    "- Use Case: Lasso is **useful when you have a large number of features, and you suspect that only a small subset of them are actually important for predicting the target variable.** It helps in feature selection by shrinking insignificant features' coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing Between Ridge and Lasso:\n",
    " * Use Ridge when:\n",
    "       -  You have many correlated features.\n",
    "       -  You believe most features should be retained in the model but need to control their influence.\n",
    " * Use Lasso when:\n",
    "       -  You suspect that only a few features are relevant.\n",
    "       -  You want a sparse model where some coefficients are exactly zero, effectively performing feature selection.\n",
    "\n",
    "* Combination (Elastic Net):\n",
    "\n",
    "    Elastic Net combines both Ridge and Lasso penalties. It’s useful when you want to balance the benefits of both methods, especially when you have many correlated features and you still want to perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cross_validation\n",
    "#1. cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "\n",
    "cross_val_score(linreg, X, y) #linreg is for linear regression by default it uses R^2 as scoring metric and k=5\n",
    "#k=5 is default value meaning 5 fold cross validation\n",
    "\n",
    "cross_val_score(linreg, X, y, cv=10, scoring='neg_mean_squared_error') #k=10 and scoring metric is mean squared error\n",
    "\n",
    "#2. using custom make_scorer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "scorer = make_scorer(mean_squared_error, greater_is_better=False) #greater_is_better=False means lower the value better the model\n",
    "\n",
    "cross_val_score(linreg, X, y, cv=10, scoring=scorer)\n",
    "#OR\n",
    "cross_val_score(linreg,X,y,scoring=make_scorer(mean_squared_error, greater_is_better=False))\n",
    "\n",
    "\n",
    "#3. using cross_validate\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cross_validate(linreg, X, y, cv=10) #but it also returns the timing.\n",
    "\n",
    "cross_validate(linreg, X, y, scoring=[\"r2\", \"neg_mean_squared_error\"]) #to get both r2 and mean squared error\n",
    " \n",
    " #And if you want to compare the train vs. test scores (e.g. to look for overfitting), that would look like this:\n",
    "\n",
    "cross_validate(linreg, X, y, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying the cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the mean of the scores\n",
    "cross_val_results = cross_validate(linreg, X, y, scoring=\"neg_mean_squared_error\", return_train_score=True)\n",
    "# Negative signs in front to convert back to MSE from -MSE\n",
    "train_avg = -cross_val_results[\"train_score\"].mean()\n",
    "test_avg = -cross_val_results[\"test_score\"].mean()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(labels, [train_avg, test_avg], color=colors)\n",
    "ax.set_ylabel(\"MSE\")\n",
    "fig.suptitle(\"Average Cross-Validation Scores\")\n",
    "\n",
    "#or to look at the distribution of scores, you could do a histogram\n",
    "\n",
    "cross_val_results = cross_validate(linreg, X, y, cv=100, scoring=\"neg_mean_squared_error\", return_train_score=True)\n",
    "train_scores = -cross_val_results[\"train_score\"]\n",
    "test_scores = -cross_val_results[\"test_score\"]\n",
    "\n",
    "fig, (left, right) = plt.subplots(ncols=2, figsize=(10,5), sharey=True)\n",
    "bins=25\n",
    "left.hist(train_scores, label=labels[0], bins=bins, color=colors[0])\n",
    "left.set_ylabel(\"Count\")\n",
    "left.set_xlabel(\"MSE\")\n",
    "right.hist(test_scores, label=labels[1], bins=bins, color=colors[1])\n",
    "right.set_xlabel(\"MSE\")\n",
    "fig.suptitle(\"Cross-Validation Score Distribution\")\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use Cross Validation and when to use Train_test_split for model validation\n",
    "- Cross validation is better than train_test_split because:\n",
    "1. it reduces dangers of overfitting that can be caused by train_test_split method.\n",
    "2. it is better with small dataset as it allows each data point to be used for both training and testing.\n",
    "- Train_test_split is used with large dataset.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
