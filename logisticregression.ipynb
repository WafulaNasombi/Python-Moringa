{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression - predictive modelling technique.\n",
    " - estimates the relationship between the dependent(target) and independent variable(predictor)\n",
    " - Types of regression:\n",
    " 1. Linear Regression \n",
    " 2. Logistic Regression \n",
    " 3. Polynomial Regression \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "- produces results in **binary format** which is used to predict the outcome of categorical dependent variable.So the outcome should be **discrete(binary)/categorical**\n",
    "- Possible outcomes:\n",
    "1. Yes Or No\n",
    "2. 0 OR 1 \n",
    "3. True OR False\n",
    "4. High OR Low\n",
    "\n",
    "- Threshold value : indicates the probability of winning(1)or losing(0) in the sigmoid curve. \n",
    "      - If value is >than threshold = output is rounded off to 1\n",
    "       - if value is < than threshold = output is rounded down to 0v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between linear and logistic regression \n",
    "1. Linear Regression \n",
    "     - y variable is continuous variables\n",
    "     - solves linear regression problems\n",
    "     - straight line graph.\n",
    "\n",
    "\n",
    "2. Logistic Regression\n",
    "      - y variable are categorical   variables.\n",
    "      - solves classification problems.\n",
    "      - S-Curve (Sigmoid Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression- Use Cases\n",
    "1. weather predictions - whether it will rain or not\n",
    "                     - snow or not.\n",
    "2. Classification problems - whether an item believes in a particular group or not. Eg if it's a bird or not.\n",
    "3. Determine illness -  identify if a patient is ill or not.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression/Classification using scikitlearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (1211594486.py, line 49)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 49\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "#logistic regression \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "#load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "print(data.head())\n",
    "\n",
    "#build the model\n",
    "logreg_model = LogisticRegression(C=1e5, fit_intercept=False, solver='liblinear')\n",
    "\"\"\"\n",
    "1. C - regularization parametric. Controls the strength of regularization to prevent overfitting.\n",
    "     - C is the inverse of the regularization strength (lambda in regularization terms).\n",
    "      -  A large value of C (like 1e12) means very weak regularization, allowing the model more freedom to fit the data without penalizing large coefficients. \n",
    "      - Conversely, a smaller value of C would imply stronger regularization, which could help prevent overfitting by constraining the model coefficients.\n",
    "  - Regularization is a technique used to prevent overfitting.We are doing inverse because the smaller the value of C, the higher the regularization strength.\n",
    "  - \n",
    "2.fit_intercept is set to False. It specifies whether a constant (a.k.a. bias or intercept) should be added to the decision function. If false the model \n",
    "assumes that the data is already centered at the origin.\n",
    "3. solver - specifies the algorithm to use in the optimization problem. Optimization problem is to minimize the cost function.\n",
    "          -liblinear is used for small datasets. \n",
    "          - Other algorithms include newton-cg, sag, saga, lbfgs.\n",
    "            - lbfgs is used for large datasets.\n",
    "            - sag and saga are faster for large datasets.\n",
    "            - newton-cg is used for multiclass problems.Mulitclass problems are problems with more than two classes.\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "#split the data\n",
    "X = data[['age', 'interest']]\n",
    "y = data['success']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "#train the model\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "#make predictions\n",
    "y_train_pred = logreg_model.predict(X_train)\n",
    "y_test_pred = logreg_model.predict(X_test)\n",
    "\n",
    "#evaluate the model\n",
    "print('Training accuracy: ', logreg_model.score(X_train, y_train))\n",
    "print('Testing accuracy: ', logreg_model.score(X_test, y_test))\n",
    "\n",
    "#to get the coefficients of the model\n",
    "logreg_model.coef_\n",
    "\n",
    "#to get the intercept of the mode\n",
    "logreg_model.intercept_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression using statsmodels.api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression using statsmodels.api \n",
    "import statsmodels.api as sm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#load the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "#split the data into training and testing data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size =0.3, random_state= 0)\n",
    "#build the model\n",
    "logreg_model = sm.Logit(X_train,y_train).fit()\n",
    "\n",
    "#print the summary \n",
    "print(logreg_model.summary())\n",
    "\n",
    "#output of the summary \n",
    "\"\"\"\n",
    "                            Logit Regression Results\n",
    "==============================================================================\n",
    "Dep. Variable:                success   No. Observations:                  700\n",
    "Model:                          Logit   Df Residuals:                      698\n",
    "Method:                           MLE   Df Model:                            1\n",
    "Date:                Fri, 27 Nov 2020   Pseudo R-squ.:                  0.9142\n",
    "Time:                        13:02:03   Log-Likelihood:                -46.290\n",
    "converged:                       True   LL-Null:                       -529.00\n",
    "Covariance Type:            nonrobust\n",
    "==============================================================================\n",
    "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "age            0.0005      0.008      0.067      0.947      -0.015       0.016\n",
    "interest       0.4984      0.086      5.782      0.000       0.329       0.668\n",
    "==============================================================================\n",
    "\n",
    "Dep Variable: The dependent variable in the model\n",
    "Model: the type of the model- Logisitic regression\n",
    "Method: The method used to fit the model. MLE stands for Maximum Likelihood Estimation\n",
    "No of Observations : The number of data points used in the model\n",
    "DF Residuals : Number of observations - number of predictions\n",
    "DF Model: Number of predictors in the model\n",
    "\n",
    "Fit Statistics:\n",
    "Pseudo R-Squared: A measure of how well the model fits the data.The closer to 1 the better the model\n",
    "age and interest: These are the predictors of the model \n",
    "age is not statistically significant as the p-value is greater than 0.05\n",
    "interest is statistically significant as the p-value is less than 0.05\n",
    "Thus the coefficient of interest means that for every unit increase in interest, the odds of success increase by 0.4984. The higher the the coefficient the higher the odds of success/ the event\n",
    "happening.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE - Maximum Likelihood Estimation\n",
    "- Method used to estimate the parameters(coefficients) of a logistic regression.\n",
    "- MLE finds the values of the coefficients that make the observed data most probable under the model.\n",
    "- The idea behind MLE is to find the parameter values that maximize the likelihood function, which represents the probability of the observed data given a set of parameters.\n",
    "\n",
    "* Likelihood Function: In logistic regression, the likelihood function is the product of probabilities of the observed outcomes, given the predictor variables and the model's coefficients.\n",
    "\n",
    "* Log-Likelihood: Because the likelihood function involves multiplying many probabilities, which can result in very small numbers, it's common to work with the log of the likelihood function. The log-likelihood is the sum of the logarithms of the individual probabilities. Maximizing the log-likelihood is equivalent to maximizing the likelihood.\n",
    "- The goal is to find the values of the coefficients β0,β1,…,βnβ0​,β1​,…,βn​ that maximize the log-likelihood function. This is typically done using numerical optimization techniques because the log-likelihood function is not linear and doesn't have a closed-form solution.\n",
    "   1. Gradient Descent - minimizing the cost function \n",
    "   2. Gradient Ascent - maximizing the log-likelihood  function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cost Function** - measures how well a model's prediction matches the real data. Types of cost function \n",
    " 1. RSS/SSR - Residual sum of Squares - it quantifies the error between the actual and the predicted values.\n",
    "   - RSS - this is basically the error(y-ypred), then square each error and sum the squared errors.\n",
    "   - Minimizing RSS: Helps find the best-fit model parameters in linear regression. The slope and intercept.\n",
    "\n",
    "     - The goal in linear regression is to find the intercept and slope(model parameter) that minimizes the cost function\n",
    "     - Minimizing the RSS ensures that the overall prediction error is minimized, leading to the best-fitting line through the data points. Thus even why \n",
    "     OLS (Ordinary Least Squares is used)- Least Squares refer to minimizing the sum of squared residuals\n",
    "\n",
    "     - Sensitive to outliers. As the squaring of outliers greatly increases the RSS\n",
    "     - USED WHEN: -  you need a straightforward measure of total squared error, often in theoretical contexts or when comparing models with consistent data points.\n",
    "2. MSE - Mean Squared Error.\n",
    "        -  Used in most regression problems, particularly when you need a differentiable cost function for optimization, and when average error per data point is of interest.\n",
    "3. MAE - Mean Absolute Error.- It is less sensitive to outliers compared to MSE and RSS.it is the absolute value of the errors |y - y_pred|\n",
    "                    - Used when you want to preserve the units. \n",
    "                    - Used when you want a cost function that’s robust to outliers, and you prefer interpretability with the same units as the data, or when dealing with problems where the median error is more relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use RSS to create a cost function\n",
    "def rss(m,X= X_train, y=y_train):\n",
    "    y_pred = m * X #m is the coefficient of the model. By coefficient of the model we mean the slope of the model\n",
    "    residual_sum_of_squares = np.sum((y_pred - y) ** 2)\n",
    "\n",
    "    return residual_sum_of_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent - Optimization algorithm. Used to minimize the Cost Function  RSS, MSE, MAE in Linear Regression\n",
    "- We use gradient descent instead of Gradient Ascent because in descent the minimum value is 0 while for ascent the maximum value is infinity thus getting to 0 is easier than getting to infinity.\n",
    "- In order to find the best model parameters.\n",
    "- How to conduct Gradient Descent:  - \n",
    "    1. Start with an initial guess for the parameters (e.g., m and b in linear regression).\n",
    "    2. Calculate the gradient (partial derivatives) of the cost function with respect to each parameter.\n",
    "    3. Adjust the parameters in the direction that reduces the cost function:\n",
    "       θnew=θold−α⋅∂J/∂θ\n",
    "      \n",
    "   Where:\n",
    "\n",
    "    θ represents the parameters (e.g., weights).\n",
    "    α is the learning rate (step size).\n",
    "    ∂J/∂θ is the gradient of the cost function\n",
    "\n",
    "    4. Repeat the gradient computation and parameter update steps until the cost function converges (i.e., changes very little between iterations) or a maximum number of iterations is reached.\n",
    "    5. When the gradient is near zero, the algorithm has found a local minimum, and the parameters are optimized.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the cost function and gradient descent to minimize the cost function\n",
    "#1. Set the initial parameters\n",
    "m = 1\n",
    "#step size/learning rate - refers to how much the parameter will be updated/adjusted in each iteration\n",
    "alpha = 0.5 # This the rate at which the model learns. The higher the learning rate the faster the model learns but the lower the learning rate the slower the model learns\n",
    "#number of iterations - the number of times the model will cycle through the data\n",
    "iterations = 1000\n",
    "\n",
    "#initialize the iteration counter\n",
    "iteration = 0 \n",
    "\n",
    "#set a precision value to determine when to stop the model. Precision is how close the model is to the true value\n",
    "precision = 0.000001\n",
    "\n",
    "\n",
    "\n",
    "#calculate the gradient descent\n",
    "while iteration < iterations:\n",
    "    #calculate the gradient of the cost function\n",
    "    gradient = np.sum(X_train * (m*X_train - y_train))#this is the derivative of the cost function\n",
    "    #update the parameters\n",
    "    m = m - alpha * gradient\n",
    "    #calculate the cost function\n",
    "    cost = rss(m)\n",
    "    #increment the iteration counter\n",
    "    iteration += 1\n",
    "    #print the cost function\n",
    "    print('Iteration: ', iteration, 'Cost: ', cost)\n",
    "    #check if the cost function has converged\n",
    "    \n",
    "\n",
    "#output of the cost function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent in Logistic regression \n",
    "- in logistic regression the cost function is based on MLE - Maximum likelihood Estimation. Maximizing the likelihood of observing the given data under a model.\n",
    "                \n",
    "- Cost function used here is log- loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent in logistic regression\n",
    " #1. start with initial guesses of the coefficients\n",
    "beta_1 = 1 # this is the slope of the model. from the equation y = beta_0 + beta_1 * x\n",
    "beta_0 = 1 # this is the intercept of the model\n",
    "alpha = 0.5 # this is the learning rate of the model\n",
    "iterations = 1000 # this is the number of iterations of the model\n",
    "precision = 0.000001 # this is the precision of the model\n",
    "\n",
    "#previous cost function\n",
    "previous_cost = float('inf') #setting the previous cost function to infinity. So that the model can start with a high cost function\n",
    "\n",
    "#2. set the sigmoid function. The sigmoid function is used to convert the linear model to a probability between 0 and 1. This is because the linear model can have any value between negative and positive infinity and \n",
    "#we are dealing with logistic regression.\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "#3. calculate the gradient descent\n",
    "iteration = 0\n",
    "while iteration < iterations:\n",
    "    #1.calculate the linear model\n",
    "    z = beta_0 + beta_1 * X_train\n",
    "    #2. calculate the predicted values\n",
    "    y_pred = sigmoid(z)\n",
    "    #3. calculate the gradient of the cost function\n",
    "    gradient_beta_0 = np.sum(y_pred - y_train)\n",
    "    gradient_beta_1 = np.sum(X_train * (y_pred - y_train))\n",
    "\n",
    "    #4. update the parameters\n",
    "    beta_0 = beta_0 - alpha * gradient_beta_0\n",
    "    beta_1 = beta_1 - alpha * gradient_beta_1\n",
    "\n",
    "    #5. calculate the cost function. The cost function is the log likelihood function. The log likelihood function is used to measure the probability of the model given the data. if the probability is high then the model is good\n",
    "    cost = -np.sum(y_train * np.log(y_pred) + (1 - y_train) * np.log(1 - y_pred))\n",
    "\n",
    "     # Check for convergence\n",
    "     \"\"\"Since previous_cost is set to infinity, the condition if abs(previous_cost - cost) < precision will not trigger the break on the first iteration because the difference will be very large (inf minus any finite number is still inf).\n",
    "\n",
    "     Subsequent Iterations: After the first iteration, previous_cost is updated to the cost from the previous iteration, and the difference between previous_cost and the current cost is compared. \n",
    "    If the difference is smaller than the defined precision, it indicates that the algorithm is converging, and the loop can be stopped.\n",
    "     \n",
    "     \"\"\"\n",
    "    if abs(previous_cost - cost) < precision: # If the difference between the previous cost and the current cost is less than the precision, break\n",
    "        break\n",
    "    previous_cost = cost # if the difference is not less than the precision, update the previous cost to the current cost\n",
    "    iteration += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification metrics\n",
    "- Gives data such as\n",
    "1. True Positive value \n",
    "2. False Positive value - value that is not supposed to be a positive. A negative value predicted as positive.\n",
    "3. True Negative\n",
    "4. False Negative - value that is not supposed to be a negative. A positive value predicted as negative.\n",
    "\n",
    "- In a confusion matrix the diagonals represent the true positives. This is because the row and column indexes are the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 1]\n",
      " [1 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TP': 4.0, 'FP': 1.0, 'TN': 4.0, 'FN': 1.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use sklearn to perform confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "actual_values =    [1,0,1,1,0,1,0,0,1,0]\n",
    "predicted_values = [1,0,1,0,0,1,1,0,1,0]\n",
    "\n",
    "print(confusion_matrix(actual_values, predicted_values))\n",
    "\n",
    "\n",
    "#doing confusion matrix manually using a function \n",
    "def conf_matrix(y_true,y_pred,num_classes=None):\n",
    "    #count the number of classes in the confusion matrix\n",
    "    if num_classes is None:\n",
    "        num_classes = len(np.unique(y_true))  #to get the number of unique classes in the true values\n",
    "        #or num_classes = max(max(y_true), max(y_pred)) + 1 #get the max of the y_true and y_pred, get the max of both then add 1 to get the number of classes\n",
    "\n",
    "    #initialize the confusion matrix\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes))#create a matrix of zeroes with num_classes as the rows and columns\n",
    "\n",
    "    #loop through the true and predicted values\n",
    "    for true,pred in zip(y_true, y_pred):\n",
    "        #increment the confusion matrix\n",
    "        confusion_matrix[true][pred] += 1 #confusion_matrix[true][pred] is the cell in the matrix that is being incremented. \n",
    "\n",
    "    #return the confusion matrix in form of a dictionary as TP, FP, TN, FN\n",
    "    return {\n",
    "        'TP': confusion_matrix[0][0],\n",
    "        'FP': confusion_matrix[1][0],\n",
    "        'TN': confusion_matrix[1][1],\n",
    "        'FN': confusion_matrix[0][1]    }\n",
    "y_test = [1,0,1,1,0,1,0,0,1,0]\n",
    "y_pred = [1,0,1,0,0,1,1,0,1,0]\n",
    "\n",
    "conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAG2CAYAAACNs6TQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuBUlEQVR4nO3de3QUdZr/8U8HSAdIOoIuCYGAuJEAAgmgYnAUWNHIeJSMv0XXZZaIwDmMoFy84iw3UePKMCheuIgaccyAwhA1okwGDIjEGQPEAQYyG0SJkoBzkIRkJIHu+v2BabcNl+5Ud5quer/OqT+qUt+qJzMcnzzP91tVDsMwDAEAAEuICncAAAAgeEjsAABYCIkdAAALIbEDAGAhJHYAACyExA4AgIWQ2AEAsBASOwAAFkJiBwDAQkjsAABYCIkdAIAQe/rpp+VwODRt2rRznvf222+rV69eiomJUb9+/bR+/fqA70ViBwAghD777DMtW7ZM/fv3P+d527Zt01133aXx48dr586dysrKUlZWlnbv3h3Q/Rx8BAYAgNCora3VwIED9dJLL+mJJ55Qenq6nn322TOee+edd6qurk4FBQXeY9dcc43S09O1dOlSv+/Z2mzQ4eTxeHTo0CHFxcXJ4XCEOxwAQIAMw9Dx48eVlJSkqKjQNZFPnDihhoYG09cxDKNJvnE6nXI6nWc8f/Lkybrllls0YsQIPfHEE+e8dnFxsWbMmOFzLDMzU/n5+QHFGNGJ/dChQ0pOTg53GAAAkyoqKtS1a9eQXPvEiRPq0T1WVUfcpq8VGxur2tpan2Nz5szR3Llzm5y7atUq7dixQ5999plf166qqlJCQoLPsYSEBFVVVQUUY0Qn9ri4OEnSVzsulSuW5QKwpl/07BfuEICQOaWT2qr13v+eh0JDQ4Oqjrj11fZL5Yprfq6oOe5R90FfqqKiQi6Xy3v8TNV6RUWFpk6dqsLCQsXExDT7ns0R0Ym9sR3iio0y9X8WcCFr7WgT7hCA0PlhlVdLTKfGxjkUG9f8+3j0Q85xuXwS+5ls375dR44c0cCBA73H3G63tmzZohdeeEH19fVq1aqVz5jExEQdPnzY59jhw4eVmJgYUJxkQwCALbgNj+nNXzfccIN27dql0tJS73bllVdqzJgxKi0tbZLUJSkjI0MbN270OVZYWKiMjIyAfs+IrtgBAPCXR4Y8av6DYIGMjYuLU9++fX2OtW/fXhdffLH3+NixY9WlSxfl5ORIkqZOnaqhQ4dq4cKFuuWWW7Rq1SqVlJRo+fLlAcVJxQ4AQBgcPHhQlZWV3v0hQ4YoLy9Py5cvV1pamtasWaP8/PwmfyCcDxU7AMAWPPLI/2b6mcebUVRUdM59SRo9erRGjx5t6j4kdgCALbgNQ24T72QzM7Yl0YoHAMBCqNgBALbQkovnwonEDgCwBY8MuW2Q2GnFAwBgIVTsAABboBUPAICFsCoeAABEHCp2AIAteH7YzIyPBCR2AIAtuE2uijcztiWR2AEAtuA2Tm9mxkcC5tgBALAQKnYAgC0wxw4AgIV45JBbDlPjIwGteAAALISKHQBgCx7j9GZmfCQgsQMAbMFtshVvZmxLohUPAICFULEDAGzBLhU7iR0AYAsewyGPYWJVvImxLYlWPAAAFkLFDgCwBVrxAABYiFtRcptoVLuDGEsokdgBALZgmJxjN5hjBwAALY2KHQBgC8yxAwBgIW4jSm7DxBx7hLxSllY8AAAWQsUOALAFjxzymKhnPYqMkp3EDgCwBbvMsdOKBwDAQqjYAQC2YH7xHK14AAAuGKfn2E18BIZWPAAAaGlU7AAAW/CYfFc8q+IBALiAMMcOAICFeBRli+fYmWMHAMBCqNgBALbgNhxym/j0qpmxLYnEDgCwBbfJxXNuWvEAAKClUbEDAGzBY0TJY2JVvIdV8QAAXDhoxQMAgIhDxQ4AsAWPzK1s9wQvlJAisQMAbMH8C2oio8kdGVECAAC/ULEDAGzB/LviI6MWJrEDAGzBLt9jJ7EDAGzBLhV7ZEQJAAD8QmIHANhC4wtqzGyBWLJkifr37y+XyyWXy6WMjAx98MEHZz0/NzdXDofDZ4uJiQn496QVDwCwBY/hkMfMc+wBju3atauefvppXX755TIMQ6+//rpGjRqlnTt36oorrjjjGJfLpbKyMu++wxF4vCR2AABC4NZbb/XZf/LJJ7VkyRJ9+umnZ03sDodDiYmJpu5LKx4AYAsek234xhfU1NTU+Gz19fXnvbfb7daqVatUV1enjIyMs55XW1ur7t27Kzk5WaNGjdKePXsC/j1J7AAAW2j8upuZTZKSk5MVHx/v3XJycs56z127dik2NlZOp1OTJk3SunXr1KdPnzOem5qaqldffVXvvPOOfve738nj8WjIkCH6+uuvA/o9acUDABCAiooKuVwu777T6TzruampqSotLVV1dbXWrFmj7Oxsbd68+YzJPSMjw6eaHzJkiHr37q1ly5Zp/vz5fsdHYgcA2IJbDrlNvGSmcWzjKnd/REdHKyUlRZI0aNAgffbZZ3ruuee0bNmy845t06aNBgwYoPLy8oDipBUPALCFYLXiTcXg8fg1Jy+dnpfftWuXOnfuHNA9qNgBAAiBmTNnauTIkerWrZuOHz+uvLw8FRUVacOGDZKksWPHqkuXLt45+scff1zXXHONUlJSdOzYMS1YsEBfffWVJkyYENB9SewAAFtwSyZb8YE5cuSIxo4dq8rKSsXHx6t///7asGGDbrzxRknSwYMHFRX1Yxfgu+++08SJE1VVVaUOHTpo0KBB2rZt21kX250NiR0AYAtm2+mBjn3llVfO+fOioiKf/UWLFmnRokWBhtUEiR0AYAt8BAYAAEQcKnYAgC0YJr/HbvA9dgAALhy04gEAQMShYgcA2EJLf7Y1XEjsAABbaPxKm5nxkSAyogQAAH6hYgcA2AKteAAALMSjKHlMNKrNjG1JkRElAADwCxU7AMAW3IZDbhPtdDNjWxKJHQBgC8yxAwBgIYbJr7sZvHkOAAC0NCp2AIAtuOWQ28SHXMyMbUkkdgCALXgMc/PkHiOIwYQQrXgAACyExI6ArX6+kzKT0rVkdpdwhwIERd/BtZr3+gHl7dijDYc+V8bN1eEOCSHg+WHxnJktElwQUb744ou69NJLFRMTo8GDB+svf/lLuEPCWZSVttX7v7tYPfp8H+5QgKCJaefRF3ti9MJjXcMdCkLII4fpLRKEPbGvXr1aM2bM0Jw5c7Rjxw6lpaUpMzNTR44cCXdo+Inv66L0P1O6a9qCCsXFu8MdDhA0JR+59PoznbXtw/hwhwKYFvbE/tvf/lYTJ07UuHHj1KdPHy1dulTt2rXTq6++Gu7Q8BMvPNZVV99Qo4HX14Y7FAAIWOOb58xskSCsib2hoUHbt2/XiBEjvMeioqI0YsQIFRcXhzEy/FRR/kUq39VW98ysDHcoANAsdpljD+vjbv/4xz/kdruVkJDgczwhIUH79u1rcn59fb3q6+u9+zU1NSGPEdKRb9poyewuylm1X9ExEfK8BwDYVEQ9x56Tk6N58+aFOwzbKf9rOx37RxtNzkz1HvO4Hdr1aXu9+9olKvjyc7VqFcYAAcAPHpl8V3yELJ4La2K/5JJL1KpVKx0+fNjn+OHDh5WYmNjk/JkzZ2rGjBne/ZqaGiUnJ4c8TrtLv+64lm3y7aAsnN5NySkndMfkIyR1ABHBMLmy3SCxn190dLQGDRqkjRs3KisrS5Lk8Xi0ceNGTZkypcn5TqdTTqezhaNEu1iPLu11wudYTDuP4jq4mxwHIlFMO7eSejR49xOTG3TZFd/r+LFW+vab6DBGhmDi624tZMaMGcrOztaVV16pq6++Ws8++6zq6uo0bty4cIcGwCZ6pn2vBWv3e/cnzTskSfrj6g5aOL1buMICmiXsif3OO+/Ut99+q9mzZ6uqqkrp6en68MMPmyyow4VlwdrycIcABM1fi2OVmZQW7jAQYmZXtrMqPgBTpkw5Y+sdAIBgsUsrPjL+/AAAAH65ICp2AABCzez73nncDQCACwiteAAAEHGo2AEAtmCXip3EDgCwBbskdlrxAABYCBU7AMAW7FKxk9gBALZgyNwja5Hy0WoSOwDAFuxSsTPHDgCAhVCxAwBswS4VO4kdAGALdknstOIBALAQKnYAgC3YpWInsQMAbMEwHDJMJGczY1sSrXgAACyEih0AYAt8jx0AAAuxyxw7rXgAACyExA4AsIXGxXNmtkAsWbJE/fv3l8vlksvlUkZGhj744INzjnn77bfVq1cvxcTEqF+/flq/fn3AvyeJHQBgC42teDNbILp27aqnn35a27dvV0lJif7t3/5No0aN0p49e854/rZt23TXXXdp/Pjx2rlzp7KyspSVlaXdu3cHdF+HYRiR8sGaJmpqahQfH6/v/n6ZXHH8jQJrykxKD3cIQMicMk6qSO+ourpaLpcrJPdozBWD1k5X6/bOZl/nVF29tv+/RaZi7dixoxYsWKDx48c3+dmdd96puro6FRQUeI9dc801Sk9P19KlS/2+B9kQAIAA1NTU+Gz19fXnHeN2u7Vq1SrV1dUpIyPjjOcUFxdrxIgRPscyMzNVXFwcUHwkdgCALRgm2/CNc+zJycmKj4/3bjk5OWe9565duxQbGyun06lJkyZp3bp16tOnzxnPraqqUkJCgs+xhIQEVVVVBfR78rgbAMAWDElmJp8bh1ZUVPi04p3Os7f3U1NTVVpaqurqaq1Zs0bZ2dnavHnzWZN7MJDYAQAIQOMqd39ER0crJSVFkjRo0CB99tlneu6557Rs2bIm5yYmJurw4cM+xw4fPqzExMSA4qMVDwCwhcY3z5nZTMfg8Zx1Tj4jI0MbN270OVZYWHjWOfmzoWIHANhCS38EZubMmRo5cqS6deum48ePKy8vT0VFRdqwYYMkaezYserSpYt3jn7q1KkaOnSoFi5cqFtuuUWrVq1SSUmJli9fHtB9SewAAITAkSNHNHbsWFVWVio+Pl79+/fXhg0bdOONN0qSDh48qKioHxvnQ4YMUV5env77v/9bjz32mC6//HLl5+erb9++Ad2XxA4AsAWP4ZCjBd8V/8orr5zz50VFRU2OjR49WqNHjw7oPj9FYgcA2IJhmFwVHyGvc2PxHAAAFkLFDgCwhZZePBcuJHYAgC2Q2AEAsJCWXjwXLsyxAwBgIVTsAABbsMuqeBI7AMAWTid2M3PsQQwmhGjFAwBgIVTsAABbYFU8AAAWYujHb6o3d3wkoBUPAICFULEDAGyBVjwAAFZik148iR0AYA8mK3ZFSMXOHDsAABZCxQ4AsAXePAcAgIXYZfEcrXgAACyEih0AYA+Gw9wCuAip2EnsAABbsMscO614AAAshIodAGAPvKDmR++++67fF7ztttuaHQwAAKFil1XxfiX2rKwsvy7mcDjkdrvNxAMAAEzwK7F7PJ5QxwEAQOhFSDvdDFNz7CdOnFBMTEywYgEAIGTs0ooPeFW82+3W/Pnz1aVLF8XGxuqLL76QJM2aNUuvvPJK0AMEACAojCBsESDgxP7kk08qNzdXzzzzjKKjo73H+/btqxUrVgQ1OAAAEJiAE/vKlSu1fPlyjRkzRq1atfIeT0tL0759+4IaHAAAweMIwnbhC3iO/ZtvvlFKSkqT4x6PRydPngxKUAAABJ1NnmMPuGLv06ePPv744ybH16xZowEDBgQlKAAA0DwBV+yzZ89Wdna2vvnmG3k8Hv3hD39QWVmZVq5cqYKCglDECACAeVTsZzZq1Ci99957+tOf/qT27dtr9uzZ2rt3r9577z3deOONoYgRAADzGr/uZmaLAM16jv26665TYWFhsGMBAAAmNfsFNSUlJdq7d6+k0/PugwYNClpQAAAEm10+2xpwYv/6669111136ZNPPtFFF10kSTp27JiGDBmiVatWqWvXrsGOEQAA85hjP7MJEybo5MmT2rt3r44ePaqjR49q79698ng8mjBhQihiBAAAfgq4Yt+8ebO2bdum1NRU77HU1FQ9//zzuu6664IaHAAAQWN2AZxVF88lJyef8UU0brdbSUlJQQkKAIBgcxinNzPjI0HArfgFCxbovvvuU0lJifdYSUmJpk6dqt/85jdBDQ4AgKCxyUdg/KrYO3ToIIfjxxZEXV2dBg8erNatTw8/deqUWrdurXvuuUdZWVkhCRQAAJyfX4n92WefDXEYAACEGHPsP8rOzg51HAAAhJZNHndr9gtqJOnEiRNqaGjwOeZyuUwFBAAAmi/gxXN1dXWaMmWKOnXqpPbt26tDhw4+GwAAFySbLJ4LOLE//PDD2rRpk5YsWSKn06kVK1Zo3rx5SkpK0sqVK0MRIwAA5tkksQfcin/vvfe0cuVKDRs2TOPGjdN1112nlJQUde/eXW+++abGjBkTijgBAIAfAq7Yjx49qssuu0zS6fn0o0ePSpJ+9rOfacuWLcGNDgCAYLHJZ1sDTuyXXXaZDhw4IEnq1auX3nrrLUmnK/nGj8IAAHChaXzznJktEgSc2MeNG6fPP/9ckvToo4/qxRdfVExMjKZPn66HHnoo6AECAAD/BZzYp0+frvvvv1+SNGLECO3bt095eXnauXOnpk6dGvQAAQAIihZePJeTk6OrrrpKcXFx6tSpk7KyslRWVnbOMbm5uXI4HD5bTExMQPc19Ry7JHXv3l3du3c3exkAACxl8+bNmjx5sq666iqdOnVKjz32mG666Sb97W9/U/v27c86zuVy+fwB8H9f6e4PvxL74sWL/b5gYzUPAMCFxCGTX3cL8PwPP/zQZz83N1edOnXS9u3bdf3115/9Pg6HEhMTmxHhaX4l9kWLFvl1MYfDQWIHAFhaTU2Nz77T6ZTT6TzvuOrqaklSx44dz3lebW2tunfvLo/Ho4EDB+qpp57SFVdc4Xd8fiX2xlXwF6pf9Oyn1o424Q4DCIkNh0rDHQIQMjXHPerQs4VuFqSPwCQnJ/scnjNnjubOnXvOoR6PR9OmTdO1116rvn37nvW81NRUvfrqq+rfv7+qq6v1m9/8RkOGDNGePXvUtWtXv8I0PccOAEBECNJHYCoqKny+i+JPtT558mTt3r1bW7duPed5GRkZysjI8O4PGTJEvXv31rJlyzR//ny/wiSxAwAQAJfLFdAHz6ZMmaKCggJt2bLF76q7UZs2bTRgwACVl5f7PSbgx90AAIhILfy4m2EYmjJlitatW6dNmzapR48eAYfsdru1a9cude7c2e8xVOwAAFsw+/a4QMdOnjxZeXl5eueddxQXF6eqqipJUnx8vNq2bStJGjt2rLp06aKcnBxJ0uOPP65rrrlGKSkpOnbsmBYsWKCvvvpKEyZM8Pu+JHYAAEJgyZIlkqRhw4b5HH/ttdd09913S5IOHjyoqKgfm+ffffedJk6cqKqqKnXo0EGDBg3Stm3b1KdPH7/v26zE/vHHH2vZsmXav3+/1qxZoy5duuiNN95Qjx499LOf/aw5lwQAILSCtHjO79ON8w8oKiry2V+0aJHfj5ifTcBz7GvXrlVmZqbatm2rnTt3qr6+XtLp5/OeeuopU8EAABAyNvkee8CJ/YknntDSpUv18ssvq02bH58dv/baa7Vjx46gBgcAAAITcCu+rKzsjK/Ci4+P17Fjx4IREwAAQdfSi+fCJeCKPTEx8YzP023dulWXXXZZUIICACDoGt88Z2aLAAEn9okTJ2rq1Kn685//LIfDoUOHDunNN9/Ugw8+qF/96lehiBEAAPNsMscecCv+0Ucflcfj0Q033KB//vOfuv766+V0OvXggw/qvvvuC0WMAADATwEndofDoV//+td66KGHVF5ertraWvXp00exsbGhiA8AgKCwyxx7s19QEx0dHdAD8wAAhFULP8ceLgEn9uHDh8vhOPsCgk2bNpkKCAAANF/AiT09Pd1n/+TJkyotLdXu3buVnZ0drLgAAAguk614y1bsZ3vV3dy5c1VbW2s6IAAAQsImrfigfbb1l7/8pV599dVgXQ4AADRD0L7uVlxcrJiYmGBdDgCA4LJJxR5wYr/99tt99g3DUGVlpUpKSjRr1qygBQYAQDDxuNtZxMfH++xHRUUpNTVVjz/+uG666aagBQYAAAIXUGJ3u90aN26c+vXrpw4dOoQqJgAA0EwBLZ5r1aqVbrrpJr7iBgCIPDZ5V3zAq+L79u2rL774IhSxAAAQMo1z7Ga2SBBwYn/iiSf04IMPqqCgQJWVlaqpqfHZAABA+Pg9x/7444/rgQce0M9//nNJ0m233ebzalnDMORwOOR2u4MfJQAAwRAhVbcZfif2efPmadKkSfroo49CGQ8AAKHBc+y+DOP0bzR06NCQBQMAAMwJ6HG3c33VDQCACxkvqDmDnj17nje5Hz161FRAAACEBK34pubNm9fkzXMAAODCEVBi/4//+A916tQpVLEAABAytOJ/gvl1AEBEs0kr3u8X1DSuigcAABcuvyt2j8cTyjgAAAgtm1TsAX+2FQCASMQcOwAAVmKTij3gj8AAAIALFxU7AMAebFKxk9gBALZglzl2WvEAAFgIFTsAwB5oxQMAYB204gEAQMShYgcA2AOteAAALMQmiZ1WPAAAFkLFDgCwBccPm5nxkYDEDgCwB5u04knsAABb4HE3AAAQcajYAQD2QCseAACLiZDkbAateAAALISKHQBgC3ZZPEdiBwDYg03m2GnFAwAQAjk5ObrqqqsUFxenTp06KSsrS2VlZecd9/bbb6tXr16KiYlRv379tH79+oDuS2IHANhCYyvezBaIzZs3a/Lkyfr0009VWFiokydP6qabblJdXd1Zx2zbtk133XWXxo8fr507dyorK0tZWVnavXt3AL+nYURIc6GpmpoaxcfHa5hGqbWjTbjDAUJiw6HScIcAhEzNcY869PxC1dXVcrlcobnHD7mi3/in1Co6ptnXcTec0K5XHmt2rN9++606deqkzZs36/rrrz/jOXfeeafq6upUUFDgPXbNNdcoPT1dS5cu9es+VOwAALSA6upqSVLHjh3Pek5xcbFGjBjhcywzM1PFxcV+34fFcwAAWwjWqviamhqf406nU06n85xjPR6Ppk2bpmuvvVZ9+/Y963lVVVVKSEjwOZaQkKCqqiq/46RiBwDYgxGETVJycrLi4+O9W05OznlvPXnyZO3evVurVq0K8i/VFBU7AMAegvS4W0VFhc8c+/mq9SlTpqigoEBbtmxR165dz3luYmKiDh8+7HPs8OHDSkxM9DtMKnYAAALgcrl8trMldsMwNGXKFK1bt06bNm1Sjx49znvtjIwMbdy40edYYWGhMjIy/I6Pih0AYAst/ea5yZMnKy8vT++8847i4uK88+Tx8fFq27atJGns2LHq0qWLt50/depUDR06VAsXLtQtt9yiVatWqaSkRMuXL/f7vlTsAAB7CNIcu7+WLFmi6upqDRs2TJ07d/Zuq1ev9p5z8OBBVVZWeveHDBmivLw8LV++XGlpaVqzZo3y8/PPueDup6jYAQAIAX9eE1NUVNTk2OjRozV69Ohm35fEDgCwBYdhyGHinWxmxrYkEjsAwB74CAwAAIg0VOwAAFvge+wAAFgJrXgAABBpqNgBALZAKx4AACuxSSuexA4AsAW7VOzMsQMAYCFU7AAAe6AVDwCAtURKO90MWvEAAFgIFTsAwB4M4/RmZnwEILEDAGyBVfEAACDiULEDAOyBVfEAAFiHw3N6MzM+EtCKBwDAQkjs8EvfwbWa9/oB5e3Yow2HPlfGzdXhDgkImdXPd1JmUrqWzO4S7lAQTEYQtggQ1sS+ZcsW3XrrrUpKSpLD4VB+fn44w8E5xLTz6Is9MXrhsa7hDgUIqbLStnr/dxerR5/vwx0KgqxxVbyZLRKENbHX1dUpLS1NL774YjjDgB9KPnLp9Wc6a9uH8eEOBQiZ7+ui9D9TumvaggrFxbvDHQ6CrfE5djNbBAjr4rmRI0dq5MiR4QwBALxeeKyrrr6hRgOvr9Xvnwt3NEDzRNSq+Pr6etXX13v3a2pqwhgNACspyr9I5bva6vn1fw93KAgRXlBzAcrJyVF8fLx3S05ODndIACzgyDdttGR2Fz3ywleKjomQ/3ojcDZZPBdRFfvMmTM1Y8YM735NTQ3JHYBp5X9tp2P/aKPJmaneYx63Q7s+ba93X7tEBV9+rlatwhggEICISuxOp1NOpzPcYQCwmPTrjmvZpn0+xxZO76bklBO6Y/IRkrpF2KUVH1GJHeET086tpB4N3v3E5AZddsX3On6slb79JjqMkQHmtYv16NJeJ3yOxbTzKK6Du8lxRDC+7hZ6tbW1Ki8v9+4fOHBApaWl6tixo7p16xbGyPBTPdO+14K1+737k+YdkiT9cXUHLZzO/1cAcKEIa2IvKSnR8OHDvfuN8+fZ2dnKzc0NU1Q4k78WxyozKS3cYQAtZsHa8vOfhIhCK74FDBs2TEaEtDYAABHOJl93i6jH3QAAwLmxeA4AYAu04gEAsBKPcXozMz4CkNgBAPbAHDsAAIg0VOwAAFtwyOQce9AiCS0SOwDAHmzy5jla8QAAWAgVOwDAFnjcDQAAK2FVPAAAiDRU7AAAW3AYhhwmFsCZGduSSOwAAHvw/LCZGR8BaMUDAGAhVOwAAFugFQ8AgJXYZFU8iR0AYA+8eQ4AAEQaKnYAgC3w5jkAAKyEVjwAAGiuLVu26NZbb1VSUpIcDofy8/PPeX5RUZEcDkeTraqqKqD7UrEDAGzB4Tm9mRkfiLq6OqWlpemee+7R7bff7ve4srIyuVwu736nTp0Cui+JHQBgDy3cih85cqRGjhwZ8G06deqkiy66KOBxjWjFAwAQgJqaGp+tvr4+qNdPT09X586ddeONN+qTTz4JeDyJHQBgD0YQNknJycmKj4/3bjk5OUEJr3Pnzlq6dKnWrl2rtWvXKjk5WcOGDdOOHTsCug6teACALQTrlbIVFRU+c+BOp9N0bJKUmpqq1NRU7/6QIUO0f/9+LVq0SG+88Ybf1yGxAwAQAJfL5ZPYQ+nqq6/W1q1bAxpDYgcA2EMEPsdeWlqqzp07BzSGxA4AsAdD5r6pHmBer62tVXl5uXf/wIEDKi0tVceOHdWtWzfNnDlT33zzjVauXClJevbZZ9WjRw9dccUVOnHihFasWKFNmzbpj3/8Y0D3JbEDAGyhpT/bWlJSouHDh3v3Z8yYIUnKzs5Wbm6uKisrdfDgQe/PGxoa9MADD+ibb75Ru3bt1L9/f/3pT3/yuYY/SOwAAITAsGHDZJzjj4Hc3Fyf/YcfflgPP/yw6fuS2AEA9mDI5Bx70CIJKRI7AMAeInDxXHPwghoAACyEih0AYA8eSQ6T4yMAiR0AYAstvSo+XGjFAwBgIVTsAAB7sMniORI7AMAebJLYacUDAGAhVOwAAHuwScVOYgcA2AOPuwEAYB087gYAACIOFTsAwB6YYwcAwEI8huQwkZw9kZHYacUDAGAhVOwAAHugFQ8AgJWYTOyKjMROKx4AAAuhYgcA2AOteAAALMRjyFQ7nVXxAACgpVGxAwDswfCc3syMjwAkdgCAPTDHDgCAhTDHDgAAIg0VOwDAHmjFAwBgIYZMJvagRRJStOIBALAQKnYAgD3QigcAwEI8HkkmnkX3RMZz7LTiAQCwECp2AIA90IoHAMBCbJLYacUDAGAhVOwAAHuwyStlSewAAFswDI8ME19oMzO2JZHYAQD2YBjmqm7m2AEAQEujYgcA2INhco49Qip2EjsAwB48HslhYp48QubYacUDAGAhVOwAAHugFQ8AgHUYHo8ME634SHncjVY8AAAWQsUOALAHWvEAAFiIx5Ac1k/stOIBALAQKnYAgD0YhiQzz7FHRsVOYgcA2ILhMWSYaMUbJHYAAC4ghkfmKnYedwMAwLa2bNmiW2+9VUlJSXI4HMrPzz/vmKKiIg0cOFBOp1MpKSnKzc0N+L4kdgCALRgew/QWiLq6OqWlpenFF1/06/wDBw7olltu0fDhw1VaWqpp06ZpwoQJ2rBhQ0D3pRUPALCHFm7Fjxw5UiNHjvT7/KVLl6pHjx5auHChJKl3797aunWrFi1apMzMTL+vE9GJvXEhwymdNPXOAeBCVnM8Mub1gOaoqT3977slFqaZzRWndFKSVFNT43Pc6XTK6XSaCU2SVFxcrBEjRvgcy8zM1LRp0wK6TkQn9uPHj0uStmp9mCMBQqdDz3BHAITe8ePHFR8fH5JrR0dHKzExUVurzOeK2NhYJScn+xybM2eO5s6da/raVVVVSkhI8DmWkJCgmpoaff/992rbtq1f14noxJ6UlKSKigrFxcXJ4XCEOxxbqKmpUXJysioqKuRyucIdDhBU/PtueYZh6Pjx40pKSgrZPWJiYnTgwAE1NDSYvpZhGE3yTTCq9WCK6MQeFRWlrl27hjsMW3K5XPyHD5bFv++WFapK/f+KiYlRTExMyO9jRmJiog4fPuxz7PDhw3K5XH5X6xKr4gEAuCBkZGRo48aNPscKCwuVkZER0HVI7AAAhEBtba1KS0tVWloq6fTjbKWlpTp48KAkaebMmRo7dqz3/EmTJumLL77Qww8/rH379umll17SW2+9penTpwd0XxI7AuJ0OjVnzpwLbk4JCAb+fSOYSkpKNGDAAA0YMECSNGPGDA0YMECzZ8+WJFVWVnqTvCT16NFD77//vgoLC5WWlqaFCxdqxYoVAT3qJkkOI1JefgsAAM6Lih0AAAshsQMAYCEkdgAALITEDgCAhZDY4bcXX3xRl156qWJiYjR48GD95S9/CXdIQFA05/OawIWKxA6/rF69WjNmzNCcOXO0Y8cOpaWlKTMzU0eOHAl3aIBpgX5eE7iQ8bgb/DJ48GBdddVVeuGFFyRJHo9HycnJuu+++/Too4+GOTogeBwOh9atW6esrKxwhwI0CxU7zquhoUHbt2/3+ZxgVFSURowYoeLi4jBGBgD4KRI7zusf//iH3G73GT8nWFVVFaaoAABnQmIHAMBCSOw4r0suuUStWrU64+cEExMTwxQVAOBMSOw4r+joaA0aNMjnc4Iej0cbN24M+HOCAIDQah3uABAZZsyYoezsbF155ZW6+uqr9eyzz6qurk7jxo0Ld2iAabW1tSovL/fuN35es2PHjurWrVsYIwMCx+Nu8NsLL7ygBQsWqKqqSunp6Vq8eLEGDx4c7rAA04qKijR8+PAmx7Ozs5Wbm9vyAQEmkNgBALAQ5tgBALAQEjsAABZCYgcAwEJI7AAAWAiJHQAACyGxAwBgISR2AAAshMQOmHT33Xf7fLt72LBhmjZtWovHUVRUJIfDoWPHjp31HIfDofz8fL+vOXfuXKWnp5uK68svv5TD4VBpaamp6wDwD4kdlnT33XfL4XDI4XAoOjpaKSkpevzxx3Xq1KmQ3/sPf/iD5s+f79e5/iRjAAgE74qHZd1888167bXXVF9fr/Xr12vy5Mlq06aNZs6c2eTchoYGRUdHB+W+HTt2DMp1AKA5qNhhWU6nU4mJierevbt+9atfacSIEXr33Xcl/dg+f/LJJ5WUlKTU1FRJUkVFhe644w5ddNFF6tixo0aNGqUvv/zSe023260ZM2booosu0sUXX6yHH35YP30r809b8fX19XrkkUeUnJwsp9OplJQUvfLKK/ryyy+97yfv0KGDHA6H7r77bkmnv56Xk5OjHj16qG3btkpLS9OaNWt87rN+/Xr17NlTbdu21fDhw33i9Ncjjzyinj17ql27drrssss0a9YsnTx5ssl5y5YtU3Jystq1a6c77rhD1dXVPj9fsWKFevfurZiYGPXq1UsvvfRSwLEACA4SO2yjbdu2amho8O5v3LhRZWVlKiwsVEFBgU6ePKnMzEzFxcXp448/1ieffKLY2FjdfPPN3nELFy5Ubm6uXn31VW3dulVHjx7VunXrznnfsWPH6ve//70WL16svXv3atmyZYqNjVVycrLWrl0rSSorK1NlZaWee+45SVJOTo5WrlyppUuXas+ePZo+fbp++ctfavPmzZJO/wFy++2369Zbb1VpaakmTJigRx99NOD/TeLi4pSbm6u//e1veu655/Tyyy9r0aJFPueUl5frrbfe0nvvvacPP/xQO3fu1L333uv9+ZtvvqnZs2frySef1N69e/XUU09p1qxZev311wOOB0AQGIAFZWdnG6NGjTIMwzA8Ho9RWFhoOJ1O48EHH/T+PCEhwaivr/eOeeONN4zU1FTD4/F4j9XX1xtt27Y1NmzYYBiGYXTu3Nl45plnvD8/efKk0bVrV++9DMMwhg4dakydOtUwDMMoKyszJBmFhYVnjPOjjz4yJBnfffed99iJEyeMdu3aGdu2bfM5d/z48cZdd91lGIZhzJw50+jTp4/Pzx955JEm1/opSca6devO+vMFCxYYgwYN8u7PmTPHaNWqlfH11197j33wwQdGVFSUUVlZaRiGYfzrv/6rkZeX53Od+fPnGxkZGYZhGMaBAwcMScbOnTvPel8AwcMcOyyroKBAsbGxOnnypDwej/7zP/9Tc+fO9f68X79+PvPqn3/+ucrLyxUXF+dznRMnTmj//v2qrq5WZWWlz6dqW7durSuvvLJJO75RaWmpWrVqpaFDh/odd3l5uf75z3/qxhtv9Dne0NCgAQMGSJL27t3b5JO5GRkZft+j0erVq7V48WLt379ftbW1OnXqlFwul8853bp1U5cuXXzu4/F4VFZWpri4OO3fv1/jx4/XxIkTveecOnVK8fHxAccDwDwSOyxr+PDhWrJkiaKjo5WUlKTWrX3/ubdv395nv7a2VoMGDdKbb77Z5Fr/8i//0qwY2rZtG/CY2tpaSdL777/vk1Cl0+sGgqW4uFhjxozRvHnzlJmZqfj4eK1atUoLFy4MONaXX365yR8arVq1ClqsAPxHYodltW/fXikpKX6fP3DgQK1evVqdOnVqUrU26ty5s/785z/r+uuvl3S6Mt2+fbsGDhx4xvP79esnj8ejzZs3a8SIEU1+3tgxcLvd3mN9+vSR0+nUwYMHz1rp9+7d27sQsNGnn356/l/y/9i2bZu6d++uX//6195jX331VZPzDh48qEOHDikpKcl7n6ioKKWmpiohIUFJSUn64osvNGbMmIDuDyA0WDwH/GDMmDG65JJLNGrUKH388cc6cOCAioqKdP/99+vrr7+WJE2dOlVPP/208vPztW/fPt17773nfAb90ksvVXZ2tu655x7l5+d7r/nWW29Jkrp37y6Hw6GCggJ9++23qq2tVVxcnB588EFNnz5dr7/+uvbv368dO3bo+eef9y5ImzRpkv73f/9XDz30kMrKypSXl6fc3NyAft/LL79cBw8e1KpVq7R//34tXrz4jAsBY2JilJ2drc8//1wff/yx7r//ft1xxx1KTEyUJM2bN085OTlavHix/v73v2vXrl167bXX9Nvf/jageAAEB4kd+EG7du20ZcsWdevWTbfffrt69+6t8ePH68SJE94K/oEHHtB//dd/KTs7WxkZGYqLi9MvfvGLc153yZIl+vd//3fde++96tWrlyZOnKi6ujpJUpcuXTRv3jw9+uijSkhI0JQpUyRJ8+fP16xZs5STk6PevXvr5ptv1vvvv68ePXpIOj3vvXbtWuXn5ystLU1Lly7VU089FdDve9ttt2n69OmaMmWK0tPTtW3bNs2aNavJeSkpKbr99tv185//XDfddJP69+/v8zjbhAkTtGLFCr322mvq16+fhg4dqtzcXG+sAFqWwzjbqh8AABBxqNgBALAQEjsAABZCYgcAwEJI7AAAWAiJHQAACyGxAwBgISR2AAAshMQOAICFkNgBALAQEjsAABZCYgcAwEJI7AAAWMj/BxPNCSD0YqTGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#using confusion matrix display to display the confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics used in Classification \n",
    "1. Precision - measures the **ACCURACY** of positive predictions.\n",
    "    - precision = true positive / (true positive + false positive)\n",
    "          - eg precision allows us to answer the questions:\n",
    "    - out of all the times the model predicted the person has the disease, how many times did the person in question actually have the disease.\n",
    "    - A model can have a high precision score but it doesn't mean it's a high performing model because:\n",
    "        - With a total of 5000 it can predict that 4000 have the disease of which 3000are correct predictions but  give a precision score of 89% while another model can predict 5 to have the disease out of the 5000 of which all the 5 are correct predictions and give a 100% precision \n",
    "2. Recall - measures **COMPLETENESS** of the positive predictions. What percentage of classes we were interested in are actually captured.\n",
    "         recall = true positive / (true positive + false negative)\n",
    "    - Out of all the patients we saw that actually had the disease, what percentage of them did our model correctly identify as having the disease?\"\n",
    "    - Precision and recall have an inverse relationship. As precision goes up recall goes down and vice versa.\n",
    "\n",
    "\n",
    "## Describing the performance of the model\n",
    "1. Accuracy - out of all the predictions(whether true positives and true negatives), what percentage is correct.\n",
    "    -  Accuracy = (true positive + true negative) /total observations\n",
    "2. F1 score/ Harmonic mean of both Precision and Recall - if both the precision and recall are high even the F1 score will be high. If the F1 score is \n",
    "   high then your model is doing well around.\n",
    "   - F1 score = 2( (precision * recall)/ (precision + recall) )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using sklearn.metrics import classification_report() it will give you the precision, recall, F1 score and support(number of occurrences of each label in y_true) for the results of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AND AUC\n",
    "- ROC : Receiver Operating Characteristic Curves- ROC graphs allow us to determine optimal precision-recall tradeoff balances specific to the problem you are looking to solve.\n",
    "- AUC : Area under the curve - are an alternative to confusion matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves\n",
    "- illustrates the true positive rate against the false positive rate.\n",
    "- TPR = RECALL = TP/ (TP + FN)\n",
    "- FPR = FP/ (FP + TN)\n",
    "\n",
    "- When training a classifier, the best performing models will have an ROC curve that hugs the upper left corner of the graph. \n",
    "- A classifier with 50-50 accuracy is deemed 'worthless'; this is no better than random guessing, as in the case of a coin flip( LINEAR LINE)\n",
    "- Models with poor ROC might have large overlaps in the probability estimates for the two classes. This would indicate that the algorithm performed poorly and had difficulty separating the two classes from each other.\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ROC curve is plotted with the False Positive Rate (FPR) on the x-axis and the True Positive Rate (TPR) on the y-axis. The curve shows the trade-off between sensitivity (true positive rate) and specificity (1 - false positive rate) across different thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC\n",
    "-  An AUC of 1 being a perfect classifier, and an AUC of 0.5 being that which has a precision of 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AUC is the area under the ROC curve. It quantifies the overall ability of the model to discriminate between the positive and negative classes. The AUC value ranges from 0 to 1, where:\n",
    "\n",
    "    AUC = 1: Perfect classifier\n",
    "    AUC = 0.5: Classifier with no discrimination ability (equivalent to random guessing)\n",
    "    AUC < 0.5: Worse than random guessing (rarely seen in practice, typically due to model or data issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- While normalizing data is not strictly necessary for logistic regression, it is a good practice, especially when features have different scales or units. MinMax scaling is one of the ways to normalize data, but you could also use standardization or other methods depending on your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#graph the ROC curve\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Define appropriate X and y\n",
    "y = df['target']\n",
    "X = df.drop(columns='target', axis=1)\n",
    "\n",
    "# Normalize the Data\n",
    "X = X.apply(lambda x : (x - x.min()) /(x.max() - x.min()),axis=0)\n",
    "\n",
    "# Split the data into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y) # stratify=y ensures that both the y_train and y_test have the same proportion\n",
    "\n",
    "# Fit a model\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "print(logreg) # Preview model params\n",
    "\n",
    "# Predict\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "# Data preview\n",
    "print(\"\")\n",
    "df.head()\n",
    "\n",
    "# Get the predicted probabilities - this is the probability of the model predicting 1 from X_test\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# False Positive Rate, True Positive Rate, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "# Area Under the Curve\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "lw = 2 # Line width\n",
    "plt.plot(fpr, tpr, color='darkorange', lw = lw, label = 'ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color = 'navy', lw = lw, linestyle = '--') # Chance line\n",
    "plt.xlim([0.0, 1.0]) # x-axis limits\n",
    "plt.ylim([0.0, 1.05]) # y-axis limits\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc = \"lower right\") #legend is the box that shows the labels of the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this lesson, we investigated another evaluation metric for classification algorithms (including logistic regression). Namely, the Receiver Operating Characteristic curve (ROC) which graphs the False Positive Rate against the True Positive Rate. The overall accuracy of a classifier can thus be quantified by the AUC, the Area Under the Curve. Perfect classifiers would have an AUC score of 1.0 while an AUC of 0.5 is deemed trivial or worthless. Next, you're going to get more practice graphing the ROC and AUC curves and making interpretations based on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class imbalance\n",
    "- 1. Biased Predictions :  For example, if 95% of the data belongs to Class A and 5% belongs to Class B, a model could achieve 95% accuracy simply by predicting Class A all the time, completely ignoring Class B.\n",
    "\n",
    "2. oor Generalization: The model tends to learn patterns only for the majority class and fails to generalize well for the minority class. This means that the model's performance is not balanced across all classes.\n",
    "\n",
    "3. Misleading Metrics: Accuracy can be a misleading metric in imbalanced datasets, as it does not account for the number of false negatives or false positives. Metrics like precision, recall, and the F1-score provide a better picture in such cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO ADDRESS class imbalance \n",
    "1. Use class weights in logistic Regression\n",
    "- This involves assigning a higher weight to the minority class, effectively telling the model that misclassifying the minority class is more costly than misclassifying the majority class.\n",
    "\n",
    "- how class weights work? \n",
    "  - Balanced Weights: The weight for each class can be inversely proportional to its frequency in the dataset. This can be done automatically in scikit-learn by setting class_weight='balanced'. This approach adjusts the weights inversely proportional to the class frequencies in the input data.\n",
    "\n",
    "   - Manual Weights: Alternatively, you can specify the class weights manually, giving more weight to the minority class based on domain knowledge or specific business requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report without Class Weights:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       226\n",
      "           1       0.95      0.88      0.91        24\n",
      "\n",
      "    accuracy                           0.98       250\n",
      "   macro avg       0.97      0.94      0.95       250\n",
      "weighted avg       0.98      0.98      0.98       250\n",
      "\n",
      "Classification Report with Class Weights:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       226\n",
      "           1       0.67      0.92      0.77        24\n",
      "\n",
      "    accuracy                           0.95       250\n",
      "   macro avg       0.83      0.93      0.87       250\n",
      "weighted avg       0.96      0.95      0.95       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#balancing class weights to reduce class imbalance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dataset with class imbalance\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10,\n",
    "                           n_clusters_per_class=1, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
    "\n",
    "# Split the data into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Fit a model with no class weights\n",
    "logreg = LogisticRegression( random_state=42)\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict with no class weights\n",
    "y_pred_no_weights = model_log.predict(X_test)\n",
    "\n",
    "print(\"Classification Report without Class Weights:\")\n",
    "print(classification_report(y_test, y_pred_no_weights))\n",
    "\n",
    "# Train a model with class weights\n",
    "logreg_balanced = LogisticRegression( class_weight='balanced', random_state=42)\n",
    "\n",
    "model_balanced = logreg_balanced.fit(X_train, y_train)\n",
    "\n",
    "# Predict with class weights\n",
    "y_pred_balanced = model_balanced.predict(X_test)\n",
    "\n",
    "print(\"Classification Report with Class Weights:\")\n",
    "print(classification_report(y_test, y_pred_balanced))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results\n",
    "  1. Without Class Weights:\n",
    "        You might notice high accuracy but poor recall (sensitivity) for the minority class (Class 1). This means that the model is not identifying many true positives for Class 1 because it is biased toward predicting the majority class (Class 0).\n",
    "  2. With Balanced Class Weights:\n",
    "        The model is more likely to correctly identify instances of the minority class (Class 1), resulting in better recall for Class 1. The precision for Class 0 might decrease slightly because the model is now penalized more for misclassifying the minority class, leading to a better balance between the classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This method is particularly useful when the cost of missing true positives (false negatives) is high, as in medical diagnoses or fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Address class imbalance by \n",
    "- **Oversampling and Undersampling**: oversampling the minority class or undersampling the majority class can help by producing a synthetic dataset that the learning algorithm is trained on\n",
    "- Undersampling can only be used when you have a truly massive dataset and can afford to lose data points. However, even with very large datasets, you are losing potentially useful data. Oversampling can run into the issue of overfitting to certain characteristics of certain data points because there will be exact replicas of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. More advanced technique for addressing class imbalance is :\n",
    "- **SMOTE - Synthetic Minority Oversampling**: rather then simply oversampling the minority class with replacement (which simply adds duplicate cases to the dataset), the algorithm generates new sample data by creating 'synthetic' examples that are combinations of the closest minority class cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Previous original class distribution\n",
    "print('Original class distribution: \\n')\n",
    "print(y.value_counts())\n",
    "smote = SMOTE(random_state= 42, sampling_strategy=0.28)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) \n",
    "# Preview synthetic sample class distribution\n",
    "print('-----------------------------------------')\n",
    "print('Synthetic sample class distribution: \\n')\n",
    "print(pd.Series(y_train_resampled).value_counts()) \n",
    "\n",
    "\"\"\"\n",
    "oUTPUT\n",
    "Original class distribution: \n",
    "\n",
    "0    99773\n",
    "1      227\n",
    "Name: is_attributed, dtype: int64\n",
    "-----------------------------------------\n",
    "Synthetic sample class distribution: \n",
    "\n",
    "1    74841\n",
    "0    74841\n",
    "Name: is_attributed, dtype: int64\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for class imbalance while previewing the data\n",
    "column.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree- a decision tree is a tree like model that makes decisions based on the features of the data. It is a supervised learning algorithm that can be used for both classification and regression problems.\n",
    "#decision trees are easy to understand and interpret. They are also non-parametric models which means they do not make any assumptions about the data. They can handle both numerical and categorical data.\n",
    "#decision trees are prone to overfitting. This is because they can keep splitting the data until each leaf node has only one data point. This can be prevented by pruning the tree or using ensemble methods like random forests.\n",
    "#decision trees are also sensitive to the data. A small change in the data can lead to a completely different tree. This can be prevented by using ensemble methods like random forests.    \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Define appropriate X and y\n",
    "y = df['target']\n",
    "X = df.drop(columns='target', axis=1)\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)\n",
    "\n",
    "#Normalize the data\n",
    "X_train = X_train.apply(lambda x : (x - x.min()) /(x.max() - x.min()),axis=0)\n",
    "X_test = X_test.apply(lambda x : (x - x.min()) /(x.max() - x.min()),axis=0)\n",
    "\n",
    "# Fit a model\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_hat_test = clf.predict(X_test)\n",
    "\n",
    "# Data preview\n",
    "print(\"\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chika",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
