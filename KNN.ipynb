{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K - Nearest Neighbors:(supervised learning)- the distance metric is used to find the K closest points to the given data point.\n",
    "- An effective classification and regression algorithm that uses nearby points to generate predictions.\n",
    "- The K-Nearest Neighbors algorithm works as follows:\n",
    "\n",
    "1. Choose a point\n",
    "2. Find the K-nearest points\n",
    "       1. K is a predefined user constant such as 1, 3, 5, or 11\n",
    "3. Predict a label for the current point:\n",
    "     1. Classification - Take the most common class of the k neighbors\n",
    "    2. Regression - Take the average target metric of the k neighbors\n",
    "    3. Both classification or regression can also be modified to use weighted averages based on the distance of the neighbors\n",
    "\n",
    "- An incredibly important decision when using the KNN algorithm is determining an appropriate distance metric. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K- means algorithm:- the distance metric groups data points together.\n",
    "- serves as an unsupervised learning clustering algorithm. \n",
    "-  K represents the number of clusters rather then the number of neighbors.\n",
    "- K-means is an iterative algorithm which repeats until convergence.\n",
    "- Nonetheless, its underlying principle is the same, in that it groups data points together using a distance metric in order to create homogeneous groupings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics- measure the distance between two points\n",
    "1. Manhattan distance- btwn two points\n",
    "2. Euclidean distance - btwn two points\n",
    "3. Minkowski distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties each distance metrics should satisfy:\n",
    "1. **Symmetry :** If x and y are two points in a metric space, then the distance between  x and y should be equal to the distance between  y and x.\n",
    "2. **Non-negativity:** Distances should always be non negative. Meaning it should be greater than or equal to zero.\n",
    "\n",
    "3. **Triangle inequality:** Given three points x, y, and z, the distance metric should satisfy the triangle inequality: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Manhattan Distance :(cityblock/taxicab)\n",
    "1. It can be calculated for continuous data.\n",
    "2. It is always non-negative.\n",
    "3. Can be more appropriate when dealing with categorical and high dimensionality data.\n",
    "- Measures the distance from one point to another traveling along the axes of the grid.\n",
    "- Get distance just like Euclidean we use the pythagoras theorem but do not use hypotenuse. \n",
    "- Instead we add distance of a to b and b to c\n",
    "- Thus distance = |(x2 - x1)| + |(y2-y1)| and \n",
    "    | -> represents the absolute value.\n",
    "\n",
    "- Where (x2,y2,z2) are for the second point q and (x1,y1,z1) are for the first point p which we are getting the distance in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#manhattan distance \n",
    "def manhattan_distance(x1, y1, x2, y2):\n",
    "    return abs(x1 - x2) + abs(y1 - y2)\n",
    "\n",
    "#call the function \n",
    "print(manhattan_distance(1, 2, 3, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "#using scipy \n",
    "from scipy.spatial import distance\n",
    "\n",
    "x =[3,6,9]\n",
    "y =[1,2,3]\n",
    "\n",
    "print(distance.cityblock(x,y)) #manhattan distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Euclidean Distance:(shortest distance(hypotenuse))\n",
    "1. Used with continuous data and data that doesn't have high dimensionality \n",
    "- Calculated based on the pythagoras theorem\n",
    "- formula : AC^2 = AB^2 + BC^2\n",
    "          : AC = sqrt(AB^2 + BC^2)\n",
    "\n",
    "  - Hence: hypotenuse = sqrt((x2-x1)^2 + (y2-y1)^2)if 3 axes then add (z2- z1)^2\n",
    "  - Where (x2,y2,z2) are for the second point q and (x1,y1,z1) are for the first point p which we are getting the distance in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To determine which distance method to use you can use cross validation from sklearn or hypeparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8284271247461903\n"
     ]
    }
   ],
   "source": [
    "#euclidean distance\n",
    "def euclidean_distance(x1,y1,x2,y2):\n",
    "    #get the square root of the sum of the squares of the differences of the coordinates\n",
    "    return ((x1-x2)**2 + (y1-y2)**2)**0.5\n",
    "\n",
    "#call the function\n",
    "print(euclidean_distance(1, 2, 3, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.123105625617661\n"
     ]
    }
   ],
   "source": [
    "#using distance function from the scipy library\n",
    "from scipy.spatial import distance\n",
    "\n",
    "x = [3,6,7]\n",
    "y = [6,8,9]\n",
    "\n",
    "print(distance.euclidean(x,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Minkowski distance:\n",
    "- both Manhattan and Euclidean are both special cases of  Minkowski distance.\n",
    "- Manhattan distance is a special case of Minkowski distance with an exponent parameter c = 1.\n",
    "- Euclidean distance is a special case of Minkowski distance with an exponent parameter c = 2.\n",
    "- This is the distance between the euclidean and manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using scipy\n",
    "from scipy.spatial import distance\n",
    "\n",
    "x = [3,6,7]\n",
    "y = [6,8,9]\n",
    "\n",
    "#minkowski distance takes a third parameter p. p is the order of the norm\n",
    "print(distance.minkowski(x,y,p=3)) #minkowski distance\n",
    "\n",
    "#in minkowski distance, if p=1, it is manhattan distance, if p=2, it is euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to get all three at once using a function and np.power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "7.0\n",
      "4.372215289689355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def distance(x,y,c=2): #define the x and y coordinates and the default value of c is 2.\n",
    "    #turn x and y into numpy arrays\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    #calculate the distance\n",
    "    return np.power(np.sum(np.power(np.abs(x-y),c)),1/c)\n",
    "\n",
    "\n",
    "test_point_1 = (1, 2)\n",
    "test_point_2 = (4, 6)\n",
    "print(distance(test_point_1, test_point_2)) # Expected Output: 5.0  . This is the euclidean distance\n",
    "print(distance(test_point_1, test_point_2, c=1)) # Expected Output: 7.0. This is the manhattan distance\n",
    "print(distance(test_point_1, test_point_2, c=3.5)) # Expected Output: 4.372215855099817. This is the minkowski distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### KNN - distance-based classifier.In KNN, the assumption is that points that are closer together (i.e., have a smaller Minkowski distance) are more likely to be similar in terms of their features and class labels.\n",
    "- In KNN, each column acts as a dimension. In a dataset with two columns, we can easily visualize this by treating values for one column as X coordinates and and the other as Y coordinates. \n",
    "\n",
    "### fitting the model \n",
    "- KNN is unique compared to other classifiers in that it does almost nothing during the \"fit\" step, and all the work during the \"predict\" step. During the \"fit\" step, KNN just stores all the training data and corresponding labels. No distances are calculated at this point.\n",
    "\n",
    "### Distance metrics\n",
    "When using KNN, you can use Manhattan, Euclidean, Minkowski distance, or any other distance metric. Choosing an appropriate distance metric is essential and will depend on the context of the problem at hand.\n",
    "\n",
    "### Evaluating model performance\n",
    "How to evaluate the model performance depends on whether you're using the model for a classification or regression task. KNN can be used for regression (by averaging the target scores from each of the K-nearest neighbors), as well as for both binary and multicategorical classification tasks.\n",
    "\n",
    "Evaluating classification performance for KNN works the same as evaluating performance for any other classification algorithm -- you need a set of predictions, and the corresponding ground-truth labels for each of the points you made a prediction on. You can then compute evaluation metrics such as Precision, Recall, Accuracy, F1-Score etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For KNN we need to ensure no underfitting or overfitting but normal fitting\n",
    "- The smaller the number of K is the tighter the model fits.\n",
    "- If K is too small it causes overfitting issues \n",
    "- If K is too large it causes underfitting issues.\n",
    "- Since the model arrives at a prediction by voting, ensure you  only use odd values for k, to avoid ties and subsequent arbitrary guesswork. \n",
    "- The best way to find an optimal value for K is to choose a minimum and maximum boundary and try them all! In practice, this means:\n",
    "\n",
    "1. Fit a KNN classifier for each value of K\n",
    "2. Generate predictions with that model\n",
    "3. Calculate and evaluate a performance metric using the predictions the model made\n",
    "4. Compare the results for every model and find the one with the lowest overall error, or highest overall score!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disadvantages of KNN\n",
    "- Not suitable for extremely large datasets. This is because large datasets increase time complexity exponentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since KNN is distance based algorithm we normalize data to ensure we are using same scale. \n",
    "- Since KNN is a distance-based classifier, if data is in different scales, then larger scaled features have a larger impact on the distance between points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERFORM KNN\n",
    "1. Clean and preprocessing data. Involves creating dummy variables.\n",
    "2. Standardize the data to be in the same scale. Eg StandardScaler from sklearn.preprocessing.\n",
    "- scaler.fit_transform = training data\n",
    "- scaler.transform = testing data(X_test)\n",
    "- turn the transformed training data to a df\n",
    "scaled_df_train = pd.DataFrame(scaled_data_train, columns = X_train.columns, index = X_train.index)\n",
    "3. Perform KNN.\n",
    "- from sklearn.neighbors import KNeighborsClassifier\n",
    "- ### Instantiate KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "### Fit the classifier\n",
    "clf.fit(scaled_df_train, y_train)\n",
    "\n",
    "### Predict on the test set\n",
    "test_preds = clf.predict(scaled_data_test)\n",
    "\n",
    "### Evaluate model \n",
    "- accuracy_score, precision_score, f1_score from sklearn.metrics\n",
    "- print(\"Accuracy:\", accuracy_score(y_test, test_preds))\n",
    "- print(\"Precision:\", precision_score(y_test, test_preds))\n",
    "- print(\"Recall:\", recall_score(y_test, test_preds))\n",
    "= print(\"F1 Score:\", f1_score(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal KNN neighbours \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the optimal value of neighbors in KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def optimal_neighbors(X_train, y_train, X_test, y_test,min_k=1, max_k=25):\n",
    "    #initialize the optimal values\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "\n",
    "    #iterate through the range of k for odd values only\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        #initialize the KNN model\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "        #fit the model\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        #make predictions\n",
    "        preds = knn.predict(X_test)\n",
    "\n",
    "        #calculate the accuracy\n",
    "        f1 = f1_score(y_test, preds)\n",
    "\n",
    "        #check if the current k is better than the best_k\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "\n",
    "    return best_k, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Euclidean: 1.0\n",
      "Accuracy Manhattan: 1.0\n",
      "Accuracy Minkowski: 1.0\n"
     ]
    }
   ],
   "source": [
    "# fitting the KNN model using the 3 different distance metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load the iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "#split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#initialize the different KNN models with specified distance metrics\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean') #or metric='minkowski', p=2\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=3, metric='manhattan') #or metric='minkowski', p=1\n",
    "knn_minkowski = KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=3)#p=3 is the order of the norm. P=1 is manhattan distance, p=2 is euclidean distance\n",
    "\n",
    "#fit the models\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "knn_minkowski.fit(X_train, y_train)\n",
    "\n",
    "#make predictions\n",
    "preds_euclidean = knn_euclidean.predict(X_test)\n",
    "preds_manhattan = knn_manhattan.predict(X_test)\n",
    "preds_minkowski = knn_minkowski.predict(X_test)\n",
    "\n",
    "#calculate the accuracy of the models\n",
    "accuracy_euclidean = accuracy_score(y_test, preds_euclidean)\n",
    "accuracy_manhattan = accuracy_score(y_test, preds_manhattan)\n",
    "accuracy_minkowski = accuracy_score(y_test, preds_minkowski)\n",
    "\n",
    "print(f'Accuracy Euclidean: {accuracy_euclidean}')\n",
    "print(f'Accuracy Manhattan: {accuracy_manhattan}')\n",
    "print(f'Accuracy Minkowski: {accuracy_minkowski}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chika",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
