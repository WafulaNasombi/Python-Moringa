{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "#this is my first comment\n",
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AVA LAVIGNE'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name =\"ava lavigne\"\n",
    "name.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my second code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "print(\"Hello World\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a [youtube](https://www.youtube.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "o\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "for letter in \"Moringa\":\n",
    " print(letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "def myfunction(a,b):\n",
    "    return a+b\n",
    "\n",
    "print(myfunction(2,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2864.79\n"
     ]
    }
   ],
   "source": [
    "#function that accepts one numeric parameter \n",
    "#function to convert radians to degrees the return the value\n",
    "def radian_to_degree(radian):\n",
    "    return radian *57.2958\n",
    "\n",
    "print(radian_to_degree(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid sort type\n"
     ]
    }
   ],
   "source": [
    "#Function to sort a list\n",
    "def sort_list(list,sort):\n",
    "    if sort == \"ascending\":\n",
    "        return sorted(list)\n",
    "    elif sort == \"descending\":\n",
    "        return sorted(list,reverse=True)\n",
    "    else:\n",
    "        return \"Invalid sort type\"\n",
    "\n",
    "print(sort_list([1,2,3,4,5],\"none\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b1111\n"
     ]
    }
   ],
   "source": [
    "#convert decimal number to binary\n",
    "def dec_to_binary(decimal):\n",
    "    return bin(decimal)\n",
    "\n",
    "\n",
    "print(dec_to_binary(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#function that accepts a list and returns the sum of the elements in the list\n",
    "def sum_list(list):\n",
    "    return len(list)\n",
    "\n",
    "print(sum_list([\"tomatoes\",\"oranges\",\"bananas\", \"mangoes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2406.5\n"
     ]
    }
   ],
   "source": [
    "#function that accepts a list and returns the average of the elements in the list\n",
    "def average_list(list):\n",
    "    return sum(list)/len(list)\n",
    "\n",
    "print(average_list([67,89,8935,535]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#function to accept a word and return the number of vowels in the word\n",
    "def count_vowels(word):\n",
    "    vowels ='a','e','i','o','u'\n",
    "    count = 0\n",
    "    for letter in word:\n",
    "        if letter in vowels:\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "# Test the function\n",
    "print(count_vowels(\"pneumonia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** **** **** 5432\n"
     ]
    }
   ],
   "source": [
    "#function to hide the credit card number\n",
    "def hide_card_number(card_number):\n",
    "    return \"**** **** **** \" + card_number[-4:]\n",
    "\n",
    "print(hide_card_number(\"1234 5678 9876 5432\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#function to accept string,count no of xs and nos of os in string\n",
    "#return either True or false\n",
    "def count_no_of(string):\n",
    "  count_x = string.count('x')\n",
    "  count_o = string.count('o')\n",
    "\n",
    "  return count_x == count_o    \n",
    "print(count_no_of(\"Policexsfe\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n"
     ]
    }
   ],
   "source": [
    "#function to create a calculator function\n",
    "def calculator(a,operator, b):\n",
    "    if operator == '+':\n",
    "        return a + b\n",
    "    elif operator == '-':\n",
    "        return a - b\n",
    "    elif operator == '*':\n",
    "        return a * b \n",
    "    elif operator == '/':\n",
    "        return a / b\n",
    "    else:\n",
    "       return \"Invalid operator\"\n",
    "\n",
    "print (calculator(6, '*', 56 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "#function to give discount\n",
    "def give_discount(fixed_price,discount):\n",
    "    return fixed_price - discount \n",
    "print(give_discount(100,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "#function to accept list with both integers and strings return \n",
    "#a list with integers only \n",
    "def my_list(list):\n",
    "    #list comprehension to iterate over every element x in the list\n",
    "    #insintance(x,int) checks if the x is in an in integers then adds it in the new list\n",
    "    integers = [x for x in list if isinstance(x, int)]\n",
    "    return sorted(integers)\n",
    "    \n",
    "     \n",
    "# Test the function\n",
    "print(my_list([6, 7, 8, \"mama\", \"toy\", 1, \"bricks\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aanndd\n"
     ]
    }
   ],
   "source": [
    "#function to accept string and return a string with repeated character\n",
    "def rep_char(string):\n",
    "    #list comprehension to iterate through each character and double each character\n",
    "    my_string =[x*2 for x in string ]\n",
    "    return ''.join(my_string)# ' ' specifies how the characters should be joined without any \n",
    "    #other characters in between it\n",
    "\n",
    "print(rep_char(\"and\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert minute to seconds\n",
    "def min_to_sec(a):\n",
    "    return  a * 60 \n",
    "minute = int(input(\"Enter minute: \\n\"))\n",
    "\n",
    "print(min_to_sec(minute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 2, 24, 78]\n"
     ]
    }
   ],
   "source": [
    "#function for dictionary \n",
    "students = {\n",
    "    \"Name\" :[\"Anthony\",\"James\", \"Christine\",\"Ali\",\"Martha\",\"Jane\"],\n",
    "    \"Grades\" :{\n",
    "        \"Pre-work\" :[10,2,31,24,78,21],\n",
    "        \"Post-work\":[78,67,89,88,90,91]\n",
    "    },\n",
    "    \"class\":\"DSF-FTO9\",\n",
    "    \"year\":2024\n",
    "}\n",
    "\n",
    "even_pre_work = students[\"Grades\"][\"Pre-work\"]\n",
    "even_pre_work = [x for x in even_pre_work if x % 2 == 0]\n",
    "print(even_pre_work)\n",
    "\n",
    "#for even in students[\"Grades\"][\"Pre-work\"]:\n",
    "    #if even % 2 == 0:\n",
    "        #print(even)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "**\n",
      "***\n",
      "****\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "#create a triangle using * without using a function\n",
    "for i in range(1,6):\n",
    "    print(\"*\" * i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "**\n",
      "***\n",
      "****\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "#create a christmas tree using two for loops\n",
    "rows = 5\n",
    "for i in range(1,rows+1):#no of rows\n",
    " for r in  range(1, i + 1):#no of columns\n",
    "     print(\"*\" , end = \"\")\n",
    " print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    *\n",
      "   **\n",
      "  ***\n",
      " ****\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "#create a triangle that starts spacing from the right not the left\n",
    "rows = 5\n",
    "for i in range(1,rows+1):\n",
    "    for j in range(1, rows+1):\n",
    "        if j < rows - i + 1:\n",
    "            print(\" \", end = \"\")\n",
    "        else:\n",
    "            print(\"*\", end = \"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    *\n",
      "   **\n",
      "  ***\n",
      " ****\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "##create a triangle that starts spacing from the right not the left\n",
    "rows = 5\n",
    "for i in  range(1,rows + 1):\n",
    " print(\" \" * (rows - i) + \"*\" * i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 Python\n",
      "#2 Java\n",
      "#3 C++\n",
      "#4 JavaScript\n",
      "#5 Ruby\n"
     ]
    }
   ],
   "source": [
    "#looping through lists\n",
    "languages = [\"Python\",\"Java\",\"C++\",\"JavaScript\",\"Ruby\"]\n",
    "i =0\n",
    "while i <(len(languages)):\n",
    "    print('#' + str(i + 1), languages[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "Java\n",
      "C++\n",
      "JavaScript\n",
      "Ruby\n"
     ]
    }
   ],
   "source": [
    "#looping through lists\n",
    "languages = [\"Python\",\"Java\",\"C++\",\"JavaScript\",\"Ruby\"]\n",
    "i =0\n",
    "while i <(len(languages)):\n",
    "    print( languages[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 Python\n",
      "#2 Go\n",
      "#3 Dart\n"
     ]
    }
   ],
   "source": [
    "#loop through using enumerate\n",
    "languages = ['Python', 'Go', 'Dart']\n",
    "for i, language in enumerate(languages):\n",
    "  print('#' + str(i + 1), languages[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Python\n",
      "2 Go\n",
      "3 Dart\n"
     ]
    }
   ],
   "source": [
    "#loop through using enumerate\n",
    "languages = ['Python', 'Go', 'Dart']\n",
    "for i, language in enumerate(languages):\n",
    "  print(str(i + 1), language) #coz we are combining a string and an integer we have to convert the integer to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Student at 0x20dbaf2adf0>,\n",
       " <__main__.Student at 0x20dbaeec520>,\n",
       " <__main__.Student at 0x20dbaf41070>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Student:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "students = []\n",
    "\n",
    "students.append(Student('Mark', 25))\n",
    "students.append(Student('Emma', 22))\n",
    "students.append(Student('Jones', 24))\n",
    "\n",
    "students\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "#using map function\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "squared = list(map(square, numbers))\n",
    "print(squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Mukesh 24\n",
      "1 Roni 50\n",
      "2 Chari 18\n"
     ]
    }
   ],
   "source": [
    "#using both enumerate and zip\n",
    "names = ['Mukesh', 'Roni', 'Chari']\n",
    "ages = [24, 50, 18]\n",
    " \n",
    "for i,( name, age) in enumerate(zip(names, ages)):#enumerate iterates through the whole loop in the form of index and the element \n",
    "    #zip combines the two lists\n",
    "    print(i, name, age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GEEKS': 2175, 'For': 1127, 'geeks': 2750}\n"
     ]
    }
   ],
   "source": [
    "#zip in list comprehension\n",
    "stocks = ['GEEKS', 'For', 'geeks']\n",
    "prices = [2175, 1127, 2750]\n",
    " \n",
    "new_dict = {stocks: prices for stocks,\n",
    "            prices in zip(stocks, prices)}\n",
    "print(new_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Player : Sachin Score : 100\n",
      " Player : Sehwag Score : 15\n",
      " Player : Gambhir Score : 17\n",
      " Player : Dravid Score : 28\n",
      " Player : Raina Score : 43\n"
     ]
    }
   ],
   "source": [
    "# Python code to demonstrate the application of\n",
    "# zip()\n",
    " \n",
    "# initializing list of players.\n",
    "players = [\"Sachin\", \"Sehwag\", \"Gambhir\", \"Dravid\", \"Raina\"]\n",
    " \n",
    "# initializing their scores\n",
    "scores = [100, 15, 17, 28, 43]\n",
    " \n",
    "# printing players and scores.\n",
    "for pl, sc in zip(players, scores):\n",
    "    print(f\" Player : {pl} Score : {sc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "#use the filter function to filter out odd numbers\n",
    "mine = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "def is_even(x):\n",
    "    #function to check if number in list is even\n",
    "    return  x % 2 == 0\n",
    "\n",
    "#call the function is_even using the filter function\n",
    "even_numbers = list(filter(is_even, mine))#filter function takes each number through the function is_even if even it returns true\n",
    "#and adds it to the list even_numbers\n",
    "print(even_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'foo': 12, 'bar': 14}, {'moo': 52, 'car': 641}, {'doo': 6, 'tar': 84}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of dictionaries\n",
    "myList = [\n",
    "\t{\n",
    "\t\t'foo':12,\n",
    "\t\t'bar':14\n",
    "\t},\n",
    "\t{\n",
    "\t\t'moo':52,\n",
    "\t\t'car':641\n",
    "\t},\n",
    "\t{\n",
    "\t\t'doo':6,\n",
    "\t\t'tar':84\n",
    "\t}\n",
    "]\n",
    "\n",
    "print(myList)\n",
    "type(myList)\n",
    "#dictionary is like an element in a list thus {'foo':12,'bar':14} is an element at index 0  in the list myList\n",
    "#{'moo':52,'car':641} is an element at index 1 in the list myList\n",
    "#and {'doo':6,'tar':84} is an element at index 2 in the list myList\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing dictionaries in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'foo': 250, 'bar': 14}, {'car': 641, 'new_key': 200}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to access the first dictionary\n",
    "myList[0]\n",
    "#to access the value of foo\n",
    "myList[0]['foo']\n",
    "#to access the value of bar\n",
    "myList[0]['bar']\n",
    "#to access the second dictionary\n",
    "myList[1]\n",
    "#to access the key of the first item dictionary\n",
    "myList[1].keys()\n",
    "#to access the first value of the dictionary at index 2\n",
    "myList[1]['moo']\n",
    "#update the key:value pair of the dictionary 1\n",
    "myList[0]['foo'] = 250\n",
    "myList[0]['foo']\n",
    "\n",
    "#add a key value item in the second dictionary \n",
    "myList[1]['new_key'] = 200\n",
    "myList\n",
    "\n",
    "#delete a key:value pair in the last dictionary\n",
    "#del myList[2]# output [{'foo': 250, 'bar': 14},{'moo': 52, 'car': 641, 'new_key': 200}]\n",
    "#myList\n",
    "\n",
    "#DELETE only one key:value pair in the dictionary at index 1\n",
    "del myList[1]['moo'] #output [{'foo': 250, 'bar': 14}, { 'car': 641, 'new_key': 200}]\n",
    "myList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'foo': 250, 'bar': 14}, {'new_key': 200}, {'joo': 48, 'par': 28}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pop a key:value pair\n",
    "#myList[1].pop('car')\n",
    "#myList\n",
    "#clear\n",
    "#myList[1].clear() ->dictionary at index 1 will come back empty\n",
    "\n",
    "# Append dictionary to list\n",
    "myList.append({'joo':48, 'par':28})\n",
    "myList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorting dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key4': 7, 'key2': 10, 'key3': 12, 'key1': 56}\n"
     ]
    }
   ],
   "source": [
    "#sorting a dictionary based on values\n",
    "dict1 = {\n",
    "    \"key1\": 56,\n",
    "    \"key2\": 10,\n",
    "    \"key3\": 12,\n",
    "    \"key4\": 7}\n",
    "\n",
    "sorted_dict = dict(sorted(dict1.items(), key=lambda item: item[1]))\n",
    "print(sorted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"orange\",'magenta','purple',\"orange\"]\n",
    "for color in colors:\n",
    "    if color == \"orange\":\n",
    "         break\n",
    "    print(color)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "#use continue to print out all the even numbers\n",
    "for i in range(0,30):\n",
    "    #if condition to check if number is odd\n",
    "    if i %2 != 0:\n",
    "        continue\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Numpy to perform math operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "[[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#create a numpy array\n",
    "numpy_array = np.array([1,2,3,4,5,])#single dimensional array\n",
    "print(numpy_array)\n",
    "\n",
    "#multi_dimensional array\n",
    "array_multi_dimensional = np.array([[1,2,3,4,5],[6,7,8,9,10]])\n",
    "print(array_multi_dimensional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  50  200  450  800 1250]\n"
     ]
    }
   ],
   "source": [
    "#getting the square area of a room given a list of room dimensions\n",
    "#assign the room dimensions to a variable as an array\n",
    "length_of_room = np.array([10,20,30,40,50])\n",
    "#width of the room\n",
    "width_of_room = np.array([5,10,15,20,25])\n",
    "#square area of the room \n",
    "sq_area = length_of_room * width_of_room\n",
    "print(sq_area)#provides output in array form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO CONVERT FAHREINHEIT TO CELSIUS\n",
    "#T(°C) = (T(°F) - 32) × 5/9\n",
    "# Average temps in NYC from January -> December (in fahrenheit)\n",
    "#nyc_avg_temps_f = [39, 42, 50, 62, 72, 80, 85, 84, 76, 65, 54, 44]\n",
    "#np_nyc_avg_temps_f = np.array(nyc_avg_temps_f)\n",
    "#np_nyc_avg_temps_c = (np_nyc_avg_temps_f - 32) * (5/9)\n",
    "\n",
    "#OUTPUT\n",
    "#With NumPy: [ 3.88888889  5.55555556 10.         16.66666667 22.22222222 26.66666667\n",
    " #29.44444444 28.88888889 24.44444444 18.33333333 12.22222222  6.66666667]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.zeros((4,3,2)) creates a 3 dimensional array of 0's\n",
    "#np.empty((4,3,2,)) creates a 3 dimensional array of 0's\n",
    "#np.zeros((2), dtype = int)\n",
    "#np.ones((2,3)) creates a 2 dimensional array of 1's\n",
    "np.ones(5)#creates a single dimensional array of 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.full((2,2), 9)#creates a 2 dimensional array of 9's output array([[9, 9],[9, 9]])\n",
    "#np.full(5,3)# creates a single dimensional array  of 3's in 5 columns output array([3, 3, 3, 3, 3])\n",
    "# Create a 1d array with 5 elements, filling them with the values 0 to 4\n",
    "np.full(5, range(5)) #output array([0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "#accessing array elements\n",
    "x = np.array([[1, 2, 3],\n",
    "               [4, 5, 6], \n",
    "               [7, 8, 9], \n",
    "               [10, 11, 12]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accessing a specific element in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#access a specific element in the array\n",
    "x[0,0]  #output 1 -> the first element in row 1 column 1\n",
    "x[0,1] #output 2 -> the second element in row 1 column 2   \n",
    "x[1,2] #output 6 -> the third element in row 2 column 3\n",
    "x[2,1] #output 8 -> the second element in row 3 column 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accessing elements of a specific row or specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  6],\n",
       "       [ 7,  8,  9],\n",
       "       [10, 11, 12]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieving the first row\n",
    "x[0] #output array([1, 2, 3])\n",
    "#retrieving the first row the first column\n",
    "x[0:,1] #array([ 1,  4,  7, 10])\n",
    "#retrieving all the values of column two from the first  row to the last\n",
    "x[0:,1] #output array([ 2,  5,  8, 11])\n",
    "#retrieving the first two rows\n",
    "x[:2] #output array([[1, 2, 3],[4, 5, 6]])\n",
    "#retrieving the values of column three from the second row\n",
    "x[1:,2]     #output array([ 6,  9, 12])\n",
    "# Retrieving the second column\n",
    "x[0:,1] #output array([ 2,  5,  8, 11])\n",
    "# Retrieving all rows after the first row\n",
    "x[1:] #output array([[ 4,  5,  6],[ 7,  8,  9],[10, 11, 12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "slicing a multidimensional array\n",
    "you cannot slice into mutliple dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  7, 10])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to slice multidimensional arrays \n",
    "x[1:3, 0:2] # this will return the first two columns of the second and third rows\n",
    "#output array([[4, 5], [7, 8]]) \n",
    "# Rows 2 through 4, columns 1 through 3\n",
    "x[2:4,1:3] #output array([[ 8,  9],[11, 12]])\n",
    "# All rows, column 0\n",
    "x[:,0] #output array([ 1,  4,  7, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  5],\n",
       "       [ 8, 11]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With a 3D array\n",
    "x = np.array([\n",
    "              [[1,2,3], [4,5,6]],\n",
    "              [[7,8,9], [10,11,12]]\n",
    "             ])\n",
    "x.shape #output (2,2,3)\n",
    "x[:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculating the median, mode and mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "2.5\n"
     ]
    }
   ],
   "source": [
    "#function for mean\n",
    "def get_mean(data):\n",
    "    # Replace None with appropriate code\n",
    "    mean = None\n",
    "    sum = 0\n",
    "    #calculate the sum of numbers \n",
    "    for number in data:\n",
    "      sum = sum + number\n",
    "    #calculate the mean \n",
    "    mean = sum/len(data)\n",
    "    #round of mean and return the rounded off mean\n",
    "    return round(mean,2)\n",
    "\n",
    "test1 = [5, 4, 1, 3, 2]\n",
    "test2 = [4, 2, 3, 1]\n",
    "\n",
    "print(get_mean(test1)) # 3.0\n",
    "print(get_mean(test2)) # 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2.5\n"
     ]
    }
   ],
   "source": [
    "#function to calculate median\n",
    "def get_median(data):\n",
    "    # Replace None with appropriate code\n",
    "    #sort the data in the list first\n",
    "    data_sorted = sorted(data)\n",
    "\n",
    "    #check if data has even or odd number of data points\n",
    "    length = len(data_sorted)\n",
    "    #condition to check \n",
    "    if length % 2 == 0:\n",
    "      #calculate median by taking two numbers adding them and divide by 2\n",
    "      median = (data_sorted[length // 2 - 1] + data_sorted[length // 2])/2\n",
    "    #condition for when odd\n",
    "    else: #odd condition\n",
    "     median = data_sorted[length // 2]\n",
    "\n",
    "\n",
    "    return median\n",
    "    # Your code here\n",
    "    # Check for even/odd and perform calculations accordingly - use if-else\n",
    "\n",
    "test1 = [5, 4, 1, 3, 2]\n",
    "test2 = [4, 2, 3, 1]\n",
    "\n",
    "print(get_median(test1)) # 3\n",
    "print(get_median(test2)) # 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "[1, 5]\n"
     ]
    }
   ],
   "source": [
    "#mode\n",
    "\n",
    "# Throughout this cell, replace None with appropriate code\n",
    "\n",
    "def get_mode(data):\n",
    "\n",
    "    # Create and populate frequency distribution\n",
    "    frequency_dict = {}\n",
    "\n",
    "    for height in data:\n",
    "        # If an element is not in the dict, add it to the dict with value 1\n",
    "        # If an element is already in the dict, +1 the value in place\n",
    "        if height in frequency_dict:\n",
    "          frequency_dict[height] += 1\n",
    "        else:\n",
    "          frequency_dict[height] = 1 \n",
    "\n",
    "    # Find the frequency of the mode(s) by finding the largest\n",
    "    # value in frequency_dict\n",
    "    highest_freq = max(frequency_dict.values())\n",
    "\n",
    "    # Create a list for mode values\n",
    "    modes = []\n",
    "\n",
    "    # From the dictionary, add element(s) to the modes list with max frequency\n",
    "    for height, frequency in frequency_dict.items():\n",
    "        if frequency == highest_freq:\n",
    "          modes.append(height)\n",
    "\n",
    "    # Return the mode list\n",
    "    return modes\n",
    "\n",
    "test1 = [1, 2, 3, 5, 5, 4]\n",
    "test2 = [1, 1, 1, 2, 3, 4, 5, 5, 5]\n",
    "\n",
    "print(get_mode(test1)) # [5]\n",
    "print(get_mode(test2)) # [1, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.67\n",
      "3.33\n",
      "3.25\n"
     ]
    }
   ],
   "source": [
    "#variance\n",
    "# Replace None with appropriate code\n",
    "\n",
    "def get_variance(sample):\n",
    "\n",
    "    # First, calculate the sample mean using get_mean()\n",
    "    sample_mean = get_mean(sample)\n",
    "\n",
    "    sum_of_squares = 0\n",
    "    for height in sample:\n",
    "        # Now, calculate the sum of squares by subtracting the sample mean\n",
    "        # from each height, squaring the result, and adding it to the total\n",
    "        sum_of_squares += (height - sample_mean)** 2\n",
    "\n",
    "    # Divide the sum of squares by the number of items in the sample -1 to calculate variance\n",
    "    variance = sum_of_squares /(len(sample)- 1)\n",
    "\n",
    "    return round(variance, 2)\n",
    "\n",
    "test1 = [1, 2, 3, 5, 5, 4]\n",
    "test2 = [1, 1, 1, 2, 3, 4, 5, 5, 5]\n",
    "print(get_variance(test1)) # 2.67\n",
    "print(get_mean(test1)) # 3.33\n",
    "print(get_variance(test2)) # 3.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard deviation\n",
    "# Replace None with appropriate code\n",
    "from math import sqrt\n",
    "\n",
    "def get_stddev(sample):\n",
    "\n",
    "    stddev = sqrt(get_variance(sample))\n",
    "\n",
    "    return round(stddev, 2)\n",
    "\n",
    "test = [120,112,131,211,312,90]\n",
    "\n",
    "get_stddev(test) # 84.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using python built in functions\n",
    " import numpy as np\n",
    " from scipy import stats\n",
    "1.for mean = np.mean(name of list)  to round it up to 2dp\n",
    "round(np.mean(name_of_list),2)\n",
    "2.for median = np.median(name_of_list)\n",
    " 3 . for mode = stats.mode(name_of_list).mode\n",
    " 4 .for variance = np.var(name_of_list,ddof= 1)\n",
    "    round off to 2 dp = round(np.var(name, ddof =1),2)\n",
    "5 . for standard_deviation = np.std(name_of_list)\n",
    "6 . for inter-quartile range = np.percentile(name_list, 75) - np.percentile(name_list, 25)\n",
    "7 . correlation = np.correlate(list_a,list_b)\n",
    "8 . covariance = np.cov(x)->x is the array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {\"color\": \"blue\", \"number\": 7}\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "three\n"
     ]
    }
   ],
   "source": [
    "def one():\n",
    "    print(\"one\")\n",
    "\n",
    "def two():\n",
    "    return \"two\"\n",
    "\n",
    "def three():\n",
    "    one()\n",
    "    two()\n",
    "    print(\"three\")\n",
    "\n",
    "three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis in base python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_table = [\n",
    "    {\"color\": \"green\", \"number\": 7},\n",
    "    {\"color\": \"red\", \"number\": 2},\n",
    "    {\"color\": \"orange\", \"number\": 1}\n",
    "]\n",
    "\n",
    "type(info_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'color': 'green', 'number': 7}\n",
      "{'color': 'red', 'number': 2}\n",
      "{'color': 'orange', 'number': 1}\n"
     ]
    }
   ],
   "source": [
    "for item in info_table:\n",
    "    print(item)                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\envs\\chika\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\admin\\anaconda3\\envs\\chika\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\chika\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#using the pandas library\n",
    "#install pandas library\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(info_table)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     green\n",
      "1       red\n",
      "2    orange\n",
      "Name: color, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'o', 'm', 'p', 'r', 'e', 'h', 'e', 'n', 's', 'i', 'o', 'n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use a list comprehension to take the first character of each string\n",
    "words = ['carbon', 'osmium', 'mercury', 'potassium', 'rhenium', 'einsteinium',\n",
    "        'hydrogen', 'erbium', 'nitrogen', 'sulfur', 'iodine', 'oxygen', 'niobium']\n",
    "\n",
    "first_chars = [word[0] for word in words]\n",
    "first_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a function to return the mode of a list\n",
    "def mode(list_numbers):\n",
    "  #create a dictionary with the num as keyword and count of num as value\n",
    "  counts = { num:list_numbers.count(num) for num in list_numbers}\n",
    "  #find the number with the highest frequency in the dictionary and return that num\n",
    "  return [ num for num in counts if counts[num] == max(counts.values())]\n",
    "\n",
    "\n",
    "#call the function\n",
    "mode([1,2,3,4,5,5,5,6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list comprehension in lists\n",
    "data = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "#return a list of numbers greater than 3\n",
    "#using for loop\n",
    "greater_than_3 =[]\n",
    "for num in data:\n",
    "    if num > 3:\n",
    "        greater_than_3.append(num)\n",
    "\n",
    "#using list comprehension\n",
    "larger_than_3 = [num for num in data if num > 3]\n",
    "larger_than_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 83, 5: 84, 7: 98}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list comprehension for dictionaries\n",
    "indices,data = [2,5,7], [83,84,98]\n",
    "zipped = zip(indices,data)\n",
    "\n",
    "#creating a dictionary using for loop\n",
    "#for_loop_dict = {}\n",
    "#for x,y in zipped:\n",
    "      #for_loop_dict[x] = y\n",
    "\n",
    "\n",
    "\n",
    "#using dict comprehension\n",
    "dict_comp = {x:y for x,y in zipped}\n",
    "dict_comp\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 166, 2: 168, 3: 196}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transforming the values of x and y\n",
    "#using for loop\n",
    "transformed_using_for_loop = {}\n",
    "for x,y in dict_comp.items():\n",
    "    transformed_using_for_loop[x//2] = y * 2\n",
    "\n",
    "#using dict comprehension\n",
    "dict_comprehension = { x//2 : y *2 for x,y in dict_comp.items()}\n",
    "\n",
    "dict_comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This line opens the file data/apple_orchard.csv in read mode. \n",
    "#The with statement ensures that the file is properly closed after its suite finishes, even if an exception is raised.\n",
    "with open(\"data/apple_orchard.csv\") as f:\n",
    "    # Read the file and print its contents\n",
    "    apple_orchard_data = list(csv.DictReader(f)) #csv.DictReader reads the file and returns each row as a dictionary of the data\n",
    "    #where the keys are the column headers from the first row of the CSV file\n",
    "    #list(csv.DictReader(f)) converts the DictReader object into a list of dictionaries. Each dictionary represents a row in the CSV file.\n",
    "    apple_tree_yields = [float(x[\"yield\"]) for x in apple_orchard_data] #    This list comprehension iterates over each dictionary in apple_orchard_data.\n",
    "    #For each dictionary x, it accesses the value associated with the key \"yield\" and converts it to a float.\n",
    "    #The resulting list, apple_tree_yields, contains the yield values as floating-point numbers.\n",
    "    print(\"Total apple tree yields:\", len(apple_tree_yields))\n",
    "    print(\"First 5 apple tree yields:\", apple_tree_yields[:5])  \n",
    "    print(\"Average yield:\", sum(apple_tree_yields) / len(apple_tree_yields))\n",
    "    print(\"Maximum yield:\", max(apple_tree_yields))\n",
    "    print(\"Minimum yield:\", min(apple_tree_yields))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freq_table(key, links): #function to build a frequency table\n",
    "    table = {} #create an empty dictionary to store the frequency of each user\n",
    "    for link in links:  #iterate over the links For each link in links, it retrieves the value associated \n",
    "        #with the given key (either \"source\" or \"target\") and stores it in user.\n",
    "        user = link[key]\n",
    "        table[user] = table.get(user, 0) + 1 # It then updates the frequency count for user in table. If user is already in table, \n",
    "        #it increments the count by 1; otherwise, it initializes it to 1. \n",
    "    return table\n",
    "\n",
    "\n",
    "def print_top_5(table):\n",
    "    for k, v in sorted(table.items(), key=lambda x: x[1], reverse=True)[:5]:# It sorts the items in table by the frequency values (x[1]) \n",
    "    #in descending order (reverse=True). It slices the sorted list to get the top 5 items ([:5]).\n",
    "        print(f\"User {k}\\t| {v} Tweets\") #        It then prints the top 5 users and their tweet counts in a formatted string.\n",
    "\n",
    "with open(\"data/twitter_graph.json\") as f:\n",
    "    twitter_data = json.load(f) #json.load(f) reads the JSON data from the file and parses it into a Python dictionary, which is stored in twitter_data.\n",
    "    print(f\"There are {len(twitter_data['nodes'])} users in this dataset\") #twitter_data['nodes'] is expected to be a list of user nodes, so \n",
    "    #en(twitter_data['nodes']) gives the number of users.\n",
    "\n",
    "\n",
    "    links = twitter_data[\"links\"] #list of dictionaries\n",
    "    sources = build_freq_table(\"source\", links)#build_freq_table(\"source\", links) creates a frequency table of the source as the key\n",
    "    targets = build_freq_table(\"target\", links)#build_freq_table(\"target\", links) creates a frequency table of the target as the key\n",
    "    print(\"Top 5 Sources:\")\n",
    "    print_top_5(sources)\n",
    "    print()\n",
    "    print(\"Top 5 Targets:\")\n",
    "    print_top_5(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating a csv file given data\n",
    "using  normal code and csv.reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.1,13.59,13.44\n",
      "13.93,13.85,13.47\n",
      "14.12,14.41,13.89\n",
      "14.42,13.55,13.43\n"
     ]
    }
   ],
   "source": [
    "track_times = [\n",
    "    [13.10, 13.59, 13.44],\n",
    "    [13.93, 13.85, 13.47],\n",
    "    [14.12, 14.41, 13.89],\n",
    "    [14.42, 13.55, 13.43]\n",
    "]\n",
    "track_times\n",
    "\n",
    "# Initialize an empty string\n",
    "track_times_csv = \"\"\n",
    "\n",
    "# Loop over all lists in the overall list\n",
    "for index, athlete_times in enumerate(track_times):\n",
    "    # Join together the values in the nested list using\n",
    "    # a comma as a separator\n",
    "    athlete_times_string = \",\".join([str(time) for time in athlete_times])\n",
    "    # Append the values to the overall string\n",
    "    track_times_csv += athlete_times_string\n",
    "    # Append a newline, unless this is the last row\n",
    "    if index < (len(track_times) - 1):\n",
    "        track_times_csv += \"\\n\"\n",
    "    \n",
    "print(track_times_csv) #that's a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writing a csv file using normal code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing it to a file\n",
    "with open(\"track_times.csv\", \"w\") as f: #opens a file named track_times.csv in write mode (\"w\").\n",
    "    f.write(track_times_csv)   #  This line writes the contents of the variable track_times_csv to the file.\n",
    "    #track_times_csv is expected to be a string containing the data to be written to the CSV file.\n",
    "\n",
    "\n",
    "#reading from a csv file\n",
    "with open(\"track_times.csv\") as f: #opens the file track_times.csv in read mode.\n",
    "    track_times_csv_from_disk = f.read() #reads the contents of the file and stores it in the variable track_times_csv.\n",
    "    print(track_times_csv_from_disk)\n",
    "#13.1,13.59,13.44\n",
    "#13.93,13.85,13.47\n",
    "#14.12,14.41,13.89\n",
    "#14.42,13.55,13.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_times_from_disk = [] #empty list that will store the parsed times from the CSV data.\n",
    "for row in track_times_csv_from_disk.split(\"\\n\"): #splits the input string into a list of rows by separating it at each newline character (\"\\n\").\n",
    "    times = [float(time) for time in row.split(\",\")] #splits the current row into individual time strings by separating it at each comma (,).\n",
    "    track_times_from_disk.append(times) #appends the list times (containing the converted float values for the current row) to the main list track_times_from_disk.\n",
    "    \n",
    "track_times_from_disk  #[[12.5, 13.1, 11.9], [14.0, 15.2, 13.3], [10.5, 12.1, 11.4]] output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv Module \n",
    "using csv reader to read the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV.Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data file does not contain headings, the csv.reader function works well. It will return a reader iterable that produces a Python list for each row of the file. There is a matching csv.writer option for writing data to files rather than reading data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"track_times.csv\") as f:\n",
    "    # Pass the file in to a \"reader\" object and specify that\n",
    "    # values without explicit quotes (i.e. all values in this\n",
    "    # dataset) should be treated as numbers\n",
    "    reader = csv.reader(f, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    # Get all of the data from the reader using `list`\n",
    "    track_times_with_csv_module = list(reader)\n",
    "    \n",
    "track_times_with_csv_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv.DictReader\n",
    "If the data file does contain headings, the csv.DictReader class works well. It will use a reader iterable that produces a Python dictionary for each row of the file. There is a matching csv.DictWriter option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"olympic_medals.csv\") as f: #opens file\n",
    "    reader = csv.DictReader(f) #reads file as a dictionary\n",
    "    olympics_data = list(reader) #combines the dictionaries into a list\n",
    "\n",
    "# Print the first 5 rows of data\n",
    "for index in range(5):\n",
    "    print(olympics_data[index])\n",
    "\n",
    "    # Print OrderedDict from first row of CSV file \n",
    "\n",
    "with open(csv_file_path) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    print(next(reader))\n",
    "\n",
    "\n",
    "#Let's get all of the data out of our file and into dictionaries, and store those dictionaries in a new list called sales.\n",
    "    sales = []\n",
    "\n",
    "with open(csv_file_path) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        sales.append(row)\n",
    "\n",
    "\n",
    "    #The number of rows in the dataset is just the length of the resulting list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV.DictWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musa_2016_gold_medals.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mDictWriter(f, fieldnames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m usa_2016_gold_medals: \u001b[38;5;66;03m#USA_2016_gold_medals is a list of dictionaries containing the event and name of each gold medalist.   \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"usa_2016_gold_medals.csv\", \"w\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"Event\", \"Name\"])\n",
    "    writer.writeheader()\n",
    "    for row in usa_2016_gold_medals: #USA_2016_gold_medals is a list of dictionaries containing the event and name of each gold medalist.   \n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To read the first line of a csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Kings_County_Real_Estate_Sales.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Kings_County_Real_Estate_Sales.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(csvfile\u001b[38;5;241m.\u001b[39mreadline())\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\chika\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Kings_County_Real_Estate_Sales.csv'"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"./Kings_County_Real_Estate_Sales.csv\"\n",
    "\n",
    "with open(csv_file_path) as csvfile:\n",
    "    print(csvfile.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the range of years for which we have sales data?\n",
    "#range -> find the highest (max) and lowest(min) value\n",
    "year_range_sales =  [sale['sale_year'] for sale in sales]\n",
    "                     \n",
    "print(max(year_range_sales))\n",
    "print(min(year_range_sales))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many properties sold in 2020?\n",
    "#create a list of all the years\n",
    "all_years = [sale['sale_year'] for sale in sales]\n",
    "#create a list of the 2020 years\n",
    "years_with_2020 = [year for year in all_years if year == 2020]\n",
    "len(years_with_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What was the mean sale price in 2020?\n",
    "#mean = sum /len \n",
    "#get the salePrices\n",
    "sales_price_for_2020 = [sale['sale_price'] for sale in sales if sale['sale_year'] == 2020]\n",
    "mean = sum(sales_price_for_2020) / len(years_with_2020)\n",
    "\n",
    "print(mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many properties sold in each year?\n",
    "#use both the unique list and the whole list of years\n",
    "#access the whole list\n",
    "sale_year_count = []\n",
    "all_years = [sale['sale_year'] for sale in sales]\n",
    "#unique years \n",
    "all_unique_years = set(all_years)\n",
    "\n",
    "#loop to iterate through and see how many times a unique year appears in the all_years list\n",
    "for unique_year in all_unique_years:\n",
    "    num_sales = len([year for year in all_years if year == unique_year])\n",
    "    sale_year_count.append((unique_year,num_sales))\n",
    "\n",
    "sale_year_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using counter to count the number of occurrences of something in dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sale_year_count = Counter([sale['sale_year'] for sale in sales]) #let's say that for all dictionaries in sales, we want to count how many property sales occured in each year.\n",
    "sale_year_count\n",
    "\n",
    "#output\n",
    "\"\"\"Counter({2004: 447,\n",
    "         2005: 442,\n",
    "         2020: 419,\n",
    "         1999: 406,\n",
    "         2006: 389,\n",
    "         2003: 380,\n",
    "         2015: 376,\n",
    "         2012: 364,\n",
    "         1993: 353,\n",
    "         1998: 349,\n",
    "         1996: 347,\n",
    "         2001: 342,\n",
    "         1992: 338,\n",
    "         2002: 325,\n",
    "         2007: 319,\n",
    "         1997: 306,\n",
    "         2014: 299,\n",
    "         2013: 295,\n",
    "         2016: 290,\n",
    "         \"\"\"\n",
    "\n",
    "#sale_year count is a dictionary with year and value pairs. To sort them in ascending order\n",
    "sale_year_count = sorted(sale_year_count.items())\n",
    "sale_year_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using datetime to convert a string to date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "for sale in sales:\n",
    "    # Transform DocumentDate from string to datetime\n",
    "    sale[\"clean_date\"] = datetime.strptime(sale[\"DocumentDate\"], \n",
    "                                           \"%m/%d/%Y\")\n",
    "    \n",
    "    # Add the sale year and month to each dictionary element of sales\n",
    "    sale[\"sale_year\"] = sale[\"clean_date\"].year\n",
    "    sale[\"sale_month\"] = sale[\"clean_date\"].month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to open a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nyc_2001_campaign_finance.json') as f:\n",
    "    data = json.load(f)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.convert file to table format to read the contents \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"max_colwidth\", 120)\n",
    "pd.DataFrame(\n",
    "    data=data['meta']['view'].values(), # values of the dictionary\n",
    "    index=data['meta']['view'].keys(), # keys of the dictionary\n",
    "    columns=[\"value\"] # column name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json.load() used to load the contents of the file into a Python object,\n",
    "with open('doc_info_list.json') as f:\n",
    "    doc_info_list_from_disk = json.load(f)\n",
    "\n",
    "    \n",
    "json.dump()  to write the contents of the Python object into the file.\n",
    "with open('doc_info_list.json', 'w') as f:\n",
    "    json.dump(doc_info_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create a dataframe(table) of a dictionary key:value pairs\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)#results is a dictionary\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use IPython to display the image with 'height' 300 for each album."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "for album in data['albums']['items']:#items is a dictionary in albums\n",
    "    for image in album['images']:\n",
    "        if image['height'] == 300:\n",
    "            loaded_image = Image(url=image['url'])\n",
    "            display(loaded_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "using pandas to run examine datasets and perform operations on the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. import pandas\n",
    "import pandas as pd\n",
    "\n",
    "#define the dictionary that will be used to create the dataframe\n",
    "mydataset = {\n",
    "  'cars': [\"BMW\", \"Volvo\", \"Ford\"],\n",
    "  'passings': [3, 7, 2]\n",
    "}\n",
    "\n",
    "myvar = pd.DataFrame(mydataset)\n",
    "\n",
    "print(myvar)\n",
    "\n",
    "#output\n",
    " #cars  passings\n",
    "#0    BMW         3\n",
    "#1  Volvo         7\n",
    "#2   Ford         2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PANDA SERIES\n",
    " is like a column in a table.\n",
    " one-dimensional array holding data of any type.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\envs\\chika\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\envs\\chika\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\envs\\chika\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\envs\\chika\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\admin\\anaconda3\\envs\\chika\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\chika\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a simple Pandas Series from a list\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "a = [1, 7, 2]\n",
    "\n",
    "myvar = pd.Series(a)\n",
    "\n",
    "print(myvar)\n",
    "\n",
    "#return the first value of the series\n",
    "print(myvar[0])\n",
    "\n",
    "#output\n",
    "#0    1\n",
    "#1    7\n",
    "#2    2\n",
    "\n",
    "\n",
    "#creating labels \n",
    "a = [1,7,2]\n",
    "myvar = pd.Series(a, index = ['x','y','z'])\n",
    "print(myvar)\n",
    "\n",
    "\n",
    "#output \n",
    "#x    1\n",
    "#y    7\n",
    "#z    2\n",
    "\n",
    "#When you have created labels, you can access an item by referring to the label.\n",
    "print(myvar('x')) # to access value 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas for a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "calories = {\"day1\": 420, \"day2\": 380, \"day3\": 390}#the keys of the dictionary become the labels of the new Series\n",
    "\n",
    "myvar = pd.Series(calories)\n",
    "\n",
    "print(myvar)\n",
    "\n",
    "\n",
    "#output \n",
    "#day1    420\n",
    "#day2    380\n",
    "#day3    390\n",
    "\n",
    "#To select only some of the items in the dictionary, use the index argument and specify only the items you want to include in the Series.\n",
    "import pandas as pd\n",
    "\n",
    "calories = {\"day1\": 420, \"day2\": 380, \"day3\": 390}\n",
    "\n",
    "myvar = pd.Series(calories, index = [\"day1\", \"day2\"])\n",
    "\n",
    "print(myvar)\n",
    "\n",
    "\"\"\" Series Methods\n",
    ".value_counts() - Returns a Series containing counts of unique values. It returns a series with how many times each unique value appears in the original series.\n",
    "slope\n",
    "2    142 // in the original DataFrame 2 appears 142 times\n",
    "1    142 // in the original DataFrame 1 appears 142 times\n",
    "0     21  // in the original DataFrame 0 appears 21 times\n",
    "\n",
    ".sort_values() - Returns a Series containing the original series sorted by values. \n",
    "# sort the values in descending order \n",
    ".sort_values(ascending = False) \n",
    ".sort_index() - Returns a Series containing the original series sorted by index.\n",
    "\"\"\"\n",
    "#creating fake data for series\n",
    "fruits = [\"apple\", \"banana\", \"cherry\"]\n",
    "fruits_series = pd.Series(np.random.choices(fruits, 100000)) #np.random.choices() function is used with strings,Categoricals\n",
    "\n",
    "#generating float series\n",
    "start_value = 10\n",
    "end_value =20\n",
    "float_series = pd.Series(np.random.uniform(start_value, end_value, 100000))\n",
    "or\n",
    "float_series = pd.Series(np.linspace(start_value, end_value, 100000))\n",
    "\n",
    "#generating integer series\n",
    "mean = 50\n",
    "std_dev = 10\n",
    "balance = pd.Series(np.random.normal(mean, std_dev, 100000))#.normal is used to generate random numbers with a normal distribution\n",
    "\n",
    "# generate random integer from 0 to 9\n",
    "random_number = np.random.randint(0, 10)\n",
    "\n",
    "#accessing the elements of a series\n",
    "fruit_series[12] #access the 12th element of the series\n",
    "fruit_series[10:20] #access the 10th to 20th element of the series. Slicing the items\n",
    "\n",
    "#creating a series with labelled index\n",
    "fruits_label_series = pd.Series( np.random.choices(fruits, 100000), index = ['a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z'.split(',')])\n",
    "\n",
    "\n",
    "#squaree some sliced elements\n",
    "squared = fruit_series[10:20] ** 2\n",
    "or \n",
    "#using for loop\n",
    "squared = []\n",
    "for value in fruit_series[10:20]:\n",
    "    squared.append(value ** 2) #to store in a different series\n",
    "\n",
    "    or using list comprehension\n",
    "squared = [value ** 2 for value in fruit_series[10:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATAFRAMES\n",
    "these are multi-dimensional arrays.\n",
    "A table of rows and columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "  \"calories\": [420, 380, 390],\n",
    "  \"duration\": [50, 40, 45]\n",
    "}\n",
    "\n",
    "myvar = pd.DataFrame(data)\n",
    "\n",
    "print(myvar)\n",
    "\n",
    "#output \n",
    "#     calories  duration\n",
    "#0       420        50\n",
    "#1       380        40\n",
    "#2       390        45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to locate a row\n",
    "#Pandas use the loc attribute to return one or more specified row(s)\n",
    "print(df.loc[0])\n",
    "\n",
    "#OUTPUT\n",
    "#calories    420\n",
    "#duration     50\n",
    "\n",
    "\n",
    "#Return row 0 and 1:\n",
    "print(df.loc[0,1])\n",
    "\n",
    "#output\n",
    " \"\"\"calories  duration\n",
    " # 0       420        50\n",
    " # 1       380        40\n",
    " \"\"\"\n",
    "\n",
    " #NAMED INDEXES\n",
    " data = {\n",
    "  \"calories\": [420, 380, 390],\n",
    "  \"duration\": [50, 40, 45]\n",
    "}\n",
    "\n",
    "df =pd.DataFrame(data, index = ['Day1','Day2','Day3'], columns = ['calories','duration'])\n",
    "print(df)\n",
    "\"\"\"OUTPUT\n",
    "   calories  duration\n",
    "  day1       420        50\n",
    "  day2       380        40\n",
    "  day3       390        45\n",
    "  \"\"\"\n",
    "\n",
    "#Locate Named Indexes\n",
    "#refer to the named index:\n",
    "print(df.loc[\"day2\"])#same as return the value for index number 1\n",
    "\n",
    "\"\"\"OUTPUT\n",
    "calories    380\n",
    "  duration     40\n",
    "\n",
    "\n",
    "  Differences between loc and iloc   \n",
    "    .loc[row_label, column_label]\n",
    "\n",
    ".iloc[row_position, column_position]\n",
    "\n",
    "Selecting via a single value : df.loc[\"day2\"]  df.iloc[1] - to return the second row\n",
    "Selecting via a list of values : df.loc[[\"day1\", \"day2\"]]  df.iloc[[0, 1]] //to return both the first and second row \n",
    "Selecting a range of data via slice : df.loc[\"day1\":\"day5\"]  df.iloc[0:5]  //to return the first 5 rows\n",
    "Selecting via conditions and callable : df.loc[lambda df: df[\"calories\"] > 380]  df.iloc[lambda x: x.index % 2 == 0] \n",
    "//to return the rows where the calories are greater than 380 and the rows where the index is an even number\n",
    "\n",
    "\n",
    "# Slicing column labels\n",
    "rows=['Thu', 'Fri'] // since Thur and Fri are used as indexes we want to print Thur row and Fri row\n",
    "df.loc[rows, 'Temperature':'Humidity' ] // to print the columns from Temperature to Humidity for the rows Thur and Fri\n",
    "\n",
    "# Slicing row labels\n",
    "cols = ['Temperature', 'Wind']\n",
    "df.loc['Mon':'Thu', cols] // to print the rows from Mon to Thu for the columns Temperature and Wind\n",
    "\n",
    "# One condition\n",
    "df.loc[df.Humidity > 50, :] // to print the rows where the Humidity is greater than 50 and all the columns\n",
    "\n",
    "# multiple conditions\n",
    "df.loc[\n",
    "    (df.Humidity > 50) & (df.Weather == 'Shower'),  // to print the rows where the Humidity is greater than 50 and the Weather is Shower\n",
    "    ['Temperature','Wind'], // to print the columns Temperature and Wind\n",
    "\n",
    "    \n",
    "    ## multiple conditions\n",
    "df.loc[\n",
    "     [:], // to print all the rows\n",
    "    (df.Humidity > 50) & (df.Weather == 'Shower') \n",
    "    ] // to print the columns where the Humidity is greater than 50 and the Weather is Shower  \n",
    "\n",
    "    df[(condition1) | (condition2)] -> Returns rows where either condition is true\n",
    "\n",
    "df[(condition1) & (condition2)] -> Returns rows where both conditions are true\n",
    "\n",
    "# \n",
    "df.loc[(df.Year == 1950) & (df.Stage == 'Group 3') , ['City']] // to print the column city values where the year is 1950 and the stage is Group 3\n",
    "    or  df.loc[(df['Year'] == 1950) & (df['Stage'] == 'Group 3'),['Attendance']]\n",
    "  \n",
    "\n",
    "\n",
    " \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATAFRAME ATTRIBUTES\n",
    "\"\"\"The shape attribute returns a tuple with the number of rows and columns in the DataFrame:\n",
    "Index attribute - returns the row labels of the DataFrame\n",
    "  df.index\n",
    "  output:\n",
    "RangeIndex(start=0, stop=178, step=1)\n",
    "\n",
    "Columns attribute - returns the column labels of the DataFrame\n",
    "   df.columns\n",
    "   output:\n",
    "Index(['calories', 'duration'], dtype='object')\n",
    "\n",
    "Values attribute - returns the actual data in the DataFrame as an NDarray\n",
    "    df.values\n",
    "    output:\n",
    "array([[420, 50],\n",
    "       [380, 40],\n",
    "       [390, 45]])\n",
    "\n",
    "T attribute - transposes data (rows become columns and vice versa)\n",
    "    df.T\n",
    "    output:\n",
    "                0    1    2\n",
    "   calories    420  380  390\n",
    "    duration     50   40   45\n",
    "\n",
    " .dtypes returns the data types of all columns in the DataFrame \n",
    "    df.dtypes\n",
    "    output:\n",
    "calories    int64\n",
    "duration    int64\n",
    "dtype: object\n",
    "\n",
    " df.size returns the number of elements in the DataFrame\n",
    "    df.size\n",
    "    output:                 \n",
    "     6\n",
    "  \n",
    "  df.empty returns True if the DataFrame is empty (no rows or columns), False otherwise\n",
    "    df.empty\n",
    "    output:\n",
    "    False\n",
    "\n",
    "  df.shape returns a tuple representing the dimensionality of the DataFrame\n",
    "    df.shape\n",
    "    output:\n",
    "    (3, 2)\n",
    "\n",
    "    pd.concat([df1, df2]) -> Concatenates two DataFrames\n",
    "    # Combine home_team_names with away_team_names and remove duplicates\n",
    "country_names = pd.concat([home_team_names, away_team_names]).unique() //home_team_names and away_team_names are two dataframes\n",
    "\n",
    "df['Total Goals'] = df['Home Team Goals'] + df['Away Team Goals'] //to add a new column to the dataframe\n",
    "\n",
    "# Display all records containing the string 'Korea'\n",
    "df.loc[df['Home Team Name'].str.contains('Korea'), 'Home Team Name'] //to display all records containing the string Korea in the Home Team Name column\n",
    "\n",
    "df['Home Team Name'] = df['Home Team Name'].replace({'Korea Republic': 'Korea', 'Korea DPR': 'Korea'}) //to replace the values in the Home Team Name column\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING FILES INTO DATA FRAMES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for csv files\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "print(df.to_string()) #.to_string() method is used to print the entire DataFrame.\n",
    "#If you have a large DataFrame with many rows, Pandas will only return the first 5 rows, and the last 5 rows:\n",
    "#or print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading json files\n",
    "import pandas as pd\n",
    "data = pd.read_json('datajson.json')  \n",
    "print(data)      \n",
    "\n",
    "#creating tables based on datajson file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. viewing the data\n",
    "#a) use of the head() method ->returns the headers and a specified number of rows, starting from the top.\n",
    "#using csv as an example\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data.csv')      \n",
    "\n",
    "print(data.head(5))     #returns the first 5 rows of the DataFrame\n",
    "\n",
    "\n",
    "#b) use of the tail() method ->returns the headers and a specified number of rows, starting from the bottom.\n",
    "print(data.tail(5))    #returns the last 5 rows of the DataFrame\n",
    "\n",
    "\n",
    "#C) use of the info() method ->returns a short summary of the DataFrame. Also tells us how many null values are in the dataset.\n",
    "print(data.info()) #returns a short summary of the dataset\n",
    "\n",
    "print(data.loc[0]) #returns the first row of the DataFrame with their column names\n",
    "print(data.loc[0:5]) #returns the first 5 rows of the DataFrame\n",
    "print(data.loc[7, \"Duration\"]) #returns the value in the 7th row in the \"Duration\" column\n",
    "print(data.loc[[0, 1, 2], [\"Duration\", \"Calories\"]]) #returns the value of the rows with the specified index values, and the specified columns.\n",
    "print(data.loc[0:5, \"Duration\"]) #returns the value of the specified rows, and the specified column.\n",
    "#print(data.loc[7,'Duration']) = 45 #changes the value in the 7th row in the \"Duration\" column to 45\n",
    "data.loc['Duration'] #returns the value of the \"Duration\" column\n",
    "data.loc['Duration'].mean() #returns the mean value for the \"Duration\" column\n",
    "data.loc[0:5,['Duration','Calories']] #returns the value of the rows with the specified index values, and the specified columns.\n",
    "\n",
    "\n",
    "print(data.describe()) #returns the statistical summary of the dataset. Information about the number of rows, the headers of the columns\n",
    "#the rows will be in form of statistics\n",
    "print(data.mean()) #returns the mean value for each column\n",
    "print(data.min()) #returns the lowest value for each column\n",
    "print(data.max()) #returns the highest value for each column\n",
    "\n",
    "#standard deviation\n",
    "print(data.std()) #returns the standard deviation for each column\n",
    "# Get the 90% quantile for all numerical columns\n",
    "#select numerical columns only\n",
    "numerical_columns = df.select_dtypes(include=['float64','int64'])\n",
    "quantiles_90 = numerical_columns.quantile(q=0.9)\n",
    "\n",
    "quantiles_90\n",
    "\n",
    "#for all columns\n",
    "quantiles_90 = df.quantile(q=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### empty cells USING dropna(),fillna()\n",
    "\n",
    "on replacing empty values:\n",
    "\n",
    "with mean- slightly increases the median and std drops the distribution has a larger mass towards the center\n",
    "\n",
    "with median-The variance is reduced, while the mean is slightly lowered. You can once again see that there is a larger mass of data near the center of the distribution.\n",
    "\n",
    "Dropping missing values leaves the distribution and associated measures of centrality unchanged, but at the cost of throwing away data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting NaNs- not a number \n",
    "df.isna() #returns a DataFrame with True values where NaNs are present, and False values where the data is not NaN.\n",
    "df.isna().sum() #returns the number of NaN values in each column of the DataFrame. It sums the True values in each column\n",
    "\"\"\"PassengerId 0\n",
    "Survived 0\n",
    "Pclass 0\n",
    "Name 0\n",
    "Sex 0\n",
    "Age 177 \n",
    "\"\"\"\n",
    "#to count the number of NaNs in each column.\n",
    "(df['Enabled'].isna()).sum() #returns the number of NaN values in the \"Enabled\" column of the DataFrame.\n",
    "#output\n",
    "#0\n",
    "df.isna().sum() #returns the number of NaN values in each column of the DataFrame.\n",
    "\n",
    "#for categorical data use .unique() method\n",
    "df['column_name'].unique() #returns an array of unique values in the \"column_name\" column of the DataFrame.\n",
    "df.unique() #returns an array of unique values in the DataFrame.\n",
    "\n",
    "\n",
    "#1. empty cells - return a new dataFrame with cleaned out empty cells\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "#remove empty cells\n",
    "new_data = data.dropna() #dropna() method removes empty cells in the DataFrame. \n",
    "print(new_data.to_string()) #.to_string() method is used to print the entire DataFrame.\n",
    "\n",
    "df.dropna(subset=['Pclass'], inplace=True) #removes rows with empty cells in the \"Pclass\" column of the DataFrame.\n",
    "\n",
    "\n",
    "df=df[df['Pclass'].str.contains('\\?') != True] #returns a dataframe with rows that don't have special characters in the\n",
    "#Pclass column\n",
    "\n",
    "#2. removing rows with null values\n",
    "import pandas as pd\n",
    "#load the csv file\n",
    "data = pd.read_csv('data.csv')  \n",
    "#remove rows with null values\n",
    "new_data = data.dropna(inplace = True) #dropna() method removes empty cells in the DataFrame.\n",
    "print(new_data.to_string()) #.to_string() method is used to print the entire DataFrame.\n",
    "\n",
    "#3. replacing empty cells with a specific value\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data.csv')\n",
    "new_data =data.fillna(130, inplace = True) #fillna() method allows us to replace empty cells with a value\n",
    "or\n",
    "animals_name_filled = animals.fillna({'name':'UNKNOWN'}) # {col_name:new_value}\n",
    "\n",
    "\n",
    "#4. replacing only for a specific column. \n",
    "import pandas as  pd\n",
    "data = pd.read_csv('data.csv')  \n",
    "#replace empty cells in a column with a value \n",
    "data['Calories'] = data['Calories'].fillna(130)   #fillna() method allows us to replace empty cells with a value\n",
    "#for the column named \"Calories\"\n",
    "or\n",
    "\n",
    "data['Calories'].fillna(130,inplace=True ) \n",
    "\n",
    "#5. replacing empty cells with the mean of that particular column\n",
    "import pandas as pd\n",
    "#read the csv file\n",
    "data = pd.read_csv('data.csv')      \n",
    "\n",
    "#get the mean of the values in the column \"Calories\"\n",
    "x = data['Calories'].mean()\n",
    "\n",
    "#replace empty cells with the mean of the column\n",
    "new_data = data['Calories'].fillna(x ,inplace = True)\n",
    "\n",
    "#6. replacing empty cells with the median of that particular column\n",
    "import pandas as pd\n",
    "#read the csv file\n",
    "data = pd.read_csv('data.csv')\n",
    "#get the median of the values in the column \"Calories\"\n",
    "median = data['Calories'].median()\n",
    "\n",
    "#replace empty cells with the median of the column\n",
    "new_data = data['Calories'].fillna(median,inplace = True)\n",
    "\n",
    "#7. replacing empty cells with the mode of that particular column\n",
    "mode = data['Calories'].mode()[0]\n",
    "\n",
    "new_data = data['Calories'].fillna(mode, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning data of wrong format - for datetime\n",
    "use pd.to_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data into a correct format of data\n",
    "#convert date\n",
    "import pandas as pd\n",
    "data = pd.read_csv('DATACSV.csv')\n",
    "#change the date column so that all the values are in datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'])#changes the values in this column to date format using the to_datetime() method.\n",
    "print(data.to_string())\n",
    "\n",
    "#for values of date in wrong formatted they will be put in the correct order of datetime\n",
    "#for values which are empty it will run a NaT value. Meaning it's not a Time value\n",
    "\n",
    "#to get rid of the empty cell\n",
    "df.dropna(subset=['Date'], inplace = True)\n",
    "\n",
    "#using strftime  and pd.to_datetime\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%Y-%m-%d').strftime('%d-%m-%Y') #changes the values in this column to date format using the to_datetime() method\n",
    "then \n",
    "pd.DateTime(data['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing wrong data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for example a value in row 7 column \"time\" was 450 instead of 45\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data.csv')\n",
    "data.loc[7, 'Time'] = 45 #change the value from 450 to 45\n",
    "\n",
    "\n",
    "#for a large set of data you can use conditionsto change the values\n",
    "for x in data.index: #iterate over the rows in the DataFrame\n",
    "    if data.loc[x, \"Duration\"] > 60: #check if the value in the \"Duration\" column is higher than 60\n",
    "        data.loc[x, \"Duration\"] = 60 #change the value to 60\n",
    "\n",
    "#to remove wrong data\n",
    "for x in data.index: #iterate through each row\n",
    "    if data.loc[x,'Calories'] > 75:\n",
    "        data.drop(x, inplace = True) #remove the row if the value in the \"Calories\" column is higher than 75\n",
    "        #subset = ['Calories'] specifies that only the \"Calories\" column should be checked for empty cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "\n",
    "df.duplicated()-to check duplicate rows\n",
    "\n",
    "df.duplicated(keep=False)-This filters the DataFrame to include only rows that are marked as duplicates.\n",
    "\n",
    "df.drop_duplicates(inplace=True)- remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Discovering duplicates\n",
    "print(df.duplicated()) #returns a boolean value for each row, True if a row is a duplicate of a previous row, otherwise False.\n",
    "#if row 4 is a duplicate of row 3, then the value for row 4 will be True, otherwise False.\n",
    "\n",
    "df.duplicated(keep=False) #returns True for all duplicates, regardless of their first occurrence.\n",
    "\n",
    "duplicates = df[df.duplicated(subset='PassengerId')]  #returns a new DataFrame containing only the duplicates in the original DataFrame.\n",
    "# Use keep=False to keep all duplicates and sort_values to put duplicates next to each other\n",
    "df[df.duplicated(keep=False)].sort_values(by='business_id')\n",
    "\n",
    "#2. removing duplicates\n",
    "df.drop_duplicates(inplace = True) #removes duplicates\n",
    "print(df.to_string()) #prints the DataFrame after the duplicates have been removed.\n",
    "unique_sets = df.drop_duplicates(subset=['prod_id']) #removes duplicates based on the \"prod_id\" column.\n",
    "total_pieces = unique_sets['piece_count'].sum() #sums the values in the \"piece_count\" column of the unique_sets DataFrame.\n",
    "#The drop_duplicates() method returns a new DataFrame, but we can also remove duplicates in the original DataFrame by setting the inplace parameter to True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.pivot()-make a new dataset with some columns as indexes, others as columns and values\n",
    "\n",
    "Both are used for reshaping a dataset but\n",
    "\n",
    ".pivot - is used for reshaping the dataset given the indexes and columns\n",
    "\n",
    "df.pivot_table()-I used for advanced reshaping but gives an error when there are duplicates.Thus you need to  to use an aggfunc=sum,mean to replace the duplicated values with the sum, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "\n",
    "    index: Column to use to make new frame’s index.\n",
    "    columns: Column to use to make new frame’s columns.\n",
    "    values: Column(s) to populate the new frame’s values.\n",
    "    aggfunc: Function to aggregate values. Default is numpy.mean.\n",
    "    fill_value: Value to replace missing values.\n",
    "    margins: Add all rows/columns (subtotals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot(index='A', columns='B', values='C') #to pivot the data\n",
    "df.pivot_table(index='A', columns='B', values='C', aggfunc='mean') #Here, the aggfunc='mean' parameter is used to aggregate duplicate values by their mean.\n",
    "\"\"\"\n",
    "data = {\n",
    "    'date': ['2024-01-01', '2024-01-01', '2024-01-02', '2024-01-02', '2024-01-01'],\n",
    "    'city': ['New York', 'Los Angeles', 'New York', 'Los Angeles', 'New York'],\n",
    "    'temperature': [30, 75, 28, 78, 32]\n",
    "}\n",
    "\"\"\"\n",
    "df = pd.DataFrame(data)\n",
    "df.pivot(index='date', columns='city', values='temperature') #to pivot the data\n",
    "\"\"\"         date         city  temperature\n",
    "0  2024-01-01     New York           30\n",
    "1  2024-01-01  Los Angeles           75\n",
    "2  2024-01-02     New York           28\n",
    "3  2024-01-02  Los Angeles           78\n",
    "\"\"\"\n",
    "df.pivot_table(index='date', columns='city', values='temperature', aggfunc='mean') #Here, the aggfunc='mean' parameter is used to aggregate duplicate values by their mean.\n",
    "\"\"\"city         Los Angeles  New York\n",
    "date\n",
    "2024-01-01            75        30\n",
    "2024-01-02            78        28\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##     stack: Turns columns into rows, making the DataFrame longer.\n",
    "    unstack: Turns rows into columns, making the DataFrame wider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Store A  Store B\n",
      "Product 1       10       15\n",
      "Product 2       20       25\n",
      "Product 3       30       35\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store A</th>\n",
       "      <th>Store B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Product 1</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product 2</th>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product 3</th>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Store A  Store B\n",
       "Product 1       10       15\n",
       "Product 2       20       25\n",
       "Product 3       30       35"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Store A': [10, 20, 30],\n",
    "    'Store B': [15, 25, 35]\n",
    "}\n",
    "df = pd.DataFrame(data, index=['Product 1', 'Product 2', 'Product 3'])\n",
    "print(df)\n",
    "\n",
    "\"\"\"\n",
    "          Store A  Store B\n",
    "Product 1       10       15\n",
    "Product 2       20       25\n",
    "Product 3       30       35\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#using .stack\n",
    "stacked_df = df.stack() #TURNS columns into rows\n",
    "stacked_df\n",
    "\n",
    "\"\"\"\n",
    "Product 1  Store A    10\n",
    "           Store B    15\n",
    "Product 2  Store A    20\n",
    "           Store B    25\n",
    "Product 3  Store A    30\n",
    "           Store B    35\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#using .unstack()\n",
    "unstacked_df = stacked_df.unstack() #TURNS rows into columns.Reverses stack\n",
    "unstacked_df\n",
    "\n",
    "\"\"\"\n",
    "\tStore A\tStore B\n",
    "Product 1\t10\t15\n",
    "Product 2\t20\t25\n",
    "Product 3\t30\t35\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Correlation\n",
    "\n",
    "finding the relationship between the different columns\n",
    "\n",
    "Anywhere from 0.6 upwards and -0.6 upwards indicates a positive correlation.   Meaning when value of one column \n",
    "\n",
    "increases other columns will increase as well\n",
    "\n",
    "1.0 and -1.0 indicates perfect positive correlation\n",
    "\n",
    "0.2 and below and -0.2 indicates both weak postive and weak negative correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "print(df.corr()) #table with a lot of numbers that represents how well the relationship is between two columns.\n",
    "\n",
    "\n",
    "\"\"\"output\n",
    "          Duration     Pulse  Maxpulse  Calories\n",
    "  Duration  1.000000 -0.155408  0.009403  0.922721\n",
    "  Pulse    -0.155408  1.000000  0.786535  0.025120\n",
    "  Maxpulse  0.009403  0.786535  1.000000  0.203814\n",
    "  Calories  0.922721  0.025120  0.203814  1.000000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming values in DATAFRAMES using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming Values using  both .map, .apply, (lambda function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFORMING DATA\n",
    "# Using .map() to Transform Values\n",
    "#1. PASSING IN A DICTIONARY IS One of the most straightforward ways to use the .map() method on a pandas Series is\n",
    "# with a dictionary of values you want to use to replace other values.\n",
    "df['DIVISION'] = df['DIVISION'].map(division_mapping) #division_mapping is a dictionary that maps the division names to their corresponding numbers\n",
    "\n",
    "#2. PASSING IN A FUNCTION Note that for a pandas Series, the .apply() method can be used interchangeably with the .map() method \n",
    "#when a function is provided\n",
    "df['On_N_Line'] = df['LINENAME'].map(contains_n) #contains_n is a function that checks if the letter 'N' is in the LINENAME column\n",
    "df['On_N_Line'] = df['LINENAME'].apply(contains_n) \n",
    "\n",
    "#As shorthand, since this function is only one line we could also pass a lambda function to determine whether or not each row was on the N line or not,\n",
    "# rather than declaring a separate function:\n",
    "df['On_N_Line'] = df['LINENAME'].map(lambda x: 'N' in x)\n",
    "df['On_N_Line'] = df['LINENAME'].apply(lambda x: 'N' in x)\n",
    "\n",
    "#.str.contains() method to check if a string contains a certain substring\n",
    "df['On_N_Line'] = df['LINENAME'].str.contains('N', regex=False) #regex=False to avoid treating the letter 'N' as a regular expression\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMING COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming columns\n",
    ".columns.str.strip()\n",
    "\n",
    ".columns.str.lower()\n",
    "\n",
    ".columns.str.upper()\n",
    "\n",
    "df.drop(columns=['new_fruits'],inplace=True)\n",
    "\n",
    "df.drop('column_name',axis=1)\n",
    "\n",
    "df.drop(0,axis=0)\n",
    "\n",
    "df.rename(columns={'old_name':'new_name'},inplace=True) // for column renaming \n",
    "\n",
    "df['Fruits'].replace({'apple':'pear'}) // to rename a value with another\n",
    "\n",
    "df['column_name'].map/astype(int) -to convert the datatype to sth else. eg string to int. it is used interchangeably with .map\n",
    "\n",
    "pd.to_datetime(df['DATE'],format='%m/%d/%Y')// to convert from dd/mm/yyyy to yyyy/mm/dd\n",
    "\n",
    "or\n",
    "df['date'].dt.strftime(\"%d-%m-%Y\") to convert from yyy-mm-dd to dd-mm-yyyy\n",
    "\n",
    "df['DATE'].dt.day_name() - returns the day of the week for each date in the DATE column\n",
    "\n",
    "df= df.set_index(df['DATE'])//Sets the index of the dataframe to be the DATE column\n",
    "\n",
    "df.reset_index()//resets the index to the previous one\n",
    "\n",
    "df_sorted = df.sort_values(by='Num_Lines', ascending=False)//sort using Num Lines column in descending order\n",
    "\n",
    "df['Enabled'].unique()-returns the unique values\n",
    "\n",
    "df['Enabled'].value_counts()-returns the counts of the unique values\n",
    "\n",
    "numeric_df = df.select_dtypes(include=['float64','int64'])//new dataframe with only the integer columns or floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.columns = df.columns.str.strip() #removes leading and trailing whitespaces from the column names\n",
    "or \n",
    "df.columns\n",
    "#then \n",
    "[col.strip() for col in df.columns] #removes leading and trailing whitespaces from the column names\n",
    "or\n",
    "df.columns.map(lambda col: col.strip()) #removes leading and trailing whitespaces from the column names\n",
    "df.columns = df.columns.str.lower() #converts the column names to lowercase\n",
    "df.columns = df.columns.str.upper() #converts the column names to uppercase\n",
    "\n",
    "#RENAMING COLUMNS\n",
    ".rename() method to rename columns in a DataFrame\n",
    "df.rename(columns={'old_name': 'new_name'}, inplace=True) #rename the column 'old_name' to 'new_name' and modify the original DataFrame directly\n",
    " #inplace= True is added to modify the original DataFrame directly\n",
    "df.rename(columns={'C/A' : 'CONTROL_AREA'}, inplace=True)\n",
    "df.rename(columns={'ENTRIES': 'TOTAL_ENTRIES'})\n",
    "\n",
    "#renaming an index\n",
    "df.rename(index={'A':'X'}, inplace=True) #rename the index 'A' to 'X'.inplace=True is added to modify the original DataFrame directly\n",
    "df.rename(index={'A':'X'})\n",
    "\n",
    "\n",
    "#RENAMING A VALUE\n",
    "df['column_name'] = df['column_name'].replace('old_value', 'new_value') #replaces the value 'old_value' in the 'column_name' column with 'new_value'\n",
    "df['fruits'] = df['fruits'].replace('Banana', 'Pear')\n",
    "#RENAMING MULTIPLE VALUES\n",
    "df['fruits'] = df['fruits'].replace({'apple': 'pear', 'banana': 'mango'})#as long there in the same column\n",
    "\n",
    "otherwise you can use the .map() method to replace multiple values in a column with a dictionary\n",
    "\n",
    "\n",
    "#DROPPING COLUMNS\n",
    "#Let's say we have determined that the DESC column doesn't matter. We can test out dropping it like this:\n",
    "df.drop('DESC', axis=1) #drops the column 'DESC'. By default, df.drop() tries to drop rows (axis=0) with the specified index, \n",
    "#but we want to drop columns (axis=1).\n",
    "df.drop(columns=['new_fruits'], inplace=True)\n",
    "df.drop(columns=['new_fruits', 'new_vegetables'], inplace=True) #drops the columns 'new_fruits' and 'new_vegetables'\n",
    "\n",
    "#DROPPING MULTIPLE COLUMNS\n",
    "df.drop(['DESC', 'DIVISION'], axis=1) #drops the columns 'DESC' and 'DIVISION'\n",
    "\n",
    "#DROPPING ROWS\n",
    "df.drop(0, axis=0) #drops the row with index 0. By default, df.drop() tries to drop columns (axis=1) with the specified index,\n",
    "#drop using df.drop(.index)\n",
    "df = df.drop(df[df['city'] == 'Chicago'].index) #drop rows where the city is Chicago\n",
    "\n",
    "# Delete rows where the city is Chicago and the age is less than 35\n",
    "df = df.drop(df[(df['city'] == 'Chicago') & (df['age'] <= 35)].index)\n",
    "\n",
    "\n",
    ".astype() method to convert the data type of a column in a DataFrame\n",
    "df['column_name'] = df['column_name'].astype('int') #converts the data type of the column 'column_name' to integer\n",
    "df.loc[:5, 'ENTRIES'].astype(int) #converts the first 5 rows of the 'ENTRIES' column to integers\n",
    "or\n",
    "df.loc[:5, 'ENTRIES'].map(int) #converts the first 5 rows of the 'ENTRIES' column to integers\n",
    "\n",
    "#to convert the 'DATE' column to a datetime data type\n",
    "pd.to_datetime()\n",
    " pd.to_datetime(df['DATE'], format='%m/%d/%Y') #converts the 'DATE' column to datetime format\n",
    "or \n",
    "pd.to_datetime(df['DATE']).head() #converts the 'DATE' column to datetime format\n",
    "or\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df.head(2)\n",
    "output for this is 2018-06-30 \n",
    "\n",
    "\n",
    ".sample() method to randomly sample rows from a DataFrame\n",
    "# Make a sample of rows so we can see various dates\n",
    "date_sample = df['DATE'].sample(n=10, random_state=0) #randomly samples 10 rows from the 'DATE' column. random_state=0 ensures that the sample is reproducible.\n",
    "\n",
    ".dt. - stores all the datetime methods\n",
    "#1. Extract the day of the week from the date\n",
    "dt.day_name() #returns the day of the week for each date in the 'DATE' column\n",
    "df['DATE'].dt.day_name() #returns the day of the week for each date in the 'DATE' column\n",
    "or\n",
    "#2. dt.round() method to round datetime values to the nearest specified unit\n",
    "rounding to the nearest 7 days:\n",
    "df['DATE'].dt.round('7D')\n",
    "\n",
    "#3. dt.to_period() method to convert datetime values to periods\n",
    "df['DATE'].dt.to_period('M') #converts the 'DATE' column to periods of months.OUtput is 2018-06\n",
    "or\n",
    "df['DATE'].dt.to_period('W') #converts the 'DATE' column to periods of weeks. Output is 2018-06-25/2018-07-01\n",
    "\n",
    "#4. dt.to_timestamp() method to convert periods to timestamps\n",
    "df['DATE'].dt.to_timestamp() #converts the 'DATE' column to timestamps.OUtput is 2018-06-30 00:00:00\n",
    "\n",
    "#5. dt.to_timedelta() method to convert datetime values to timedeltas       \n",
    "df['DATE'].dt.to_timedelta() #converts the 'DATE' column to timedeltas. Output is 0 days\n",
    "\n",
    "Setting a New Index\n",
    "#set one of the columns as the index of the DataFrame, such as when graphing.\n",
    "df = df.set_index('DATE') #sets the 'DATE' column as the index of the DataFrame\n",
    "df.head()\n",
    "\n",
    "Or the opposite, resetting the index so that the current index becomes a column and a new index is created:\n",
    "\n",
    "df.reset_index() #resets the index of the DataFrame\n",
    "#output is the index column is added to the DataFrame and the original index is reset. eg 0  2018-06-30  0  1  2018-06-30  1  2  2018-06-30  2  3  2018-06-30  3  4  2018-06-30  4\n",
    "\n",
    "df_sorted = df.sort_values(by='Num_Lines', ascending=False)   #sorts the DataFrame by the 'Num_Lines' column in descending order\n",
    "\n",
    "df['Enabled'].unique() shows us all the unique values contained in the column.\n",
    "df['review_difficulty'].unique() #output is ['Average' 'Easy' 'Challenging' 'Very Easy' 'Very Challenging']\n",
    "\n",
    "df['Enabled'].value_counts() shows us a count for how many times each unique value is present in a dataset, giving us a feel for the distribution of values in the column.\n",
    " df['review_difficulty'].value_counts() #output is Easy  102 Average  102 Challenging  102 Very Easy  102 Very Challenging  102\n",
    " #Sometimes, we'll need to make changes to our dataset, or to compute functions on our data that aren't built-in to Pandas.\n",
    "#We can do this by passing lambda values into the apply() method when working with Pandas series, and the .applymap() method when working with Pandas DataFrames.\n",
    "#For example, let's say we want to convert all the values in the 'Num_Lines' column to strings:\n",
    "display(df['Age'].apply(lambda x: x**2).head()) #squares each value in the 'Age' column\n",
    "\n",
    "string_df = df.applymap(lambda x: str(x)) #converts all the values in the DataFrame to strings\n",
    "\n",
    " Select only the numeric columns\n",
    "numeric_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .groupby() method\n",
    "\n",
    " for aggregation of data\n",
    "\n",
    " for transformation of data\n",
    " \n",
    " for filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. for aggregate functions min(), max(), mean(), count(),median()\n",
    "# Calculate the sum of sales for each category\n",
    "category_sales_sum = df.groupby('Category')['Sales'].sum()\n",
    "\"\"\" \n",
    "Category\n",
    "A    250\n",
    "B    450\n",
    "C    650\n",
    "Name: Sales, dtype: int64\n",
    "\"\"\"\n",
    "\n",
    "df.groupby(['Sex', 'Pclass']).mean() #group by multiple columns. sex and pclass will be first and 2nd columns.Returns a dataframe will all columns\n",
    "\n",
    "df.groupby(['Sex', 'Pclass'])['Survived'].mean()#slices the dataframe to only show the 'Survived' column.\n",
    "\n",
    "\n",
    "#for multiple aggregations\n",
    "# Perform multiple aggregations\n",
    "aggregated_df = df.groupby('Category').agg(\n",
    "    total_sales=('Sales', 'sum'),\n",
    "    average_sales=('Sales', 'mean'),\n",
    "    count=('Sales', 'size')\n",
    ")\n",
    "\"\"\"\n",
    "            total_sales  average_sales  count\n",
    "Category                                    \n",
    "A                 250          125.0      2\n",
    "B                 450          225.0      2\n",
    "C                 650          325.0      2\n",
    "\"\"\"\n",
    "#2. TRANSFORMING DATA\n",
    "# Normalize sales within each category\n",
    "df['Normalized Sales'] = df.groupby('Category')['Sales'].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "\"\"\"\n",
    "Category  Sales  Normalized_Sales\n",
    "0        A    100         -1.0\n",
    "1        A    150          1.0\n",
    "2        B    200         -1.0\n",
    "3        B    250          1.0\n",
    "4        C    300         -1.0\n",
    "5        C    350          1.0\n",
    "\"\"\"\n",
    "\n",
    "#3. FILTERING DATA\n",
    "# Filter categories with total sales greater than 400\n",
    "filtered_df = df.groupby('Category').filter(lambda x: x['Sales'].sum() > 400)\n",
    "print(filtered_df)\n",
    "\"\"\"\n",
    "  Category  Sales\n",
    "2        B    200\n",
    "3        B    250\n",
    "4        C    300\n",
    "5        C    350\n",
    "\"\"\"\n",
    "grouped = df.groupby(['State','Gender'])[['Deaths','Population']].agg([ 'mean','min','max','std'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating dataframe using fake data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(777) #sets the random seed to 777\n",
    "\n",
    "data = pd.DataFrame({'A':np.random.randn(365).cumsum(), #creates a DataFrame with 365 rows and 3 columns\n",
    "                    'B':np.random.randn(365).cumsum() + 25,  #cumsum() method calculates the cumulative sum of the values in the array\n",
    "                    'C':np.random.randn(365).cumsum() - 25},  #-25 shifts the values down by 25\n",
    "                     index = pd.date_range('1/1/2018', periods = 365)) #creates a date range starting from 1/1/2018 with 365 periods\n",
    "\n",
    "data.head()\n",
    "\n",
    "\t            A        \tB\t      C\n",
    "2018-01-01\t-0.468209\t25.435990\t-22.997943\n",
    "2018-01-02\t-1.291034\t26.479220\t-22.673404\n",
    "2018-01-03\t-1.356414\t25.832356\t-21.669027\n",
    "2018-01-04\t-2.069776\t26.456703\t-21.408310\n",
    "2018-01-05\t-1.163425\t25.864281\t-22.685208\n",
    "\n",
    "data.plot() #plots the data in the DataFrame\n",
    "data.plot('A', 'B', kind='scatter'); #creates a scatter plot of columns 'A' and 'B'\n",
    "data.plot.scatter('A', 'C',  \n",
    "                  c = 'B',\n",
    "                  s = data['B'],\n",
    "                  colormap = 'viridis'); #creates a scatter plot of columns 'A' and 'C' with 'B' as the color and size of the points\n",
    "data.plot.box(); #creates a box plot of the data in the DataFrame\n",
    "data.plot.hist(alpha=0.7); #creates a histogram of the data in the DataFrame\n",
    "data.plot.kde(); #creates a kernel density estimate plot of the data in the DataFrame\n",
    "scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde'); #creates a scatter matrix of the data in the DataFrame\n",
    "parallel_coordinates(data, 'B'); #creates a parallel coordinates plot of the data in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA VISUALIZATION WITH PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    " %matplotlib inline  #to display the plots in the Jupyter notebook after the cell that creates the plot  \n",
    "\n",
    "data = pd.read_csv('data.csv') #loads the data from the 'data.csv' file into a DataFrame\n",
    "#1. histogram\n",
    "plt.hist(df['OverallCond'],bins = 10, range =(0,10) , edgecolor = 'black', color ='skyblue') #0 to 10 is the range of the data\n",
    "plt.title('Overall Condition')\n",
    "plt.xlabel('OverallCond')\n",
    "plt.ylabel('Frquency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#subplot\n",
    "plt.subplot(1,2,1) #1 row, 2 columns, 1st subplot\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(10,10)) #creates a 3x4 grid of subplots with a figure size of 10x10\n",
    "x = np.linspace(start=-10, stop=10, num=10*83) #creates an array of 830 values from -10 to 10\n",
    "for i in range(12): #iterates over the range 0-11\n",
    "    row = i//4 #calculates the row number\n",
    "    col = i%4   #calculates the column number\n",
    "    ax = axes[row, col] #selects the current axis\n",
    "    ax.scatter(x, x**i) #creates a scatter plot of x^i\n",
    "    ax.set_title(f'Plot of x^{i}') #sets the title of the plot\n",
    "plt.show()\n",
    "\n",
    "#creating a barchart. \n",
    "#for you to create a bar graph you need to have a dataset that has two columns. One column will be the x-axis and the other column will be the y-axis.\n",
    "#can use the groupby method to create the dataset\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "groupby_sum.plot(kind='bar')# groupby_sum is the DataFrame with the total sales for each category\n",
    "plt.title('Total Deaths by State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Total Deaths')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#to plot a horizontal bar chart\n",
    "pivot.plot(kind='barh', figsize=(15, 8)) #pivot is the DataFrame with the total sales for each category and subcategory\n",
    "\n",
    "pivot.plot(kind='barh', figsize=(15, 8),stacked=True)# pivot has State and Gender,to stack the female and male gender values for each state\n",
    "\n",
    "\n",
    "# Plot a stacked bar chart for eye color by hair color\n",
    "eye_hair_counts = cleaned_heroes_df.groupby(['Hair color', 'Eye color']).size().unstack().fillna(0)#using the columns create a dataset that can be plotted\n",
    "\n",
    "# Plotting\n",
    "eye_hair_counts.plot(kind='bar', stacked=True, figsize=(12, 8))\n",
    "plt.title('Stacked Bar Chart of Eye Color by Hair Color')\n",
    "plt.xlabel('Hair Color')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Eye Color')\n",
    "plt.show()\n",
    "\n",
    "#getting the chi square\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Perform Chi-Square test\n",
    "chi2, p, dof, expected = chi2_contingency(eye_hair_crosstab)\n",
    "\n",
    "print(f\"Chi-Square Statistic: {chi2}\")\n",
    "print(f\"P-Value: {p}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"There is a statistically significant relationship between eye color and hair color.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant relationship between eye color and hair color.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMBDA FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['index'].map(lambda x: x**2).head() #squares each value in the 'index' column\n",
    "string_df = df.applymap(lambda x: str(x)) #converts all the values in the DataFrame to strings\n",
    "df['text'].map(lambda x: len(x.split())) #counts the number of words in each value in the 'text' column\n",
    "\n",
    "\n",
    "#generating rows and columns using // and %\n",
    "for i in range(12):\n",
    "    print(f'i: {i}, Row: {i//4} Column: {i%4}')\n",
    "\"\"\"\n",
    "i: 0, Row: 0 Column: 0\n",
    "i: 1, Row: 0 Column: 1\n",
    "i: 2, Row: 0 Column: 2\n",
    "i: 3, Row: 0 Column: 3\n",
    "i: 4, Row: 1 Column: 0\n",
    "i: 5, Row: 1 Column: 1\n",
    "i: 6, Row: 1 Column: 2\n",
    "i: 7, Row: 1 Column: 3\n",
    "i: 8, Row: 2 Column: 0\n",
    "i: 9, Row: 2 Column: 1\n",
    "i: 10, Row: 2 Column: 2\n",
    "i: 11, Row: 2 Column: 3\n",
    "\"\"\"\n",
    "#get average no of words in the text column\n",
    "df['text'].map(lambda x: len(x.split())).mean() #counts the number of words in each value in the 'text' column and calculates the mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.where()\n",
    "np.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_age = np.where(data['Age'].isna(), mean_age, data['Age']) #fills the empty cells in the 'Age' column with the mean age\n",
    "data['Age'] = filled_age #replaces the 'Age' column with the filled_age values\n",
    "\n",
    "#np.select() method to replace values based on multiple conditions\n",
    "conditions = [data['Age'] < 18, data['Age'] >= 18] #creates a list of conditions\n",
    "choices = ['Child', 'Adult'] #creates a list of choices\n",
    "data['Age Group'] = np.select(conditions, choices, default='Unknown') #replaces values based on the conditions and choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation of dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. using pd.concat()\n",
    "to_concat =[df1,df2,df3]\n",
    "\n",
    "big_df = pd.concat(to_concat) #concatenates the DataFrames in the list to create a new DataFrame\n",
    "\n",
    "\n",
    "#2. using joins\n",
    "outer join- returns all records from both tables\n",
    "inner join- returns only the  records with matching keys in both tables\n",
    "left join- returns all records on the left table + records with matching keys in right table\n",
    "right join - returns all records on the right table + records with matching keys in left table\n",
    "\n",
    "\n",
    "#DataFrames contain a built-in .join() method. \n",
    "# By default, the table calling the .join() method is always the left table. \n",
    "\n",
    "joined_df = df1.join(df2, how='inner') #joins df1 and df2 using an inner join. It can also be left,right,outer\n",
    "#if how not specified it will automatically do a left join\n",
    "\n",
    "\"\"\"\n",
    "NOTE: If both tables contain columns with the same name, the join will throw an error due to a naming collision, \n",
    "since the resulting table would have multiple columns with the same name. To solve this, pass in a value to lsuffix= or rsuffix=, \n",
    "which will append this suffix to the offending columns to resolve the naming collisions.\n",
    "\n",
    "df1_and_4 = df1.join(df4, how='inner', lsuffix='_df1', rsuffix='_df4') #joins df1 and df4 using an inner join, appending '_df1' to the columns from df1 and '_df4' to the columns from df4\n",
    "\"\"\"\n",
    "#pd.concat with axis\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "\n",
    "df2 = pd.DataFrame({'C': [7, 8, 9], 'D': [10, 11, 12]})\n",
    "\n",
    "result = pd.concat([df1, df2], axis=1) #to concatenate horizontally along the  columns specify axis =1\n",
    "\"\"\"\n",
    "Output:\n",
    "\n",
    "   A  B  C   D\n",
    "\n",
    "0  1  4  7  10\n",
    "\n",
    "1  2  5  8  11\n",
    "\n",
    "2  3  6  9  12\n",
    "\"\"\"\n",
    "\n",
    "or \n",
    "df1 = pd.DataFrame({'A': [1, 2, 3],# all have differnt columns\n",
    "\n",
    "                    'B': [4, 5, 6]})\n",
    "\n",
    "df2 = pd.DataFrame({'C': [7, 8, 9],\n",
    "\n",
    "                    'D': [10, 11, 12]})\n",
    "\n",
    "result = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "print(result)\n",
    "\"\"\"\n",
    "A  B  C   D\n",
    "\n",
    "0  1  4  7  10\n",
    "\n",
    "1  2  5  8  11\n",
    "\n",
    "2  3  6  9  12\n",
    "\n",
    "\"\"\"\n",
    "#pd.concat\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "\n",
    "df2 = pd.DataFrame({'A': [7, 8, 9], 'B': [10, 11, 12]})\n",
    "\n",
    "result = pd.concat([df1, df2], axis=0) #to concatenate vertically along the rows specify axis =0\n",
    "\"\"\"\n",
    "Output:\n",
    "\n",
    "   A   B\n",
    "0  1   4\n",
    "\n",
    "1  2   5\n",
    "\n",
    "2  3   6\n",
    "\n",
    "0  7  10\n",
    "\n",
    "1  8  11\n",
    "\n",
    "2  9  12\n",
    "\"\"\"\n",
    "\n",
    "#ignore index\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=[0, 1, 2])\n",
    "\n",
    "df2 = pd.DataFrame({'A': [7, 8, 9], 'B': [10, 11, 12]}, index=[2, 3, 4])\n",
    "\n",
    "result = pd.concat([df1, df2], ignore_index=True) #to ignore the index\n",
    "\"\"\"\n",
    "Output:\n",
    "\n",
    "   A   B\n",
    "0  1   4\n",
    "1  2   5\n",
    "\n",
    "2  3   6\n",
    "\n",
    "3  7  10\n",
    "\n",
    "4  8  11\n",
    "\n",
    "5  9  12\n",
    "\"\"\"\n",
    "#.append and pd.concat\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "\n",
    "df2 = pd.DataFrame({'A': [7, 8, 9], 'B': [10, 11, 12]})\n",
    "\n",
    "result = df1.append(df2) and pd.concat([df1, df2]) #to append the two dataframes\n",
    "\"\"\"\n",
    "   A   B\n",
    "\n",
    "0  1   4\n",
    "\n",
    "1  2   5\n",
    "\n",
    "2  3   6\n",
    "\n",
    "0  7  10\n",
    "\n",
    "1  8  11\n",
    "\n",
    "2  9  12\n",
    "\"\"\"\n",
    "#join\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=[0, 1, 2])\n",
    "\n",
    "df2 = pd.DataFrame({'C': [7, 8, 9], 'D': [10, 11, 12]}, index=[2, 3, 4])\n",
    "\n",
    "result = df1.join(df2)\n",
    "\n",
    "print(result)\n",
    "\"\"\"\n",
    "     A    B    C     D\n",
    "0  1.0  4.0  NaN   NaN\n",
    "1  2.0  5.0  NaN   NaN\n",
    "2  3.0  6.0  7.0  10.0\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list \n",
    "dectractors= df['Rating'].apply(lambda rating: rating < 6).index.tolist() #creates a list of the indices of the detractors\n",
    "or\n",
    "dectractors= list(filter(lambda rating: rating < 6, df['Rating'])) #creates a list of the detractors\n",
    "\n",
    "names= []\n",
    "for i in detractors: #iterates over the detractors list \n",
    "    names.append(df['Name'].iloc[i]) #appends the name of the customer at index i to the names list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .get.level_values -for both outer and innermost level columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outermost level columns\n",
    "name_of_dataset.columns.get_level_values(0) #returns the outermost level of the columns in the DataFrame in form of a list\n",
    "eg. grouped.columns.get_level_values(0) #returns the outermost level of the columns in the grouped DataFrame\n",
    "\n",
    "\n",
    "#inner most level columns\n",
    "name_of_dataset.columns.get_level_values(-1/1) #returns the innermost level of the columns in the DataFrame in form of a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening a dataframe\n",
    "Flattening is defined as Converting or Changing data format to a narrow format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use of .stack and .unstack()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STACKING AND UNSTACKING DataFrames\n",
    "\n",
    ".stack() method has stacked our DataFrame from a flattened format into one with a multi-hierarchical index. \n",
    "\n",
    "This is an easy, quick way to aggregate our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.stack() #changes it from a dataset to an aggregated data\n",
    "\"\"\"\n",
    "0    State                    Alabama\n",
    "     Gender                    Female\n",
    "     Deaths_mean            10753.325\n",
    "     Deaths_min                    10\n",
    "     Deaths_max                116297\n",
    "                            ...      \n",
    "101  Deaths_std           2569.276968\n",
    "     Population_mean         139223.6\n",
    "     Population_min               244\n",
    "     Population_max            694760\n",
    "     Population_std     241359.853616\n",
    "Length: 1020, dtype: object\n",
    "\n",
    "\"\"\"\n",
    "grouped.unstack() #returns it back to the table format\n",
    "\"\"\"\n",
    "State\tAlabama\tAlaska\tArizona\tArkansas\tCalifornia\tColorado\tConnecticut\tDelaware\tDistrict of Columbia\tFlorida\t...\tSouth Dakota\tTennessee\tTexas\tUtah\tVermont\tVirginia\tWashington\tWest Virginia\tWisconsin\tWyoming\n",
    "Gender\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "Female\t10753.325\t679.975000\t8998.386364\t6621.615385\t48312.840909\t6460.162791\t7144.641026\t2000.029412\t1497.580645\t36019.071429\t...\t1932.757576\t13334.325\t33897.953488\t3046.547619\t2124.695652\t13232.146341\t9796.863636\t6247.225806\t9918.113636\t1161.031250\n",
    "Male\t10765.850\t860.357143\t10036.204545\t6301.690476\t49555.522727\t6442.500000\t6315.300000\t1940.914286\t1534.80645\n",
    "\"\"\"\n",
    "#calling it again returns a flattened structure\n",
    "grouped.unstack()\n",
    "\"\"\"\n",
    "State          Gender\n",
    "Alabama        Female    10753.325000\n",
    "               Male      10765.850000\n",
    "Alaska         Female      679.975000\n",
    "               Male        860.357143\n",
    "Arizona        Female     8998.386364\n",
    "                             ...     \n",
    "West Virginia  Male       6211.612903\n",
    "Wisconsin      Female     9918.113636\n",
    "               Male       9573.454545\n",
    "Wyoming        Female     1161.031250\n",
    "               Male       1149.514286\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting outliers \n",
    "1. Has two ways of getting outliers\n",
    "    1. using z scores - if it's a normal distribution graph \n",
    "    2. using iqr -if it's a skewed distribution. Inter-Quartile Range\n",
    "    3. Box plots - a Box and Whisker Chart shows the following points of data:\n",
    "\n",
    "Minimum range\n",
    "\n",
    "Lower quartile\n",
    "\n",
    "Median\n",
    "\n",
    "Upper quartile\n",
    "\n",
    "Maximum range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using z scores \n",
    "#1. manual way \n",
    "given dataset, get the mean of a series , get the std then get the z scores\n",
    "df['z_scores'] = (df['column'] - df['column'].mean()) / df['column'].std() #calculates the z-scores for the 'column' column \n",
    "\n",
    "#define the outlier threshold\n",
    "outlier_threshold = 3\n",
    "\n",
    "#find the outliers\n",
    "outliers = df[df['z_scores'].abs() > outlier_threshold] #finds the outliers in the 'z_scores' column\n",
    "\n",
    "#. using scipy\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df['z_scores'] = zscore(df['column']) #calculates the z-scores for the 'column' column\n",
    "\n",
    "\n",
    "#using .apply() method to calculate the z-scores for multiple columns\n",
    "\n",
    "df[['z_scores_A', 'z_scores_B']] = df[['A', 'B']].apply(zscore) #calculates the z-scores for the 'A' and 'B' columns\n",
    "\n",
    "\n",
    "#2. using iqr\n",
    "#1. Calculate the first quartile\n",
    "Q1 = df['column'].quantile(0.25)\n",
    "Q2 = df['column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5*IQR\n",
    "upper_bound = Q3 + 1.5*IQR\n",
    "\n",
    "# Find the outliers\n",
    "outliers = df[(df['column'] < lower_bound) | (df['column'] > upper_bound)] #finds the outliers in the 'column' column\n",
    "\n",
    "\n",
    "#3. using box plots\n",
    "# Create a box plot using the matplotlib library\n",
    "plt.boxplot(df['column']) #creates a box plot of the 'column' column    \n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping rows\n",
    "calculate the percentage of data missing, put it in a table \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the sum of missing values in each column\n",
    "missing_values = df.isnull().sum().sort_values(ascending=True) #calculates the sum of missing values in each column\n",
    "#calculate the percentage of missing values in each column\n",
    "missing_values_percentage = (df.isnull().sum() / len(df)).sort_values(ascending=True) * 100 #calculates the percentage of missing values in each column\n",
    "#put the data in a  dataframe with missing values and percentages an index as missing_count.index\n",
    "missing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_values_percentage  index=missing_count.index}) #creates a DataFrame with the missing values and percentages\n",
    "\n",
    "\n",
    "#dropping all rows by index where the value is 0\n",
    "df = df.drop(df[df['column'] == 0].index) #drops all rows where the value in the 'column' column is 0\n",
    "\n",
    "#then drop columns with all missing values that are not that significant\n",
    "df = df.dropna(axis=1, how='all') #drops columns with all missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transposing columns to indexes\n",
    "df2_transposed = df2.T #transposes the DataFrame df2\n",
    "#renaming the index to 'name'\n",
    "df2_transposed = df2_transposed.rename(index={'old_name': 'new_name'}) #renames the index 'old_name' to 'new_name'\n",
    "or \n",
    "df2_transposed = df2_transposed.rename_axis('name') #renames the index to 'name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tableau "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. used for sharing the data analytics with others\n",
    "2. used for creating dashboards. sort and display data to gain deeper insight into the data being analyzed. \n",
    "3. supports creation of stories.A story is simply a collection of dashboards, worksheets, and tables organized in a manner to present the data and analysis in a format that is easy for the audience to view and understand.\n",
    "\n",
    "it is very similar to a powerpoint\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chika",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
